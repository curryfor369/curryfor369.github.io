<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <link rel="stylesheet" href="/lib/gitalk/gitalk.css">
  <script src="/lib/gitalk/gitalk.min.js"></script>

  
<script type="text/javascript">

var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?23e3ceecdbd521f5862af597a6ebd0bc";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();

</script>




  

  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://iscurry.com">
  <title>Hive(2)详细 | Curry&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="第1章 Hive入门1.1 什么是HiveHive：由Facebook开源用于解决海量结构化日志的数据统计。 Hive是基于Hadoop的一个&#x3D;&#x3D;数据仓库工具&#x3D;&#x3D;，可以将&#x3D;&#x3D;结构化的数据文件映射为一张表&#x3D;&#x3D;，并提供&#x3D;&#x3D;类SQL&#x3D;&#x3D;查询功能。 &#x3D;&#x3D;本质是：将HQL转化成MapReduce程序&#x3D;&#x3D;">
<meta property="og:type" content="article">
<meta property="og:title" content="Hive(2)详细">
<meta property="og:url" content="http://iscurry.com/2020/01/01/Hive(2)%E8%AF%A6%E7%BB%86/index.html">
<meta property="og:site_name" content="Curry&#39;s Blog">
<meta property="og:description" content="第1章 Hive入门1.1 什么是HiveHive：由Facebook开源用于解决海量结构化日志的数据统计。 Hive是基于Hadoop的一个&#x3D;&#x3D;数据仓库工具&#x3D;&#x3D;，可以将&#x3D;&#x3D;结构化的数据文件映射为一张表&#x3D;&#x3D;，并提供&#x3D;&#x3D;类SQL&#x3D;&#x3D;查询功能。 &#x3D;&#x3D;本质是：将HQL转化成MapReduce程序&#x3D;&#x3D;">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113926.png">
<meta property="og:image" content="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113927.png">
<meta property="og:image" content="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113928.png">
<meta property="og:image" content="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113929.png">
<meta property="og:image" content="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113930.png">
<meta property="og:image" content="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113931.png">
<meta property="og:image" content="https://gitee.com/curryfor369/picgo/raw/master/img/20200922163426.png">
<meta property="og:image" content="d:/桌面/冉辰星总结/03-hive/Hive(2)详细.assets/20200918113932.png">
<meta property="og:image" content="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113934.png">
<meta property="og:image" content="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113935.png">
<meta property="og:image" content="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113936.png">
<meta property="article:published_time" content="2020-01-01T11:20:20.000Z">
<meta property="article:modified_time" content="2020-09-25T02:17:41.428Z">
<meta property="article:author" content="curry">
<meta property="article:tag" content="Detail">
<meta property="article:tag" content="Hive">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113926.png">
  
    <link rel="alternative" href="/atom.xml" title="Curry&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/ball.ico">
  
  <link rel="stylesheet" type="text/css" href="/./main.0cf68a.css">
  <style type="text/css">
  
    #container.show {
      background: linear-gradient(200deg,#a0cfe4,#e8c37e);
    }
  </style>
  

  
<script type="text/javascript">

var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?23e3ceecdbd521f5862af597a6ebd0bc";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();

</script>




<meta name="generator" content="Hexo 5.1.1"></head>

<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="mymenucontainer" onclick="myFunction(this)">
      <div class="bar1"></div>
      <div class="bar2"></div>
      <div class="bar3"></div>
    </div>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/img/avatar.jpg" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/"></a></h1>
		</hgroup>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/archives/index.html">归档</a></li>
	        
				<li><a href="/categories">分类</a></li>
	        
				<li><a href="/tags">标签</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">友链</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">关于我</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/curryfor369/" title="github"><i class="icon-github"></i></a>
		        
					<a class="rss" target="_blank" href="/atom.xml" title="rss"><i class="icon-rss"></i></a>
		        
					<a class="bilibili" target="_blank" href="https://space.bilibili.com/314420635" title="bilibili"><i class="icon-bilibili"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="/img/avatar.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author"></h1>
			</hgroup>
			
			
			
				
			
				
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/curryfor369/" title="github"><i class="icon-github"></i></a>
			        
						<a class="rss" target="_blank" href="/atom.xml" title="rss"><i class="icon-rss"></i></a>
			        
						<a class="bilibili" target="_blank" href="https://space.bilibili.com/314420635" title="bilibili"><i class="icon-bilibili"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 70%">
				
				
					<li style="width: 25%"><a href="/">主页</a></li>
		        
					<li style="width: 25%"><a href="/archives/index.html">归档</a></li>
		        
					<li style="width: 25%"><a href="/categories">分类</a></li>
		        
					<li style="width: 25%"><a href="/tags">标签</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            </article>
<article id="post-Hive(2)详细" class="article article-type-post " itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Hive(2)详细
    </h1>
  

        
        <a href="/2020/01/01/Hive(2)%E8%AF%A6%E7%BB%86/" class="archive-article-date">
  	<time datetime="2020-01-01T11:20:20.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2020-01-01</time>
</a>
        
      </header>
    
    <!-- 目录内容 -->

    <p class="show-toc-btn" id="show-toc-btn" onclick="showToc();" style="display:none">
          <span class="btn-bg"></span>
          <span class="btn-text">文章导航</span>
          </p>
    <div id="toc-article" class="toc-article">
        <span id="toc-close" class="toc-close" title="隐藏导航" onclick="showBtn();">×</span>
        <strong class="toc-title">文章目录</strong>
           <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC1%E7%AB%A0-Hive%E5%85%A5%E9%97%A8"><span class="toc-number">1.</span> <span class="toc-text">第1章 Hive入门</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E4%BB%80%E4%B9%88%E6%98%AFHive"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 什么是Hive</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-Hive%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 Hive的优缺点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-1-%E4%BC%98%E7%82%B9"><span class="toc-number">1.2.1.</span> <span class="toc-text">1.2.1 优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-2-%E7%BC%BA%E7%82%B9"><span class="toc-number">1.2.2.</span> <span class="toc-text">1.2.2 缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-Hive%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 Hive架构原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-Hive%E5%92%8C%E6%95%B0%E6%8D%AE%E5%BA%93%E6%AF%94%E8%BE%83"><span class="toc-number">1.4.</span> <span class="toc-text">1.4 Hive和数据库比较</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-1-%E6%9F%A5%E8%AF%A2%E8%AF%AD%E8%A8%80"><span class="toc-number">1.4.1.</span> <span class="toc-text">1.4.1 查询语言</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-2-%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%BD%8D%E7%BD%AE"><span class="toc-number">1.4.2.</span> <span class="toc-text">1.4.2 数据存储位置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-3-%E6%95%B0%E6%8D%AE%E6%9B%B4%E6%96%B0"><span class="toc-number">1.4.3.</span> <span class="toc-text">1.4.3 数据更新</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-4-%E7%B4%A2%E5%BC%95"><span class="toc-number">1.4.4.</span> <span class="toc-text">1.4.4 索引</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-5-%E6%89%A7%E8%A1%8C"><span class="toc-number">1.4.5.</span> <span class="toc-text">1.4.5 执行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-6-%E6%89%A7%E8%A1%8C%E5%BB%B6%E8%BF%9F"><span class="toc-number">1.4.6.</span> <span class="toc-text">1.4.6 执行延迟</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-7-%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7"><span class="toc-number">1.4.7.</span> <span class="toc-text">1.4.7 可扩展性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-8-%E6%95%B0%E6%8D%AE%E8%A7%84%E6%A8%A1"><span class="toc-number">1.4.8.</span> <span class="toc-text">1.4.8 数据规模</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC2%E7%AB%A0-Hive%E5%AE%89%E8%A3%85"><span class="toc-number">2.</span> <span class="toc-text">第2章 Hive安装</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Hive%E5%AE%89%E8%A3%85%E5%9C%B0%E5%9D%80"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 Hive安装地址</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-Hive%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 Hive安装部署</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E5%B0%86%E6%9C%AC%E5%9C%B0%E6%96%87%E4%BB%B6%E5%AF%BC%E5%85%A5Hive%E6%A1%88%E4%BE%8B"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 将本地文件导入Hive案例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-MySql%E5%AE%89%E8%A3%85-root"><span class="toc-number">2.4.</span> <span class="toc-text">2.4 MySql安装(&#x3D;&#x3D;root&#x3D;&#x3D;)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-1-%E5%AE%89%E8%A3%85%E5%8C%85%E5%87%86%E5%A4%87"><span class="toc-number">2.4.1.</span> <span class="toc-text">2.4.1 安装包准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-2-%E5%AE%89%E8%A3%85MySql%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="toc-number">2.4.2.</span> <span class="toc-text">2.4.2 安装MySql服务器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-3-%E5%AE%89%E8%A3%85MySql%E5%AE%A2%E6%88%B7%E7%AB%AF"><span class="toc-number">2.4.3.</span> <span class="toc-text">2.4.3 安装MySql客户端</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-4-MySql%E4%B8%ADuser%E8%A1%A8%E4%B8%AD%E4%B8%BB%E6%9C%BA%E9%85%8D%E7%BD%AE"><span class="toc-number">2.4.4.</span> <span class="toc-text">2.4.4 MySql中user表中主机配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-Hive%E5%85%83%E6%95%B0%E6%8D%AE%E9%85%8D%E7%BD%AE%E5%88%B0MySql"><span class="toc-number">2.5.</span> <span class="toc-text">2.5 Hive元数据配置到MySql</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-1-%E9%A9%B1%E5%8A%A8%E6%8B%B7%E8%B4%9D"><span class="toc-number">2.5.1.</span> <span class="toc-text">2.5.1 驱动拷贝</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-2-%E9%85%8D%E7%BD%AEMetastore%E5%88%B0MySql"><span class="toc-number">2.5.2.</span> <span class="toc-text">2.5.2 配置Metastore到MySql</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-3-%E5%A4%9A%E7%AA%97%E5%8F%A3%E5%90%AF%E5%8A%A8Hive%E6%B5%8B%E8%AF%95"><span class="toc-number">2.5.3.</span> <span class="toc-text">2.5.3 多窗口启动Hive测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-6-HiveJDBC%E8%AE%BF%E9%97%AE"><span class="toc-number">2.6.</span> <span class="toc-text">2.6 HiveJDBC访问</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-1-%E5%90%AF%E5%8A%A8hiveserver2%E6%9C%8D%E5%8A%A1"><span class="toc-number">2.6.1.</span> <span class="toc-text">2.6.1 启动hiveserver2服务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-2-%E5%90%AF%E5%8A%A8beeline"><span class="toc-number">2.6.2.</span> <span class="toc-text">2.6.2 启动beeline</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-3-%E8%BF%9E%E6%8E%A5hiveserver2"><span class="toc-number">2.6.3.</span> <span class="toc-text">2.6.3 连接hiveserver2</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-7-Hive%E5%B8%B8%E7%94%A8%E4%BA%A4%E4%BA%92%E5%91%BD%E4%BB%A4"><span class="toc-number">2.7.</span> <span class="toc-text">2.7 Hive常用交互命令</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-8-Hive%E5%85%B6%E4%BB%96%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C"><span class="toc-number">2.8.</span> <span class="toc-text">2.8 Hive其他命令操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-9-Hive%E5%B8%B8%E8%A7%81%E5%B1%9E%E6%80%A7%E9%85%8D%E7%BD%AE"><span class="toc-number">2.9.</span> <span class="toc-text">2.9 Hive常见属性配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-9-1-Hive%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%BD%8D%E7%BD%AE%E9%85%8D%E7%BD%AE"><span class="toc-number">2.9.1.</span> <span class="toc-text">2.9.1 Hive数据仓库位置配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-9-2-%E6%9F%A5%E8%AF%A2%E5%90%8E%E4%BF%A1%E6%81%AF%E6%98%BE%E7%A4%BA%E9%85%8D%E7%BD%AE"><span class="toc-number">2.9.2.</span> <span class="toc-text">2.9.2 查询后信息显示配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-9-3-Hive%E8%BF%90%E8%A1%8C%E6%97%A5%E5%BF%97%E4%BF%A1%E6%81%AF%E9%85%8D%E7%BD%AE"><span class="toc-number">2.9.3.</span> <span class="toc-text">2.9.3 Hive运行日志信息配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-9-4-%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%E6%96%B9%E5%BC%8F"><span class="toc-number">2.9.4.</span> <span class="toc-text">2.9.4 参数配置方式</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC3%E7%AB%A0-Hive%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">3.</span> <span class="toc-text">第3章 Hive数据类型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 基本数据类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E9%9B%86%E5%90%88%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 集合数据类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E7%B1%BB%E5%9E%8B%E8%BD%AC%E5%8C%96"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 类型转化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC4%E7%AB%A0-DDL%E6%95%B0%E6%8D%AE%E5%AE%9A%E4%B9%89"><span class="toc-number">4.</span> <span class="toc-text">第4章 DDL数据定义</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 创建数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 查询数据库</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1-%E6%98%BE%E7%A4%BA%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">4.2.1.</span> <span class="toc-text">4.2.1 显示数据库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2-%E6%9F%A5%E7%9C%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AF%A6%E6%83%85"><span class="toc-number">4.2.2.</span> <span class="toc-text">4.2.2 查看数据库详情</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-3-%E5%88%87%E6%8D%A2%E5%BD%93%E5%89%8D%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">4.2.3.</span> <span class="toc-text">4.3.3 切换当前数据库</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-%E4%BF%AE%E6%94%B9%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">4.3.</span> <span class="toc-text">4.3 修改数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-%E5%88%A0%E9%99%A4%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">4.4.</span> <span class="toc-text">4.4 删除数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-5-%E5%88%9B%E5%BB%BA%E8%A1%A8"><span class="toc-number">4.5.</span> <span class="toc-text">4.5 创建表</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-1-%E7%AE%A1%E7%90%86%E8%A1%A8"><span class="toc-number">4.5.1.</span> <span class="toc-text">4.5.1 管理表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-2-%E5%A4%96%E9%83%A8%E8%A1%A8"><span class="toc-number">4.5.2.</span> <span class="toc-text">4.5.2 外部表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-3-%E7%AE%A1%E7%90%86%E8%A1%A8%E4%B8%8E%E5%A4%96%E9%83%A8%E8%A1%A8%E7%9A%84%E4%BA%92%E7%9B%B8%E8%BD%AC%E6%8D%A2"><span class="toc-number">4.5.3.</span> <span class="toc-text">4.5.3 管理表与外部表的互相转换</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-6-%E5%88%86%E5%8C%BA%E8%A1%A8"><span class="toc-number">4.6.</span> <span class="toc-text">4.6 分区表</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-1-%E5%88%86%E5%8C%BA%E8%A1%A8%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-number">4.6.1.</span> <span class="toc-text">4.6.1 分区表基本操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-2-%E5%88%86%E5%8C%BA%E8%A1%A8%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-number">4.6.2.</span> <span class="toc-text">4.6.2 分区表注意事项</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-7-%E4%BF%AE%E6%94%B9%E8%A1%A8"><span class="toc-number">4.7.</span> <span class="toc-text">4.7 修改表</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-1-%E9%87%8D%E5%91%BD%E5%90%8D%E8%A1%A8"><span class="toc-number">4.7.1.</span> <span class="toc-text">4.7.1 重命名表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-2-%E5%A2%9E%E5%8A%A0%E3%80%81%E4%BF%AE%E6%94%B9%E5%92%8C%E5%88%A0%E9%99%A4%E8%A1%A8%E5%88%86%E5%8C%BA"><span class="toc-number">4.7.2.</span> <span class="toc-text">4.7.2 增加、修改和删除表分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-3-%E5%A2%9E%E5%8A%A0-%E4%BF%AE%E6%94%B9-%E6%9B%BF%E6%8D%A2%E5%88%97%E4%BF%A1%E6%81%AF"><span class="toc-number">4.7.3.</span> <span class="toc-text">4.7.3 增加&#x2F;修改&#x2F;替换列信息</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-8-%E5%88%A0%E9%99%A4%E8%A1%A8"><span class="toc-number">4.8.</span> <span class="toc-text">4.8 删除表</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC5%E7%AB%A0-DML%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="toc-number">5.</span> <span class="toc-text">第5章 DML数据操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5"><span class="toc-number">5.1.</span> <span class="toc-text">5.1 数据导入</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-1-%E5%90%91%E8%A1%A8%E4%B8%AD%E8%A3%85%E8%BD%BD%E6%95%B0%E6%8D%AE%EF%BC%88Load%EF%BC%89"><span class="toc-number">5.1.1.</span> <span class="toc-text">5.1.1 向表中装载数据（Load）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-2-%E9%80%9A%E8%BF%87%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E5%90%91%E8%A1%A8%E4%B8%AD%E6%8F%92%E5%85%A5%E6%95%B0%E6%8D%AE%EF%BC%88Insert%EF%BC%89"><span class="toc-number">5.1.2.</span> <span class="toc-text">5.1.2 通过查询语句向表中插入数据（Insert）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-3-%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E4%B8%AD%E5%88%9B%E5%BB%BA%E8%A1%A8%E5%B9%B6%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%EF%BC%88As-Select%EF%BC%89"><span class="toc-number">5.1.3.</span> <span class="toc-text">5.1.3 查询语句中创建表并加载数据（As Select）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-4-%E5%88%9B%E5%BB%BA%E8%A1%A8%E6%97%B6%E9%80%9A%E8%BF%87Location%E6%8C%87%E5%AE%9A%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E8%B7%AF%E5%BE%84"><span class="toc-number">5.1.4.</span> <span class="toc-text">5.1.4 创建表时通过Location指定加载数据路径</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-5-Import%E6%95%B0%E6%8D%AE%E5%88%B0%E6%8C%87%E5%AE%9AHive%E8%A1%A8%E4%B8%AD"><span class="toc-number">5.1.5.</span> <span class="toc-text">5.1.5 Import数据到指定Hive表中</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA"><span class="toc-number">5.2.</span> <span class="toc-text">5.2 数据导出</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-1-Insert%E5%AF%BC%E5%87%BA"><span class="toc-number">5.2.1.</span> <span class="toc-text">5.2.1 Insert导出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-2-Hadoop%E5%91%BD%E4%BB%A4%E5%AF%BC%E5%87%BA%E5%88%B0%E6%9C%AC%E5%9C%B0"><span class="toc-number">5.2.2.</span> <span class="toc-text">5.2.2 Hadoop命令导出到本地</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-3-Hive-Shell-%E5%91%BD%E4%BB%A4%E5%AF%BC%E5%87%BA"><span class="toc-number">5.2.3.</span> <span class="toc-text">5.2.3 Hive Shell 命令导出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-4-Export%E5%AF%BC%E5%87%BA%E5%88%B0HDFS%E4%B8%8A"><span class="toc-number">5.2.4.</span> <span class="toc-text">5.2.4 Export导出到HDFS上</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-5-Sqoop%E5%AF%BC%E5%87%BA"><span class="toc-number">5.2.5.</span> <span class="toc-text">5.2.5 Sqoop导出</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-%E6%B8%85%E9%99%A4%E8%A1%A8%E4%B8%AD%E6%95%B0%E6%8D%AE%EF%BC%88Truncate%EF%BC%89"><span class="toc-number">5.3.</span> <span class="toc-text">5.3 清除表中数据（Truncate）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC6%E7%AB%A0-%E6%9F%A5%E8%AF%A2"><span class="toc-number">6.</span> <span class="toc-text">第6章 查询</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-%E5%9F%BA%E6%9C%AC%E6%9F%A5%E8%AF%A2%EF%BC%88Select%E2%80%A6From%EF%BC%89"><span class="toc-number">6.1.</span> <span class="toc-text">6.1 基本查询（Select…From）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-1-%E5%85%A8%E8%A1%A8%E5%92%8C%E7%89%B9%E5%AE%9A%E5%88%97%E6%9F%A5%E8%AF%A2"><span class="toc-number">6.1.1.</span> <span class="toc-text">6.1.1 全表和特定列查询</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-2-%E5%88%97%E5%88%AB%E5%90%8D"><span class="toc-number">6.1.2.</span> <span class="toc-text">6.1.2 列别名</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-3-%E7%AE%97%E6%9C%AF%E8%BF%90%E7%AE%97%E7%AC%A6"><span class="toc-number">6.1.3.</span> <span class="toc-text">6.1.3 算术运算符</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-4-%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0"><span class="toc-number">6.1.4.</span> <span class="toc-text">6.1.4 常用函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-5-Limit%E8%AF%AD%E5%8F%A5"><span class="toc-number">6.1.5.</span> <span class="toc-text">6.1.5 Limit语句</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-Where%E8%AF%AD%E5%8F%A5"><span class="toc-number">6.2.</span> <span class="toc-text">6.2 Where语句</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-1-%E6%AF%94%E8%BE%83%E8%BF%90%E7%AE%97%E7%AC%A6%EF%BC%88Between-In-Is-Null%EF%BC%89"><span class="toc-number">6.2.1.</span> <span class="toc-text">6.2.1 比较运算符（Between&#x2F;In&#x2F; Is Null）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-2-Like%E5%92%8CRLike"><span class="toc-number">6.2.2.</span> <span class="toc-text">6.2.2 Like和RLike</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-3-%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E7%AC%A6%EF%BC%88And-Or-Not%EF%BC%89"><span class="toc-number">6.2.3.</span> <span class="toc-text">6.2.3 逻辑运算符（And&#x2F;Or&#x2F;Not）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-%E5%88%86%E7%BB%84"><span class="toc-number">6.3.</span> <span class="toc-text">6.3 分组</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-1-Group-By%E8%AF%AD%E5%8F%A5"><span class="toc-number">6.3.1.</span> <span class="toc-text">6.3.1 Group By语句</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-2-Having%E8%AF%AD%E5%8F%A5"><span class="toc-number">6.3.2.</span> <span class="toc-text">6.3.2 Having语句</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-Join%E8%AF%AD%E5%8F%A5"><span class="toc-number">6.4.</span> <span class="toc-text">6.4 Join语句</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-1-%E7%AD%89%E5%80%BCJoin"><span class="toc-number">6.4.1.</span> <span class="toc-text">6.4.1 等值Join</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-2-%E8%A1%A8%E7%9A%84%E5%88%AB%E5%90%8D"><span class="toc-number">6.4.2.</span> <span class="toc-text">6.4.2 表的别名</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-3-%E5%86%85%E8%BF%9E%E6%8E%A5"><span class="toc-number">6.4.3.</span> <span class="toc-text">6.4.3 内连接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-4-%E5%B7%A6%E5%A4%96%E8%BF%9E%E6%8E%A5"><span class="toc-number">6.4.4.</span> <span class="toc-text">6.4.4 左外连接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-5-%E5%8F%B3%E5%A4%96%E8%BF%9E%E6%8E%A5"><span class="toc-number">6.4.5.</span> <span class="toc-text">6.4.5 右外连接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-6-%E6%BB%A1%E5%A4%96%E8%BF%9E%E6%8E%A5"><span class="toc-number">6.4.6.</span> <span class="toc-text">6.4.6 满外连接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-7-%E5%A4%9A%E8%A1%A8%E8%BF%9E%E6%8E%A5"><span class="toc-number">6.4.7.</span> <span class="toc-text">6.4.7 多表连接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-8-%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%A7%AF"><span class="toc-number">6.4.8.</span> <span class="toc-text">6.4.8 笛卡尔积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-9-%E8%BF%9E%E6%8E%A5%E8%B0%93%E8%AF%8D%E4%B8%AD%E4%B8%8D%E6%94%AF%E6%8C%81or"><span class="toc-number">6.4.9.</span> <span class="toc-text">6.4.9 连接谓词中不支持or</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-5-%E6%8E%92%E5%BA%8F"><span class="toc-number">6.5.</span> <span class="toc-text">6.5 排序</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-1-%E5%85%A8%E5%B1%80%E6%8E%92%E5%BA%8F%EF%BC%88Order-By%EF%BC%89"><span class="toc-number">6.5.1.</span> <span class="toc-text">6.5.1 全局排序（Order By）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-2-%E6%8C%89%E7%85%A7%E5%88%AB%E5%90%8D%E6%8E%92%E5%BA%8F"><span class="toc-number">6.5.2.</span> <span class="toc-text">6.5.2 按照别名排序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-3-%E5%A4%9A%E4%B8%AA%E5%88%97%E6%8E%92%E5%BA%8F"><span class="toc-number">6.5.3.</span> <span class="toc-text">6.5.3 多个列排序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-4-%E6%AF%8F%E4%B8%AAMapReduce%E5%86%85%E9%83%A8%E6%8E%92%E5%BA%8F%EF%BC%88Sort-By%EF%BC%89"><span class="toc-number">6.5.4.</span> <span class="toc-text">6.5.4 每个MapReduce内部排序（Sort By）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-5-%E5%88%86%E5%8C%BA%E6%8E%92%E5%BA%8F%EF%BC%88Distribute-By%EF%BC%89"><span class="toc-number">6.5.5.</span> <span class="toc-text">6.5.5 分区排序（Distribute By）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-6-Cluster-By"><span class="toc-number">6.5.6.</span> <span class="toc-text">6.5.6 Cluster By</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-6-%E5%88%86%E6%A1%B6%E5%8F%8A%E6%8A%BD%E6%A0%B7%E6%9F%A5%E8%AF%A2"><span class="toc-number">6.6.</span> <span class="toc-text">6.6 分桶及抽样查询</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-6-1-%E5%88%86%E6%A1%B6%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8"><span class="toc-number">6.6.1.</span> <span class="toc-text">6.6.1 分桶表数据存储</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-6-2-%E5%88%86%E6%A1%B6%E6%8A%BD%E6%A0%B7%E6%9F%A5%E8%AF%A2"><span class="toc-number">6.6.2.</span> <span class="toc-text">6.6.2 分桶抽样查询</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-7-%E5%85%B6%E4%BB%96%E5%B8%B8%E7%94%A8%E6%9F%A5%E8%AF%A2%E5%87%BD%E6%95%B0"><span class="toc-number">6.7.</span> <span class="toc-text">6.7 其他常用查询函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-7-1-%E7%A9%BA%E5%AD%97%E6%AE%B5%E8%B5%8B%E5%80%BC"><span class="toc-number">6.7.1.</span> <span class="toc-text">6.7.1 空字段赋值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-7-2-CASE-WHEN"><span class="toc-number">6.7.2.</span> <span class="toc-text">6.7.2 CASE WHEN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-7-2-%E8%A1%8C%E8%BD%AC%E5%88%97"><span class="toc-number">6.7.3.</span> <span class="toc-text">6.7.2 行转列</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-7-3-%E5%88%97%E8%BD%AC%E8%A1%8C"><span class="toc-number">6.7.4.</span> <span class="toc-text">6.7.3 列转行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-7-4-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0"><span class="toc-number">6.7.5.</span> <span class="toc-text">6.7.4 窗口函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-7-5-Rank"><span class="toc-number">6.7.6.</span> <span class="toc-text">6.7.5 Rank</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC7%E7%AB%A0-%E5%87%BD%E6%95%B0"><span class="toc-number">7.</span> <span class="toc-text">第7章 函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-%E7%B3%BB%E7%BB%9F%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0"><span class="toc-number">7.1.</span> <span class="toc-text">7.1 系统内置函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0"><span class="toc-number">7.2.</span> <span class="toc-text">7.2 自定义函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3-%E8%87%AA%E5%AE%9A%E4%B9%89UDF%E5%87%BD%E6%95%B0"><span class="toc-number">7.3.</span> <span class="toc-text">7.3 自定义UDF函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC8%E7%AB%A0-%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%AD%98%E5%82%A8"><span class="toc-number">8.</span> <span class="toc-text">第8章 压缩和存储</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-Hadoop%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E6%94%AF%E6%8C%81Snappy%E5%8E%8B%E7%BC%A9"><span class="toc-number">8.1.</span> <span class="toc-text">8.1 Hadoop源码编译支持Snappy压缩</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-1-%E8%B5%84%E6%BA%90%E5%87%86%E5%A4%87"><span class="toc-number">8.1.1.</span> <span class="toc-text">8.1.1 资源准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-2-jar%E5%8C%85%E5%AE%89%E8%A3%85"><span class="toc-number">8.1.2.</span> <span class="toc-text">8.1.2 jar包安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-3-%E7%BC%96%E8%AF%91%E6%BA%90%E7%A0%81"><span class="toc-number">8.1.3.</span> <span class="toc-text">8.1.3 编译源码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-Hadoop%E5%8E%8B%E7%BC%A9%E9%85%8D%E7%BD%AE"><span class="toc-number">8.2.</span> <span class="toc-text">8.2 Hadoop压缩配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-1-MR%E6%94%AF%E6%8C%81%E7%9A%84%E5%8E%8B%E7%BC%A9%E7%BC%96%E7%A0%81"><span class="toc-number">8.2.1.</span> <span class="toc-text">8.2.1 MR支持的压缩编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-2-%E5%8E%8B%E7%BC%A9%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE"><span class="toc-number">8.2.2.</span> <span class="toc-text">8.2.2 压缩参数配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-3-%E5%BC%80%E5%90%AFMap%E8%BE%93%E5%87%BA%E9%98%B6%E6%AE%B5%E5%8E%8B%E7%BC%A9"><span class="toc-number">8.3.</span> <span class="toc-text">8.3 开启Map输出阶段压缩</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-4-%E5%BC%80%E5%90%AFReduce%E8%BE%93%E5%87%BA%E9%98%B6%E6%AE%B5%E5%8E%8B%E7%BC%A9"><span class="toc-number">8.4.</span> <span class="toc-text">8.4 开启Reduce输出阶段压缩</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-5-%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F"><span class="toc-number">8.5.</span> <span class="toc-text">8.5 文件存储格式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-5-1-%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8%E5%92%8C%E8%A1%8C%E5%BC%8F%E5%AD%98%E5%82%A8"><span class="toc-number">8.5.1.</span> <span class="toc-text">8.5.1 列式存储和行式存储</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-5-2-TextFile%E6%A0%BC%E5%BC%8F"><span class="toc-number">8.5.2.</span> <span class="toc-text">8.5.2 TextFile格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-5-3-Orc%E6%A0%BC%E5%BC%8F"><span class="toc-number">8.5.3.</span> <span class="toc-text">8.5.3 Orc格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-5-4-Parquet%E6%A0%BC%E5%BC%8F"><span class="toc-number">8.5.4.</span> <span class="toc-text">8.5.4 Parquet格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-5-5-%E4%B8%BB%E6%B5%81%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E5%AF%B9%E6%AF%94%E5%AE%9E%E9%AA%8C"><span class="toc-number">8.5.5.</span> <span class="toc-text">8.5.5 主流文件存储格式对比实验</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-6-%E5%AD%98%E5%82%A8%E5%92%8C%E5%8E%8B%E7%BC%A9%E7%BB%93%E5%90%88"><span class="toc-number">8.6.</span> <span class="toc-text">8.6 存储和压缩结合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-6-1-%E4%BF%AE%E6%94%B9Hadoop%E9%9B%86%E7%BE%A4%E5%85%B7%E6%9C%89Snappy%E5%8E%8B%E7%BC%A9%E6%96%B9%E5%BC%8F"><span class="toc-number">8.6.1.</span> <span class="toc-text">8.6.1 修改Hadoop集群具有Snappy压缩方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-6-2-%E6%B5%8B%E8%AF%95%E5%AD%98%E5%82%A8%E5%92%8C%E5%8E%8B%E7%BC%A9"><span class="toc-number">8.6.2.</span> <span class="toc-text">8.6.2 测试存储和压缩</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC9%E7%AB%A0-%E4%BC%81%E4%B8%9A%E7%BA%A7%E8%B0%83%E4%BC%98"><span class="toc-number">9.</span> <span class="toc-text">第9章 企业级调优</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#9-1-Fetch%E6%8A%93%E5%8F%96"><span class="toc-number">9.1.</span> <span class="toc-text">9.1 Fetch抓取</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-2-%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F"><span class="toc-number">9.2.</span> <span class="toc-text">9.2 本地模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-3-%E8%A1%A8%E7%9A%84%E4%BC%98%E5%8C%96"><span class="toc-number">9.3.</span> <span class="toc-text">9.3 表的优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-1-%E5%B0%8F%E8%A1%A8%E3%80%81%E5%A4%A7%E8%A1%A8Join"><span class="toc-number">9.3.1.</span> <span class="toc-text">9.3.1 小表、大表Join</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-2-%E5%A4%A7%E8%A1%A8Join%E5%A4%A7%E8%A1%A8"><span class="toc-number">9.3.2.</span> <span class="toc-text">9.3.2 大表Join大表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-3-MapJoin"><span class="toc-number">9.3.3.</span> <span class="toc-text">9.3.3 MapJoin</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-4-Group-By"><span class="toc-number">9.3.4.</span> <span class="toc-text">9.3.4 Group By</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-5-Count-Distinct-%E5%8E%BB%E9%87%8D%E7%BB%9F%E8%AE%A1"><span class="toc-number">9.3.5.</span> <span class="toc-text">9.3.5 Count(Distinct) 去重统计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-6-%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%A7%AF"><span class="toc-number">9.3.6.</span> <span class="toc-text">9.3.6 笛卡尔积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-7-%E8%A1%8C%E5%88%97%E8%BF%87%E6%BB%A4"><span class="toc-number">9.3.7.</span> <span class="toc-text">9.3.7 行列过滤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-8-%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%B0%83%E6%95%B4"><span class="toc-number">9.3.8.</span> <span class="toc-text">9.3.8 动态分区调整</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-9-%E5%88%86%E6%A1%B6"><span class="toc-number">9.3.9.</span> <span class="toc-text">9.3.9 分桶</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-10-%E5%88%86%E5%8C%BA"><span class="toc-number">9.3.10.</span> <span class="toc-text">9.3.10 分区</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-4-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C"><span class="toc-number">9.4.</span> <span class="toc-text">9.4 数据倾斜</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-4-1-%E5%90%88%E7%90%86%E8%AE%BE%E7%BD%AEMap%E6%95%B0"><span class="toc-number">9.4.1.</span> <span class="toc-text">9.4.1 合理设置Map数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-4-2-%E5%B0%8F%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E5%90%88%E5%B9%B6"><span class="toc-number">9.4.2.</span> <span class="toc-text">9.4.2 小文件进行合并</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-4-3-%E5%A4%8D%E6%9D%82%E6%96%87%E4%BB%B6%E5%A2%9E%E5%8A%A0Map%E6%95%B0"><span class="toc-number">9.4.3.</span> <span class="toc-text">9.4.3 复杂文件增加Map数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-4-4-%E5%90%88%E7%90%86%E8%AE%BE%E7%BD%AEReduce%E6%95%B0"><span class="toc-number">9.4.4.</span> <span class="toc-text">9.4.4 合理设置Reduce数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-5-%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C"><span class="toc-number">9.5.</span> <span class="toc-text">9.5 并行执行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-6-%E4%B8%A5%E6%A0%BC%E6%A8%A1%E5%BC%8F"><span class="toc-number">9.6.</span> <span class="toc-text">9.6 严格模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-7-JVM%E9%87%8D%E7%94%A8"><span class="toc-number">9.7.</span> <span class="toc-text">9.7 JVM重用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-8-%E6%8E%A8%E6%B5%8B%E6%89%A7%E8%A1%8C"><span class="toc-number">9.8.</span> <span class="toc-text">9.8 推测执行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-9-%E5%8E%8B%E7%BC%A9"><span class="toc-number">9.9.</span> <span class="toc-text">9.9 压缩</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-10-%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%EF%BC%88Explain%EF%BC%89"><span class="toc-number">9.10.</span> <span class="toc-text">9.10 执行计划（Explain）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC10%E7%AB%A0-Hive%E5%AE%9E%E6%88%98%E4%B9%8B%E8%B0%B7%E7%B2%92%E5%BD%B1%E9%9F%B3"><span class="toc-number">10.</span> <span class="toc-text">第10章 Hive实战之谷粒影音</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#10-1-%E9%9C%80%E6%B1%82%E6%8F%8F%E8%BF%B0"><span class="toc-number">10.1.</span> <span class="toc-text">10.1 需求描述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-2-%E9%A1%B9%E7%9B%AE"><span class="toc-number">10.2.</span> <span class="toc-text">10.2 项目</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-1-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84"><span class="toc-number">10.2.1.</span> <span class="toc-text">10.2.1 数据结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-2-ETL%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE"><span class="toc-number">10.2.2.</span> <span class="toc-text">10.2.2 ETL原始数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-3-%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="toc-number">10.3.</span> <span class="toc-text">10.3 准备工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-3-1-%E5%88%9B%E5%BB%BA%E8%A1%A8"><span class="toc-number">10.3.1.</span> <span class="toc-text">10.3.1 创建表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-3-2-%E5%AF%BC%E5%85%A5ETL%E5%90%8E%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="toc-number">10.3.2.</span> <span class="toc-text">10.3.2 导入ETL后的数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-3-3-%E5%90%91ORC%E8%A1%A8%E6%8F%92%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-number">10.3.3.</span> <span class="toc-text">10.3.3 向ORC表插入数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-4-%E4%B8%9A%E5%8A%A1%E5%88%86%E6%9E%90"><span class="toc-number">10.4.</span> <span class="toc-text">10.4 业务分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-4-1-%E7%BB%9F%E8%AE%A1%E8%A7%86%E9%A2%91%E8%A7%82%E7%9C%8B%E6%95%B0Top10"><span class="toc-number">10.4.1.</span> <span class="toc-text">10.4.1 统计视频观看数Top10</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-4-2-%E7%BB%9F%E8%AE%A1%E8%A7%86%E9%A2%91%E7%B1%BB%E5%88%AB%E7%83%AD%E5%BA%A6Top10"><span class="toc-number">10.4.2.</span> <span class="toc-text">10.4.2 统计视频类别热度Top10</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-4-3-%E7%BB%9F%E8%AE%A1%E5%87%BA%E8%A7%86%E9%A2%91%E8%A7%82%E7%9C%8B%E6%95%B0%E6%9C%80%E9%AB%98%E7%9A%8420%E4%B8%AA%E8%A7%86%E9%A2%91%E7%9A%84%E6%89%80%E5%B1%9E%E7%B1%BB%E5%88%AB%E4%BB%A5%E5%8F%8A%E7%B1%BB%E5%88%AB%E5%8C%85%E5%90%ABTop20%E8%A7%86%E9%A2%91%E7%9A%84%E4%B8%AA%E6%95%B0"><span class="toc-number">10.4.3.</span> <span class="toc-text">10.4.3 统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-4-4-%E7%BB%9F%E8%AE%A1%E8%A7%86%E9%A2%91%E8%A7%82%E7%9C%8B%E6%95%B0Top50%E6%89%80%E5%85%B3%E8%81%94%E8%A7%86%E9%A2%91%E7%9A%84%E6%89%80%E5%B1%9E%E7%B1%BB%E5%88%ABRank"><span class="toc-number">10.4.4.</span> <span class="toc-text">10.4.4 统计视频观看数Top50所关联视频的所属类别Rank</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-4-5-%E7%BB%9F%E8%AE%A1%E6%AF%8F%E4%B8%AA%E7%B1%BB%E5%88%AB%E4%B8%AD%E7%9A%84%E8%A7%86%E9%A2%91%E7%83%AD%E5%BA%A6Top10%EF%BC%8C%E4%BB%A5Music%E4%B8%BA%E4%BE%8B"><span class="toc-number">10.4.5.</span> <span class="toc-text">10.4.5 统计每个类别中的视频热度Top10，以Music为例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-4-6-%E7%BB%9F%E8%AE%A1%E6%AF%8F%E4%B8%AA%E7%B1%BB%E5%88%AB%E4%B8%AD%E8%A7%86%E9%A2%91%E6%B5%81%E9%87%8FTop10%EF%BC%8C%E4%BB%A5Music%E4%B8%BA%E4%BE%8B"><span class="toc-number">10.4.6.</span> <span class="toc-text">10.4.6 统计每个类别中视频流量Top10，以Music为例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-4-7-%E7%BB%9F%E8%AE%A1%E4%B8%8A%E4%BC%A0%E8%A7%86%E9%A2%91%E6%9C%80%E5%A4%9A%E7%9A%84%E7%94%A8%E6%88%B7Top10%E4%BB%A5%E5%8F%8A%E4%BB%96%E4%BB%AC%E4%B8%8A%E4%BC%A0%E7%9A%84%E8%A7%82%E7%9C%8B%E6%AC%A1%E6%95%B0%E5%9C%A8%E5%89%8D20%E7%9A%84%E8%A7%86%E9%A2%91"><span class="toc-number">10.4.7.</span> <span class="toc-text">10.4.7 统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-4-8-%E7%BB%9F%E8%AE%A1%E6%AF%8F%E4%B8%AA%E7%B1%BB%E5%88%AB%E8%A7%86%E9%A2%91%E8%A7%82%E7%9C%8B%E6%95%B0Top10"><span class="toc-number">10.4.8.</span> <span class="toc-text">10.4.8 统计每个类别视频观看数Top10</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC11%E7%AB%A0-%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-number">11.</span> <span class="toc-text">第11章 常见错误及解决方案</span></a></li></ol>
         </div>
   <script type="text/javascript">
    function showToc(){
        var toc_article = document.getElementById("toc-article");
        var show_toc_btn = document.getElementById("show-toc-btn");
        toc_article.setAttribute("style","display:block");
        show_toc_btn.setAttribute("style","display:none");
        };
    function showBtn(){
        var toc_article = document.getElementById("toc-article");
        var show_toc_btn = document.getElementById("show-toc-btn");
        toc_article.setAttribute("style","display:none");
        show_toc_btn.setAttribute("style","display:block");
        };
   </script>
      
<!-- 目录内容结束 -->


    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="第1章-Hive入门"><a href="#第1章-Hive入门" class="headerlink" title="第1章 Hive入门"></a>第1章 Hive入门</h1><h2 id="1-1-什么是Hive"><a href="#1-1-什么是Hive" class="headerlink" title="1.1 什么是Hive"></a>1.1 什么是Hive</h2><p>Hive：由Facebook开源用于解决海量结构化日志的数据统计。</p>
<p>Hive是基于Hadoop的一个==<strong>数据仓库工具</strong>==，可以将==<strong>结构化的数据文件映射为一张表</strong>==，并提供==<strong>类SQL</strong>==查询功能。</p>
<p>==<strong>本质是：将HQL转化成MapReduce程序</strong>==</p>
<p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113926.png" alt="image-20200917232726291"></p>
<a id="more"></a>

<p>1）Hive处理的数据存储在HDFS</p>
<p>2）Hive分析数据底层的实现是MapReduce</p>
<p>3）执行程序运行在Yarn上</p>
<h2 id="1-2-Hive的优缺点"><a href="#1-2-Hive的优缺点" class="headerlink" title="1.2 Hive的优缺点"></a>1.2 Hive的优缺点</h2><h3 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h3><ol>
<li><p>操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）。</p>
</li>
<li><p>避免了去写MapReduce，减少开发人员的学习成本。</p>
</li>
<li><p>Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合。</p>
</li>
<li><p>Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高。</p>
</li>
<li><p>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。</p>
</li>
</ol>
<h3 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h3><p>1．Hive的HQL表达能力有限</p>
<blockquote>
<p>  （1）迭代式算法无法表达</p>
</blockquote>
<blockquote>
<p>  （2）数据挖掘方面不擅长</p>
</blockquote>
<p>2．Hive的效率比较低</p>
<blockquote>
<p>  （1）Hive自动生成的MapReduce作业，通常情况下不够智能化</p>
</blockquote>
<blockquote>
<p>  （2）Hive调优比较困难，粒度较粗</p>
</blockquote>
<h2 id="1-3-Hive架构原理"><a href="#1-3-Hive架构原理" class="headerlink" title="1.3 Hive架构原理"></a>1.3 Hive架构原理</h2><p>Hive架构原理</p>
<p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113927.png" alt="image-20200917233006917"></p>
<p>1．用户接口：ClientCLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问hive）</p>
<p>2．元数据：Metastore</p>
<blockquote>
<p>  元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；</p>
</blockquote>
<blockquote>
<p>  <strong>==默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore==</strong></p>
</blockquote>
<p>3．Hadoop</p>
<blockquote>
<p>  使用HDFS进行存储，使用MapReduce进行计算。</p>
</blockquote>
<p>4．驱动器：Driver</p>
<blockquote>
<p>  （1）解析器（SQLParser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</p>
</blockquote>
<blockquote>
<p>  （2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。</p>
</blockquote>
<blockquote>
<p>  （3）优化器（Query Optimizer）：对逻辑执行计划进行优化。</p>
</blockquote>
<blockquote>
<p>（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。</p>
</blockquote>
<p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113928.png" alt="image-20200917233230956"></p>
<p>Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。</p>
<h2 id="1-4-Hive和数据库比较"><a href="#1-4-Hive和数据库比较" class="headerlink" title="1.4 Hive和数据库比较"></a>1.4 Hive和数据库比较</h2><p>由于 Hive 采用了类似SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive理解为数据库。其实从结构上来看，Hive和数据库除了拥有类似的查询语言，再无类似之处。本文将从多个方面来阐述 Hive和数据库的差异。数据库可以用在 Online 的应用中，但是Hive是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。</p>
<h3 id="1-4-1-查询语言"><a href="#1-4-1-查询语言" class="headerlink" title="1.4.1 查询语言"></a>1.4.1 查询语言</h3><p>由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发。</p>
<h3 id="1-4-2-数据存储位置"><a href="#1-4-2-数据存储位置" class="headerlink" title="1.4.2 数据存储位置"></a>1.4.2 数据存储位置</h3><p>Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS中的。而数据库则可以将数据保存在块设备或者本地文件系统中。</p>
<h3 id="1-4-3-数据更新"><a href="#1-4-3-数据更新" class="headerlink" title="1.4.3 数据更新"></a>1.4.3 数据更新</h3><p>由于Hive是针对数据仓库应用设计的，而**==数据仓库的内容是读多写少的==<strong>。因此，</strong>==Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的==**。而数据库中的数据通常是需要经常进行修改的，因此可以使用<br>INSERT INTO … VALUES 添加数据，使用 UPDATE … SET修改数据。</p>
<h3 id="1-4-4-索引"><a href="#1-4-4-索引" class="headerlink" title="1.4.4 索引"></a>1.4.4 索引</h3><p>Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。Hive要访问数据中满足条件的特定值时，需要**==暴力扫描整个数据==**，因此访问延迟较高。由于MapReduce 的引入， Hive可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了Hive 不适合在线数据查询。</p>
<h3 id="1-4-5-执行"><a href="#1-4-5-执行" class="headerlink" title="1.4.5 执行"></a>1.4.5 执行</h3><p>Hive中大多数查询的执行是通过 Hadoop 提供的 MapReduce来实现的。而数据库通常有自己的执行引擎。执行引擎还有Tez,Spark等等</p>
<h3 id="1-4-6-执行延迟"><a href="#1-4-6-执行延迟" class="headerlink" title="1.4.6 执行延迟"></a>1.4.6 执行延迟</h3><p>Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce本身具有较高的延迟，因此在利用MapReduce执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。</p>
<h3 id="1-4-7-可扩展性"><a href="#1-4-7-可扩展性" class="headerlink" title="1.4.7 可扩展性"></a>1.4.7 可扩展性</h3><p>由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的（世界上最大Hadoop集群在 Yahoo!，2009年的规模在4000 台节点左右）。而数据库由于 ACID语义的严格限制，扩展行非常有限。目前最先进的并行数据库Oracle在理论上的扩展能力也只有100台左右。</p>
<h3 id="1-4-8-数据规模"><a href="#1-4-8-数据规模" class="headerlink" title="1.4.8 数据规模"></a>1.4.8 数据规模</h3><p>由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。</p>
<h1 id="第2章-Hive安装"><a href="#第2章-Hive安装" class="headerlink" title="第2章 Hive安装"></a>第2章 Hive安装</h1><h2 id="2-1-Hive安装地址"><a href="#2-1-Hive安装地址" class="headerlink" title="2.1 Hive安装地址"></a>2.1 Hive安装地址</h2><p>1．Hive官网地址</p>
<p><a target="_blank" rel="noopener" href="http://hive.apache.org/">http://hive.apache.org/</a></p>
<p>2．文档查看地址</p>
<p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></p>
<p>3．下载地址</p>
<p><a target="_blank" rel="noopener" href="http://archive.apache.org/dist/hive/">http://archive.apache.org/dist/hive/</a></p>
<p>4．github地址</p>
<p><a target="_blank" rel="noopener" href="https://github.com/apache/hive">https://github.com/apache/hive</a></p>
<h2 id="2-2-Hive安装部署"><a href="#2-2-Hive安装部署" class="headerlink" title="2.2 Hive安装部署"></a>2.2 Hive安装部署</h2><p>1．Hive安装及配置</p>
<blockquote>
<p>  （1）把apache-hive-1.2.1-bin.tar.gz上传到linux的/opt/software目录下</p>
</blockquote>
<blockquote>
<p>  （2）解压apache-hive-1.2.1-bin.tar.gz到/opt/module/目录下面</p>
</blockquote>
<blockquote>
<p>  [xing@hadoop102 software]$ tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/</p>
</blockquote>
<blockquote>
<p>  （3）修改apache-hive-1.2.1-bin.tar.gz的名称为hive</p>
</blockquote>
<blockquote>
<p>  [xing@hadoop102 module]$ mv apache-hive-1.2.1-bin/ hive</p>
</blockquote>
<blockquote>
<p>  （4）修改/opt/module/hive/conf目录下的hive-env.sh.template名称为hive-env.sh</p>
</blockquote>
<blockquote>
<p>  [xing@hadoop102 conf]$ mv hive-env.sh.template hive-env.sh</p>
</blockquote>
<p>（5）配置hive-env.sh文件</p>
<blockquote>
<p>  （a）配置HADOOP_HOME路径</p>
</blockquote>
<blockquote>
<p>  export HADOOP_HOME=/opt/module/hadoop-2.7.2</p>
</blockquote>
<blockquote>
<p>  （b）配置HIVE_CONF_DIR路径</p>
</blockquote>
<blockquote>
<p>  export HIVE_CONF_DIR=/opt/module/hive/conf</p>
</blockquote>
<p>2．Hadoop集群配置</p>
<p>（1）必须启动hdfs和yarn</p>
<blockquote>
<p>  [xing@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</p>
</blockquote>
<blockquote>
<p>  [xing@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</p>
</blockquote>
<p>（2）在HDFS上创建/tmp和/user/hive/warehouse两个目录并修改他们的同组权限可写</p>
<blockquote>
<p>  [xing@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir /tmp</p>
</blockquote>
<blockquote>
<p>  [xing@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir -p /user/hive/warehouse</p>
</blockquote>
<blockquote>
<p>  [xing@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /tmp</p>
</blockquote>
<blockquote>
<p>  [xing@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /user/hive/warehouse</p>
</blockquote>
<p>3．Hive基本操作</p>
<p>（1）启动hive</p>
<blockquote>
<p>  [xing@hadoop102 hive]$ bin/hive</p>
</blockquote>
<p>（2）查看数据库</p>
<blockquote>
<p>  hive&gt; show databases;</p>
</blockquote>
<p>（3）打开默认数据库</p>
<blockquote>
<p>  hive&gt; use default;</p>
</blockquote>
<p>（4）显示default数据库中的表</p>
<blockquote>
<p>  hive&gt; show tables;</p>
</blockquote>
<p>（5）创建一张表</p>
<blockquote>
<p>  hive&gt; create table student(id int, name string);</p>
</blockquote>
<p>（6）显示数据库中有几张表</p>
<blockquote>
<p>  hive&gt; show tables;</p>
</blockquote>
<p>（7）查看表的结构</p>
<blockquote>
<p>  hive&gt; desc student;</p>
</blockquote>
<p>（8）向表中插入数据</p>
<blockquote>
<p>  hive&gt; insert into student values(1000,”ss”);</p>
</blockquote>
<p>（9）查询表中数据</p>
<blockquote>
<p>  hive&gt; select * from student;</p>
</blockquote>
<p>（10）退出hive</p>
<blockquote>
<p>  hive&gt; quit;</p>
</blockquote>
<h2 id="2-3-将本地文件导入Hive案例"><a href="#2-3-将本地文件导入Hive案例" class="headerlink" title="2.3 将本地文件导入Hive案例"></a>2.3 将本地文件导入Hive案例</h2><p>需求</p>
<p>将本地/opt/module/datas/student.txt这个目录下的数据导入到hive的student(id int,name string)表中。</p>
<p>1．数据准备</p>
<blockquote>
<p>  在/opt/module/datas这个目录下准备数据</p>
</blockquote>
<blockquote>
<p>  （1）在/opt/module/目录下创建datas</p>
</blockquote>
<blockquote>
<p>  [xing@hadoop102 module]$ mkdir datas</p>
</blockquote>
<blockquote>
<p>  （2）在/opt/module/datas/目录下创建student.txt文件并添加数据</p>
</blockquote>
<blockquote>
<p>  [xing@hadoop102 datas]$ touch student.txt</p>
</blockquote>
<blockquote>
<p>  [xing@hadoop102 datas]$ vi student.txt</p>
</blockquote>
<blockquote>
<p>  1001 zhangshan</p>
</blockquote>
<blockquote>
<p>  1002 lishi</p>
</blockquote>
<blockquote>
<p>  1003 zhaoliu</p>
</blockquote>
<p><strong>==注意以tab键间隔==</strong></p>
<p>2．Hive实际操作</p>
<blockquote>
<p>  （1）启动hive</p>
</blockquote>
<blockquote>
<p>  [xing@hadoop102 hive]$ bin/hive</p>
</blockquote>
<blockquote>
<p>  （2）显示数据库</p>
</blockquote>
<blockquote>
<p>  hive&gt; show databases;</p>
</blockquote>
<blockquote>
<p>  （3）使用default数据库</p>
</blockquote>
<blockquote>
<p>  hive&gt; use default;</p>
</blockquote>
<blockquote>
<p>  （4）显示default数据库中的表</p>
</blockquote>
<blockquote>
<p>  hive&gt; show tables;</p>
</blockquote>
<blockquote>
<p>  （5）删除已创建的student表</p>
</blockquote>
<blockquote>
<p>  hive&gt; drop table student;</p>
</blockquote>
<blockquote>
<p>  （6）创建student表, 并声明文件分隔符’\t’</p>
</blockquote>
<blockquote>
<p>  hive&gt; create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</p>
</blockquote>
<blockquote>
<p>  （7）加载/opt/module/datas/student.txt 文件到student数据库表中。</p>
</blockquote>
<blockquote>
<p>  hive&gt; load data local inpath ‘/opt/module/datas/student.txt’ into table student;</p>
</blockquote>
<blockquote>
<p>  （8）Hive查询结果</p>
</blockquote>
<blockquote>
<p>  hive&gt; select * from student;</p>
</blockquote>
<blockquote>
<p>  OK</p>
</blockquote>
<blockquote>
<p>  1001 zhangshan</p>
</blockquote>
<blockquote>
<p>  1002 lishi</p>
</blockquote>
<blockquote>
<p>  1003 zhaoliu</p>
</blockquote>
<blockquote>
<p>  Time taken: 0.266 seconds, Fetched: 3 row(s)</p>
</blockquote>
<p>3．遇到的问题</p>
<p>再打开一个客户端窗口启动hive，会产生java.sql.SQLException异常。</p>
<blockquote>
<p>  Exception in thread “main” java.lang.RuntimeException: java.lang.RuntimeException:<br>   Unable to instantiate<br>   org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient<br>          at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)<br>          at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)<br>          at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)<br>          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>          at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)<br>          at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>          at java.lang.reflect.Method.invoke(Method.java:606)<br>          at org.apache.hadoop.util.RunJar.run(RunJar.java:221)<br>          at org.apache.hadoop.util.RunJar.main(RunJar.java:136)<br>  Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient<br>          at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)<br>          at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)<br>          at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)<br>          at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)<br>          at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)<br>          at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)<br>          at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)<br>  … 8 more</p>
</blockquote>
<p>原因是，**==Metastore默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore;==**</p>
<h2 id="2-4-MySql安装-root"><a href="#2-4-MySql安装-root" class="headerlink" title="2.4 MySql安装(==root==)"></a>2.4 MySql安装(==root==)</h2><h3 id="2-4-1-安装包准备"><a href="#2-4-1-安装包准备" class="headerlink" title="2.4.1 安装包准备"></a>2.4.1 安装包准备</h3><p>1．查看mysql是否安装，如果安装了，卸载mysql</p>
<p>（1）查看</p>
<blockquote>
<p>  [root@hadoop102 桌面]# rpm -qa|grep mysql</p>
</blockquote>
<blockquote>
<p>  mysql-libs-5.1.73-7.el6.x86_64</p>
</blockquote>
<p>（2）卸载</p>
<blockquote>
<p>  [root@hadoop102 桌面]# rpm -e –nodeps mysql-libs-5.1.73-7.el6.x86_64</p>
</blockquote>
<p>2．解压mysql-libs.zip文件到当前目录</p>
<blockquote>
<p>  [root@hadoop102 software]# unzip mysql-libs.zip</p>
</blockquote>
<blockquote>
<p>  [root@hadoop102 software]# ls</p>
</blockquote>
<blockquote>
<p>  mysql-libs.zip</p>
</blockquote>
<blockquote>
<p>  mysql-libs</p>
</blockquote>
<p>3．进入到mysql-libs文件夹下</p>
<blockquote>
<p>  [root@hadoop102 mysql-libs]# ll</p>
</blockquote>
<blockquote>
<p>  总用量 76048</p>
</blockquote>
<blockquote>
<p>  -rw-r–r–. 1 root root 18509960 3月 26 2015 MySQL-client-5.6.24-1.el6.x86_64.rpm</p>
</blockquote>
<blockquote>
<p>  -rw-r–r–. 1 root root 3575135 12月 1 2013 mysql-connector-java-5.1.27.tar.gz</p>
</blockquote>
<blockquote>
<p>  -rw-r–r–. 1 root root 55782196 3月 26 2015 MySQL-server-5.6.24-1.el6.x86_64.rpm</p>
</blockquote>
<h3 id="2-4-2-安装MySql服务器"><a href="#2-4-2-安装MySql服务器" class="headerlink" title="2.4.2 安装MySql服务器"></a>2.4.2 安装MySql服务器</h3><p>1．安装mysql服务端</p>
<blockquote>
<p>  [root@hadoop102 mysql-libs]# rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm</p>
</blockquote>
<p>2．查看产生的随机密码</p>
<blockquote>
<p>  [root@hadoop102 mysql-libs]# cat /root/.mysql_secret</p>
</blockquote>
<blockquote>
<p>  OEXaQuS8IWkG19Xs</p>
</blockquote>
<p>3．查看mysql状态</p>
<blockquote>
<p>  [root@hadoop102 mysql-libs]# service mysql status</p>
</blockquote>
<p>4．启动mysql</p>
<blockquote>
<p>  [root@hadoop102 mysql-libs]# service mysql start</p>
</blockquote>
<h3 id="2-4-3-安装MySql客户端"><a href="#2-4-3-安装MySql客户端" class="headerlink" title="2.4.3 安装MySql客户端"></a>2.4.3 安装MySql客户端</h3><p>1．安装mysql客户端</p>
<blockquote>
<p>  [root@hadoop102 mysql-libs]# rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm</p>
</blockquote>
<p>2．链接mysql</p>
<blockquote>
<p>  [root@hadoop102 mysql-libs]# mysql -uroot -pOEXaQuS8IWkG19Xs</p>
</blockquote>
<p>3．修改密码</p>
<blockquote>
<p>  mysql&gt;SET PASSWORD=PASSWORD(‘000000’);</p>
</blockquote>
<p>4．退出mysql</p>
<blockquote>
<p>  mysql&gt;exit</p>
</blockquote>
<h3 id="2-4-4-MySql中user表中主机配置"><a href="#2-4-4-MySql中user表中主机配置" class="headerlink" title="2.4.4 MySql中user表中主机配置"></a>2.4.4 MySql中user表中主机配置</h3><p>配置只要是root用户+密码，在任何主机上都能登录MySQL数据库。</p>
<p>1．进入mysql</p>
<blockquote>
<p>  [root@hadoop102 mysql-libs]# mysql -uroot -p000000</p>
</blockquote>
<p>2．显示数据库</p>
<blockquote>
<p>  mysql&gt;show databases;</p>
</blockquote>
<p>3．使用mysql数据库</p>
<blockquote>
<p>  mysql&gt;use mysql;</p>
</blockquote>
<p>4．展示mysql数据库中的所有表</p>
<blockquote>
<p>  mysql&gt;show tables;</p>
</blockquote>
<p>5．展示user表的结构</p>
<blockquote>
<p>  mysql&gt;desc user;</p>
</blockquote>
<p>6．查询user表</p>
<blockquote>
<p>  mysql&gt;select User, Host, Password from user;</p>
</blockquote>
<p>7．修改user表，把Host表内容修改为%</p>
<blockquote>
<p>  mysql&gt;update user set host=’%’ where host=’localhost’;</p>
</blockquote>
<p>8．删除root用户的其他host</p>
<blockquote>
<p>  mysql&gt;delete from user where Host=’hadoop102’;</p>
</blockquote>
<blockquote>
<p>  mysql&gt;delete from user where Host=’127.0.0.1’;</p>
</blockquote>
<blockquote>
<p>  mysql&gt;delete from user where Host=’::1’;</p>
</blockquote>
<p>9．刷新</p>
<blockquote>
<p>  mysql&gt;flush privileges;</p>
</blockquote>
<p>10．退出</p>
<blockquote>
<p>  mysql&gt;quit;</p>
</blockquote>
<h2 id="2-5-Hive元数据配置到MySql"><a href="#2-5-Hive元数据配置到MySql" class="headerlink" title="2.5 Hive元数据配置到MySql"></a>2.5 Hive元数据配置到MySql</h2><h3 id="2-5-1-驱动拷贝"><a href="#2-5-1-驱动拷贝" class="headerlink" title="2.5.1 驱动拷贝"></a>2.5.1 驱动拷贝</h3><p>1．在/opt/software/mysql-libs目录下解压mysql-connector-java-5.1.27.tar.gz驱动包</p>
<blockquote>
<p>[root@hadoop102 mysql-libs]# tar -zxvf mysql-connector-java-5.1.27.tar.gz</p>
</blockquote>
<p>2．拷贝/opt/software/mysql-libs/mysql-connector-java-5.1.27目录下的mysql-connector-java-5.1.27-bin.jar到/opt/module/hive/lib/</p>
<blockquote>
<p>[root@hadoop102 mysql-connector-java-5.1.27]# </p>
<p>cp mysql-connector-java-5.1.27-bin.jar /opt/module/hive/lib/</p>
</blockquote>
<h3 id="2-5-2-配置Metastore到MySql"><a href="#2-5-2-配置Metastore到MySql" class="headerlink" title="2.5.2 配置Metastore到MySql"></a>2.5.2 配置Metastore到MySql</h3><p>1．在/opt/module/hive/conf目录下创建一个hive-site.xml</p>
<blockquote>
<p>  [xing@hadoop102 conf]$ touch hive-site.xml</p>
</blockquote>
<blockquote>
<p>  [xing@hadoop102 conf]$ vi hive-site.xml</p>
</blockquote>
<p>2．根据官方文档配置参数，拷贝数据到hive-site.xml文件中</p>
<p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin">https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin</a></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">value</span>&gt;</span>000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>3．配置完毕后，如果启动hive异常，可以重新启动虚拟机。（重启后，别忘了启动hadoop集群）</p>
<h3 id="2-5-3-多窗口启动Hive测试"><a href="#2-5-3-多窗口启动Hive测试" class="headerlink" title="2.5.3 多窗口启动Hive测试"></a>2.5.3 多窗口启动Hive测试</h3><p>1．先启动MySQL</p>
<blockquote>
<p>  [xing@hadoop102 mysql-libs]$ mysql -uroot -p000000</p>
</blockquote>
<p>查看有几个数据库</p>
<blockquote>
<p>  mysql&gt; show databases;</p>
<p>  +——————–+</p>
<p>  | Database      |</p>
<p>  +——————–+</p>
<p>  | information_schema |</p>
<p>  | mysql       |</p>
<p>  | performance_schema |</p>
<p>  | test        |</p>
<p>  +——————–+</p>
</blockquote>
<p>2．再次打开多个窗口，分别启动hive</p>
<blockquote>
<p>  [xing@hadoop102 hive]$ bin/hive</p>
</blockquote>
<p>3．启动hive后，回到MySQL窗口查看数据库，显示增加了metastore数据库</p>
<blockquote>
<p>  mysql&gt; show databases;</p>
<p>  +——————–+</p>
<p>  | Database      |</p>
<p>  +——————–+</p>
<p>  | information_schema |</p>
<p>  | metastore     |</p>
<p>  | mysql       |</p>
<p>  | performance_schema |</p>
<p>  | test        |</p>
<p>  +——————–+</p>
</blockquote>
<h2 id="2-6-HiveJDBC访问"><a href="#2-6-HiveJDBC访问" class="headerlink" title="2.6 HiveJDBC访问"></a>2.6 HiveJDBC访问</h2><h3 id="2-6-1-启动hiveserver2服务"><a href="#2-6-1-启动hiveserver2服务" class="headerlink" title="2.6.1 启动hiveserver2服务"></a>2.6.1 启动hiveserver2服务</h3><blockquote>
<p>  [xing@hadoop102 hive]$ bin/hiveserver2</p>
</blockquote>
<h3 id="2-6-2-启动beeline"><a href="#2-6-2-启动beeline" class="headerlink" title="2.6.2 启动beeline"></a>2.6.2 启动beeline</h3><blockquote>
<p>  [xing@hadoop102 hive]$ bin/beeline</p>
<p>  Beeline version 1.2.1 by Apache Hive</p>
<p>  beeline&gt;</p>
</blockquote>
<h3 id="2-6-3-连接hiveserver2"><a href="#2-6-3-连接hiveserver2" class="headerlink" title="2.6.3 连接hiveserver2"></a>2.6.3 连接hiveserver2</h3><blockquote>
<p>  beeline&gt; !connect jdbc:hive2://hadoop102:10000（回车）</p>
<p>  Connecting to jdbc:hive2://hadoop102:10000</p>
<p>  Enter username for jdbc:hive2://hadoop102:10000: xing（回车）</p>
<p>  Enter password for jdbc:hive2://hadoop102:10000: （直接回车）</p>
<p>  Connected to: Apache Hive (version 1.2.1)</p>
<p>  Driver: Hive JDBC (version 1.2.1)</p>
<p>  Transaction isolation: TRANSACTION_REPEATABLE_READ</p>
<p>  0: jdbc:hive2://hadoop102:10000&gt; show databases;</p>
<p>  +—————-+–+</p>
<p>  | database_name |</p>
<p>  +—————-+–+</p>
<p>  | default    |</p>
<p>  | hive_db2    |</p>
<p>  +—————-+–+</p>
</blockquote>
<h2 id="2-7-Hive常用交互命令"><a href="#2-7-Hive常用交互命令" class="headerlink" title="2.7 Hive常用交互命令"></a>2.7 Hive常用交互命令</h2><blockquote>
<p>[xing@hadoop102 hive]$ bin/hive -help  usage: hive  </p>
<p>-d,–define  &lt;key=value&gt;     Variable  subsitution to apply to hive                   </p>
<p>​                                             commands. e.g. -d A=B or  –define A=B      </p>
<p>–database <databasename>    Specify the database to use </p>
<p>-e  <quoted-query-string>     SQL  from command line   </p>
<p>-f  <filename>          SQL  from files   </p>
<p>-H,</p>
<p>​    –help            Print help information      </p>
<p>​    –hiveconf &lt;property=value&gt;   Use value for given property    </p>
<p>​    –hivevar  &lt;key=value&gt;     Variable  subsitution to apply to hive                   </p>
<p>​                                    commands.  e.g. –hivevar A=B  </p>
<p>-i  <filename>           Initialization SQL file   </p>
<p>-S,–silent           Silent mode in  interactive shell  </p>
<p>-v,–verbose           Verbose mode (echo  executed SQL to the console)  </p>
</blockquote>
<p>1．“-e”不进入hive的交互窗口执行sql语句</p>
<blockquote>
<p>  [xing@hadoop102 hive]$ bin/hive -e “select id from student;”</p>
</blockquote>
<p>2．“-f”执行脚本中sql语句</p>
<p>（1）在/opt/module/datas目录下创建hivef.sql文件</p>
<blockquote>
<p>  [xing@hadoop102 datas]$ touch hivef.sql</p>
</blockquote>
<p>文件中写入正确的sql语句</p>
<blockquote>
<p>select *from student;</p>
</blockquote>
<p>（2）执行文件中的sql语句</p>
<blockquote>
<p>  [xing@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql</p>
</blockquote>
<p>（3）执行文件中的sql语句并将结果写入文件中</p>
<blockquote>
<p>  [xing@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql &gt; /opt/module/datas/hive_result.txt</p>
</blockquote>
<h2 id="2-8-Hive其他命令操作"><a href="#2-8-Hive其他命令操作" class="headerlink" title="2.8 Hive其他命令操作"></a>2.8 Hive其他命令操作</h2><p>1．退出hive窗口：</p>
<blockquote>
<p>  hive(default)&gt;exit;</p>
</blockquote>
<blockquote>
<p>  hive(default)&gt;quit;</p>
</blockquote>
<blockquote>
<p>  在新版的hive中没区别了，在以前的版本是有的：</p>
</blockquote>
<blockquote>
<p>  exit:先隐性提交数据，再退出；</p>
</blockquote>
<blockquote>
<p>  quit:不提交数据，退出；</p>
</blockquote>
<p>2．在hive cli命令窗口中如何查看hdfs文件系统</p>
<blockquote>
<p>  hive(default)&gt;dfs -ls /;</p>
</blockquote>
<p>3．在hive cli命令窗口中如何查看本地文件系统</p>
<blockquote>
<p>  hive(default)&gt;! ls /opt/module/datas;</p>
</blockquote>
<p>4．查看在hive中输入的所有历史命令</p>
<p>（1）进入到当前用户的根目录/root或/home/xing</p>
<p>（2）查看. hivehistory文件</p>
<blockquote>
<p>  [xing@hadoop102 ~]$ cat .hivehistory</p>
</blockquote>
<h2 id="2-9-Hive常见属性配置"><a href="#2-9-Hive常见属性配置" class="headerlink" title="2.9 Hive常见属性配置"></a>2.9 Hive常见属性配置</h2><h3 id="2-9-1-Hive数据仓库位置配置"><a href="#2-9-1-Hive数据仓库位置配置" class="headerlink" title="2.9.1 Hive数据仓库位置配置"></a>2.9.1 Hive数据仓库位置配置</h3><p>1）Default数据仓库的最原始位置是在hdfs上的：/user/hive/warehouse路径下。</p>
<p>2）**==在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹。==**</p>
<p>3）修改default数据仓库原始位置（将hive-default.xml.template如下配置信息拷贝到hive-site.xml文件中）。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>


<p>配置同组用户有执行权限</p>
<blockquote>
<p>  bin/hdfs dfs -chmod g+w /user/hive/warehouse</p>
</blockquote>
<h3 id="2-9-2-查询后信息显示配置"><a href="#2-9-2-查询后信息显示配置" class="headerlink" title="2.9.2 查询后信息显示配置"></a>2.9.2 查询后信息显示配置</h3><p>1）在hive-site.xml文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<p>2）重新启动hive，对比配置前后差异。</p>
<p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113929.png" alt="image-20200917235726107"></p>
<h3 id="2-9-3-Hive运行日志信息配置"><a href="#2-9-3-Hive运行日志信息配置" class="headerlink" title="2.9.3 Hive运行日志信息配置"></a>2.9.3 Hive运行日志信息配置</h3><p>1．Hive的log默认存放在/tmp/xing/hive.log目录下（当前用户名下）</p>
<p>2．修改hive的log存放日志到/opt/module/hive/logs</p>
<p>（1）修改/opt/module/hive/conf/hive-log4j.properties.template文件名称为</p>
<p>hive-log4j.properties</p>
<blockquote>
<p>  [xing@hadoop102 conf]$ pwd</p>
</blockquote>
<blockquote>
<p>  /opt/module/hive/conf</p>
</blockquote>
<blockquote>
<p>  [xing@hadoop102 conf]$ mv hive-log4j.properties.template<br>  hive-log4j.properties</p>
</blockquote>
<p>（2）在hive-log4j.properties文件中修改log存放位置</p>
<blockquote>
<p>  hive.log.dir=/opt/module/hive/logs</p>
</blockquote>
<h3 id="2-9-4-参数配置方式"><a href="#2-9-4-参数配置方式" class="headerlink" title="2.9.4 参数配置方式"></a>2.9.4 参数配置方式</h3><p>1．查看当前所有的配置信息</p>
<blockquote>
<p>  hive&gt;set;</p>
</blockquote>
<p>2．参数的配置三种方式</p>
<p>（1）配置文件方式</p>
<blockquote>
<p>  默认配置文件：hive-default.xml</p>
</blockquote>
<blockquote>
<p>  用户自定义配置文件：hive-site.xml</p>
</blockquote>
<blockquote>
<p>  注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。</p>
</blockquote>
<p>（2）命令行参数方式</p>
<blockquote>
<p>  启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。</p>
</blockquote>
<blockquote>
<p>  例如：</p>
</blockquote>
<blockquote>
<p>  [xing@hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;</p>
</blockquote>
<blockquote>
<p>  注意：仅对本次hive启动有效</p>
</blockquote>
<blockquote>
<p>  查看参数设置：</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; set mapred.reduce.tasks;</p>
</blockquote>
<p>（3）参数声明方式</p>
<blockquote>
<p>  可以在HQL中使用SET关键字设定参数</p>
</blockquote>
<blockquote>
<p>  例如：</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; set mapred.reduce.tasks=100;</p>
</blockquote>
<blockquote>
<p>  注意：仅对本次hive启动有效。</p>
</blockquote>
<blockquote>
<p>  查看参数设置</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; set mapred.reduce.tasks;</p>
</blockquote>
<p>上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</p>
<h1 id="第3章-Hive数据类型"><a href="#第3章-Hive数据类型" class="headerlink" title="第3章 Hive数据类型"></a>第3章 Hive数据类型</h1><h2 id="3-1-基本数据类型"><a href="#3-1-基本数据类型" class="headerlink" title="3.1 基本数据类型"></a>3.1 基本数据类型</h2><table>
<thead>
<tr>
<th>Hive数据类型</th>
<th>Java数据类型</th>
<th>长度</th>
<th>例子</th>
</tr>
</thead>
<tbody><tr>
<td>TINYINT</td>
<td>byte</td>
<td>1byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>SMALINT</td>
<td>short</td>
<td>2byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>INT</td>
<td>int</td>
<td>4byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BIGINT</td>
<td>long</td>
<td>8byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>boolean</td>
<td>布尔类型，true或者false</td>
<td>TRUE FALSE</td>
</tr>
<tr>
<td>FLOAT</td>
<td>float</td>
<td>单精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>double</td>
<td>双精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>STRING</td>
<td>string</td>
<td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td>
<td>‘now is the time’ “for all good men”</td>
</tr>
<tr>
<td>TIMESTAMP</td>
<td></td>
<td>时间类型</td>
<td></td>
</tr>
<tr>
<td>BINARY</td>
<td></td>
<td>字节数组</td>
<td></td>
</tr>
</tbody></table>
<p>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</p>
<h2 id="3-2-集合数据类型"><a href="#3-2-集合数据类型" class="headerlink" title="3.2 集合数据类型"></a>3.2 集合数据类型</h2><table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
<th>语法示例</th>
</tr>
</thead>
<tbody><tr>
<td>STRUCT</td>
<td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td>
<td>struct()</td>
</tr>
<tr>
<td>MAP</td>
<td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td>
<td>map()</td>
</tr>
<tr>
<td>ARRAY</td>
<td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td>
<td>Array()</td>
</tr>
</tbody></table>
<p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p>
<p>案例实操</p>
<ol>
<li>假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问的格式为</li>
</ol>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;songsong&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;friends&quot;</span>: [<span class="string">&quot;bingbing&quot;</span> , <span class="string">&quot;lili&quot;</span>] ,       <span class="comment">//列表Array, </span></span><br><span class="line">    <span class="attr">&quot;children&quot;</span>: &#123;                      <span class="comment">//键值Map,</span></span><br><span class="line">        <span class="attr">&quot;xiao song&quot;</span>: <span class="number">18</span> ,</span><br><span class="line">        <span class="attr">&quot;xiaoxiao song&quot;</span>: <span class="number">19</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">&quot;address&quot;</span>: &#123;                      <span class="comment">//结构Struct,</span></span><br><span class="line">        <span class="attr">&quot;street&quot;</span>: <span class="string">&quot;hui long guan&quot;</span> ,</span><br><span class="line">        <span class="attr">&quot;city&quot;</span>: <span class="string">&quot;beijing&quot;</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>2）基于上述数据结构，我们在Hive里创建对应的表，并导入数据。</p>
<p>创建本地测试文件test.txt</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</span><br></pre></td></tr></table></figure>

<p><strong>==注意==：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。</strong></p>
<p>3）Hive上创建测试表test</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">test</span>(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">friends <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">children <span class="keyword">map</span>&lt;<span class="keyword">string</span>, <span class="built_in">int</span>&gt;,</span><br><span class="line">address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>, city:<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;_&#x27;</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;:&#x27;</span></span><br><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<blockquote>
<p>字段解释</p>
</blockquote>
<blockquote>
<p>  row format delimited fields terminated by ‘,’ – 列分隔符</p>
</blockquote>
<blockquote>
<p>  collection items terminated by ‘_’ –MAP STRUCT 和 ARRAY的分隔符(数据分割符号)</p>
</blockquote>
<blockquote>
<p>  map keys terminated by ‘:’ – MAP中的key与value的分隔符</p>
</blockquote>
<blockquote>
<p>  lines terminated by ‘\n’; – 行分隔符</p>
</blockquote>
<p>4）导入文本数据到测试表</p>
<blockquote>
<p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/test.txt’ into table test</p>
</blockquote>
<p>5）访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式</p>
<blockquote>
<p>hive (default)&gt; select friends[1],children[‘xiao song’],address.city from test</p>
<p>where name=”songsong”;</p>
<p>OK</p>
<p>_c0   _c1   city</p>
<p>lili  18   beijing</p>
<p>Time taken: 0.076 seconds, Fetched: 1 row(s)</p>
</blockquote>
<h2 id="3-3-类型转化"><a href="#3-3-类型转化" class="headerlink" title="3.3 类型转化"></a>3.3 类型转化</h2><p>Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。</p>
<p>1．隐式类型转换规则如下</p>
<p>（1）任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换BIGINT。</p>
<p>（2）所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。</p>
<p>（3）TINYINT、SMALLINT、INT都可以转换为FLOAT。</p>
<p>（4）BOOLEAN类型不可以转换为任何其它的类型。</p>
<p>2．可以使用CAST操作显示进行数据类型转换</p>
<p>例如CAST(‘1’ AS INT)将把字符串’1’转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。</p>
<h1 id="第4章-DDL数据定义"><a href="#第4章-DDL数据定义" class="headerlink" title="第4章 DDL数据定义"></a>第4章 DDL数据定义</h1><h2 id="4-1-创建数据库"><a href="#4-1-创建数据库" class="headerlink" title="4.1 创建数据库"></a>4.1 创建数据库</h2><p>1）创建一个数据库，数据库在HDFS上的默认存储路径是/user/hive/warehouse/*.db。</p>
<blockquote>
<p>hive (default)&gt; create database db_hive;</p>
</blockquote>
<p>2）避免要创建的数据库已经存在错误，增加if not exists判断。（标准写法）</p>
<blockquote>
<p>hive (default)&gt; create database db_hive;</p>
<p>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exists</p>
<p>hive (default)&gt; create database if not exists db_hive;</p>
</blockquote>
<p>3）创建一个数据库，指定数据库在HDFS上存放的位置</p>
<p>hive (default)&gt; create database db_hive2 location ‘/db_hive2.db’;</p>
<h2 id="4-2-查询数据库"><a href="#4-2-查询数据库" class="headerlink" title="4.2 查询数据库"></a>4.2 查询数据库</h2><h3 id="4-2-1-显示数据库"><a href="#4-2-1-显示数据库" class="headerlink" title="4.2.1 显示数据库"></a>4.2.1 显示数据库</h3><p>1．显示数据库</p>
<blockquote>
<p>  hive&gt; show databases;</p>
</blockquote>
<p>2．过滤显示查询的数据库</p>
<blockquote>
<p>  hive&gt; show databases like ‘db_hive*’;</p>
<p>  OK</p>
<p>  db_hive</p>
<p>  db_hive_1</p>
</blockquote>
<h3 id="4-2-2-查看数据库详情"><a href="#4-2-2-查看数据库详情" class="headerlink" title="4.2.2 查看数据库详情"></a>4.2.2 查看数据库详情</h3><p>1．显示数据库信息</p>
<blockquote>
<p>  hive&gt; desc database db_hive;</p>
<p>  OK</p>
<p>  db_hive hdfs://hadoop102:9000/user/hive/warehouse/db_hive.db xingUSER</p>
</blockquote>
<p>2．显示数据库详细信息，extended</p>
<blockquote>
<p>  hive&gt; desc database extended db_hive;</p>
<p>  OK</p>
<p>  db_hive   hdfs://hadoop102:9000/user/hive/warehouse/db_hive.db xingUSER </p>
<p>  40.3.3 切换当前数据库</p>
<p>  hive (default)&gt; use db_hive;</p>
</blockquote>
<h3 id="4-3-3-切换当前数据库"><a href="#4-3-3-切换当前数据库" class="headerlink" title="4.3.3 切换当前数据库"></a>4.3.3 切换当前数据库</h3><blockquote>
<p>  hive (default)&gt; use db_hive;</p>
</blockquote>
<h2 id="4-3-修改数据库"><a href="#4-3-修改数据库" class="headerlink" title="4.3 修改数据库"></a>4.3 修改数据库</h2><p>用户可以使用ALTERDATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。**==数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。==**</p>
<blockquote>
<p>  hive (default)&gt; alter database db_hive set dbproperties(‘createtime’=’20170830’);</p>
</blockquote>
<p>在hive中查看修改结果</p>
<blockquote>
<p>  hive&gt; desc database extended db_hive;<br>db_name comment location owner_name owner_type parameters<br> db_hive hdfs://hadoop102:8020/user/hive/warehouse/db_hive.db xing USER{createtime=20170830}</p>
</blockquote>
<h2 id="4-4-删除数据库"><a href="#4-4-删除数据库" class="headerlink" title="4.4 删除数据库"></a>4.4 删除数据库</h2><p>1．删除空数据库</p>
<blockquote>
<p>  hive&gt;drop database db_hive2;</p>
</blockquote>
<p>2．如果删除的数据库不存在，最好采用 if exists判断数据库是否存在</p>
<blockquote>
<p>  hive&gt; drop database db_hive;</p>
</blockquote>
<blockquote>
<p>  FAILED: SemanticException [Error 10072]: Database does not exist: db_hive</p>
</blockquote>
<blockquote>
<p>  hive&gt; drop database if exists db_hive2;</p>
</blockquote>
<p>3．如果数据库不为空，可以采用cascade命令，强制删除</p>
<blockquote>
<p>  hive&gt; drop database db_hive;</p>
</blockquote>
<blockquote>
<p>  FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask.nvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)</p>
</blockquote>
<blockquote>
<p>  hive&gt; drop database db_hive cascade;</p>
</blockquote>
<h2 id="4-5-创建表"><a href="#4-5-创建表" class="headerlink" title="4.5 创建表"></a>4.5 创建表</h2><p>1．建表语法</p>
<blockquote>
<p>  CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name</p>
<p>  [(col_name data_type [COMMENT col_comment], …)] </p>
<p>  [COMMENT table_comment]</p>
<p>  [PARTITIONED BY (col_name data_type [COMMENT col_comment], …)] </p>
<p>  [CLUSTERED BY (col_name, col_name, …) </p>
<p>  [SORTED BY (col_name [ASC|DESC], …)] INTO num_buckets BUCKETS] </p>
<p>  [ROW FORMAT row_format] </p>
<p>  [STORED AS file_format]</p>
<p>  [LOCATION hdfs_path]</p>
</blockquote>
<p>2．字段解释说明</p>
<blockquote>
<p>  （1）CREATE TABLE<br>  创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IFNOT EXISTS 选项来忽略这个异常。</p>
</blockquote>
<blockquote>
<p>  （2）EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</p>
</blockquote>
<blockquote>
<p>（3）COMMENT：为表和列添加注释。</p>
</blockquote>
<blockquote>
<p>（4）PARTITIONED BY创建分区表</p>
</blockquote>
<blockquote>
<p>（5）CLUSTERED BY创建分桶表</p>
</blockquote>
<blockquote>
<p>（6）SORTED BY不常用</p>
</blockquote>
<blockquote>
<p>  （7）ROW FORMAT</p>
</blockquote>
<blockquote>
<p>  DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char]</p>
</blockquote>
<blockquote>
<p>  [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]</p>
</blockquote>
<blockquote>
<p>  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value,<br>  property_name=property_value, …)]</p>
</blockquote>
<blockquote>
<p>用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT<br>或者ROW FORMAT<br>DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。</p>
<p>SerDe是Serialize/Deserilize的简称，目的是用于序列化和反序列化。</p>
</blockquote>
<blockquote>
<p>（8）STORED AS指定存储文件类型</p>
</blockquote>
<blockquote>
<p>  常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）</p>
</blockquote>
<blockquote>
<p>  如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用STORED AS SEQUENCEFILE。</p>
</blockquote>
<blockquote>
<p>（9）LOCATION ：指定表在HDFS上的存储位置。</p>
</blockquote>
<blockquote>
<p>（10）LIKE允许用户复制现有的表结构，但是不复制数据。</p>
</blockquote>
<h3 id="4-5-1-管理表"><a href="#4-5-1-管理表" class="headerlink" title="4.5.1 管理表"></a>4.5.1 管理表</h3><p>1．理论</p>
<p>默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置hive.metastore.warehouse.dir(如，/user/hive/warehouse)所定义的目录的子目录下。**==当我们删除一个管理表时，Hive也会删除这个表中数据==**。管理表不适合和其他工具共享数据。</p>
<p>2．案例实操</p>
<p>（1）普通创建表</p>
<blockquote>
<p>create table if not exists student2(</p>
<p>​    id int,</p>
<p>​    name string</p>
<p>)</p>
<p>row format delimited fields terminated by ‘\t’</p>
<p>stored as textfile</p>
<p>location ‘/user/hive/warehouse/student2’;</p>
</blockquote>
<p>（2）根据查询结果创建表（查询的结果会添加到新创建的表中）</p>
<blockquote>
<p>create table if not exists student3 as select id, name from student;</p>
</blockquote>
<p>（3）根据已经存在的表结构创建表</p>
<blockquote>
<p>create  table if not exists student4 like student;    </p>
</blockquote>
<p>（4）查询表的类型</p>
<blockquote>
<p>  hive (default)&gt; desc formatted student2;</p>
</blockquote>
<blockquote>
<p>  Table Type: MANAGED_TABLE</p>
</blockquote>
<h3 id="4-5-2-外部表"><a href="#4-5-2-外部表" class="headerlink" title="4.5.2 外部表"></a>4.5.2 外部表</h3><p>1．理论</p>
<p>因为表是外部表，所以Hive并非认为其完全拥有这份数据。**==删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。==**</p>
<p>2．管理表和外部表的使用场景</p>
<p>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p>
<p>3．案例实操</p>
<p>分别创建部门和员工外部表，并向表中导入数据。</p>
<p>（1）原始数据 </p>
<p>​    <a href="./dept.txt">dept.txt</a></p>
<p>​    <a href="./emp.txt">emp.txt</a></p>
<p>（2）建表语句</p>
<p>创建部门表</p>
<blockquote>
<p>create external table if not exists default.dept( </p>
<p>deptno int,</p>
<p>dname string, </p>
<p>loc int </p>
<p>) </p>
<p>row format delimited fields terminated by ‘\t’; </p>
</blockquote>
<p>创建员工表</p>
<blockquote>
<p>create external table if not exists default.emp( </p>
<p>empno int,</p>
<p> ename string, </p>
<p>job string,</p>
<p> mgr int, </p>
<p>hiredate string,  </p>
<p>sal double, </p>
<p> comm double, </p>
<p>deptno int</p>
<p>) </p>
<p>row format delimited fields terminated by ‘\t’; |</p>
</blockquote>
<p>（3）查看创建的表</p>
<blockquote>
<p>  hive (default)&gt; show tables;</p>
<p>  OK</p>
<p>  tab_name</p>
<p>  dept</p>
<p>  emp</p>
</blockquote>
<p>（4）向外部表中导入数据</p>
<p>导入数据</p>
<blockquote>
<p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/dept.txt’ into table default.dept;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/emp.txt’ into table default.emp;</p>
</blockquote>
<p>查询结果</p>
<blockquote>
<p>  hive (default)&gt; select * from emp;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; select * from dept;</p>
</blockquote>
<p>（5）查看表格式化数据</p>
<blockquote>
<p>  hive (default)&gt; desc formatted dept;</p>
</blockquote>
<blockquote>
<p>  Table Type: EXTERNAL_TABLE</p>
</blockquote>
<h3 id="4-5-3-管理表与外部表的互相转换"><a href="#4-5-3-管理表与外部表的互相转换" class="headerlink" title="4.5.3 管理表与外部表的互相转换"></a>4.5.3 管理表与外部表的互相转换</h3><p>（1）查询表的类型</p>
<blockquote>
<p>  hive (default)&gt; desc formatted student2;</p>
</blockquote>
<blockquote>
<p>  Table Type: MANAGED_TABLE</p>
</blockquote>
<p>（2）修改内部表student2为外部表</p>
<blockquote>
<p>  alter table student2 set tblproperties(‘EXTERNAL’=’TRUE’);</p>
</blockquote>
<p>（3）查询表的类型</p>
<blockquote>
<p>  hive (default)&gt; desc formatted student2;</p>
</blockquote>
<blockquote>
<p>  Table Type: EXTERNAL_TABLE</p>
</blockquote>
<p>（4）修改外部表student2为内部表</p>
<blockquote>
<p>  alter table student2 set tblproperties(‘EXTERNAL’=’FALSE’);</p>
</blockquote>
<p>（5）查询表的类型</p>
<blockquote>
<p>  hive (default)&gt; desc formatted student2;</p>
</blockquote>
<blockquote>
<p>  Table Type: MANAGED_TABLE</p>
</blockquote>
<p><strong>==注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！==</strong></p>
<h2 id="4-6-分区表"><a href="#4-6-分区表" class="headerlink" title="4.6 分区表"></a>4.6 <a name="4.6">分区表</a></h2><p>分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。**==Hive中的分区就是分目录==**，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。</p>
<h3 id="4-6-1-分区表基本操作"><a href="#4-6-1-分区表基本操作" class="headerlink" title="4.6.1 分区表基本操作"></a>4.6.1 分区表基本操作</h3><p>1．引入分区表（需要根据日期对日志进行管理）</p>
<blockquote>
<p>  /user/hive/warehouse/log_partition/20170702/20170702.log</p>
</blockquote>
<blockquote>
<p>  /user/hive/warehouse/log_partition/20170703/20170703.log</p>
</blockquote>
<blockquote>
<p>  /user/hive/warehouse/log_partition/20170704/20170704.log</p>
</blockquote>
<p>2．创建分区表语法</p>
<blockquote>
<p>hive (default)&gt; create table dept_partition(</p>
<p>deptno int, </p>
<p>dname string, </p>
<p>loc string</p>
<p>)</p>
<p>partitioned by (month string)</p>
<p>row format delimited fields terminated by ‘\t’;</p>
</blockquote>
<p>3．加载数据到分区表中</p>
<blockquote>
<p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/dept.txt’ into table default.dept_partition partition(month=’201709’);</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/dept.txt’ into table default.dept_partition partition(month=’201708’);</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/dept.txt’ into table default.dept_partition partition(month=’201707’);</p>
</blockquote>
<p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113930.png" alt="image-20200918001733383"></p>
<p>4．查询分区表中数据</p>
<p>单分区查询</p>
<blockquote>
<p>  hive (default)&gt; select * from dept_partition where month=’201709’;</p>
</blockquote>
<blockquote>
<p>  多分区联合查询</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; select * from dept_partition where month=’201709’</p>
<p>  ​       union</p>
<p>  ​       select * from dept_partition where month=’201708’</p>
<p>  ​       union</p>
<p>  ​       select * from dept_partition where month=’201707’;</p>
<p>  _u3.deptno   _u3.dname    _u3.loc _u3.month</p>
<p>  10   ACCOUNTING   NEW YORK    201707</p>
<p>  10   ACCOUNTING   NEW YORK    201708</p>
<p>  10   ACCOUNTING   NEW YORK    201709</p>
<p>  20   RESEARCH    DALLAS 201707</p>
<p>  20   RESEARCH    DALLAS 201708</p>
<p>  20   RESEARCH    DALLAS 201709</p>
<p>  30   SALES  CHICAGO 201707</p>
<p>  30   SALES  CHICAGO 201708</p>
<p>  30   SALES  CHICAGO 201709</p>
<p>  40   OPERATIONS   BOSTON 201707</p>
<p>  40   OPERATIONS   BOSTON 201708</p>
<p>  40   OPERATIONS   BOSTON 201709</p>
</blockquote>
<p>5．增加分区</p>
<p>创建单个分区</p>
<blockquote>
<p>  hive (default)&gt; alter table dept_partition add partition(month=’201706’) ;</p>
</blockquote>
<p>同时创建多个分区</p>
<blockquote>
<p>  hive (default)&gt; alter table dept_partition add partition(month=’201705’) partition(month=’201704’);</p>
</blockquote>
<p>6．删除分区</p>
<p>删除单个分区</p>
<blockquote>
<p>  hive (default)&gt; alter table dept_partition drop partition (month=’201704’);</p>
</blockquote>
<p>同时删除多个分区</p>
<blockquote>
<p>  hive (default)&gt; alter table dept_partition drop partition (month=’201705’), partition (month=’201706’);</p>
</blockquote>
<p>7．查看分区表有多少分区</p>
<blockquote>
<p>  hive&gt; show partitions dept_partition;</p>
</blockquote>
<p>8．查看分区表结构</p>
<blockquote>
<p>  hive&gt; desc formatted dept_partition;</p>
</blockquote>
<blockquote>
<p>  # Partition Information</p>
</blockquote>
<blockquote>
<p>  # col_name data_type comment</p>
</blockquote>
<blockquote>
<p>  month string</p>
</blockquote>
<h3 id="4-6-2-分区表注意事项"><a href="#4-6-2-分区表注意事项" class="headerlink" title="4.6.2 分区表注意事项"></a>4.6.2 分区表注意事项</h3><p>1．创建二级分区表</p>
<blockquote>
<p>hive (default)&gt; create table dept_partition2(</p>
<p>​        deptno int, dname string, loc string</p>
<p>)</p>
<p>partitioned by (month string, day string)</p>
<p>row format delimited fields terminated by ‘\t’;</p>
</blockquote>
<p>2．正常的加载数据</p>
<p>（1）加载数据到二级分区表中</p>
<blockquote>
<p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/dept.txt’ into table default.dept_partition2 partition(month=’201709’, day=’13’);</p>
</blockquote>
<p>（2）查询分区数据</p>
<blockquote>
<p>  hive (default)&gt; select * from dept_partition2 where month=’201709’ and day=’13’;</p>
</blockquote>
<p>3．把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式</p>
<p>（1）方式一：上传数据后修复</p>
<p>上传数据</p>
<blockquote>
<p>  hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=12;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=12;</p>
</blockquote>
<p>查询数据（查询不到刚上传的数据）</p>
<blockquote>
<p>  hive (default)&gt; select * from dept_partition2 where month=’201709’ and day=’12’;</p>
</blockquote>
<p>执行修复命令</p>
<blockquote>
<p>  hive&gt; msck repair table dept_partition2;</p>
</blockquote>
<p>再次查询数据</p>
<blockquote>
<p>  hive (default)&gt; select * from dept_partition2 where month=’201709’ and day=’12’;</p>
</blockquote>
<p>（2）方式二：上传数据后添加分区</p>
<blockquote>
<p>  上传数据</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=11;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=11;</p>
</blockquote>
<blockquote>
<p>  执行添加分区</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; alter table dept_partition2 add partition(month=’201709’,day=’11’);</p>
</blockquote>
<blockquote>
<p>  查询数据</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; select * from dept_partition2 where month=’201709’ and day=’11’;</p>
</blockquote>
<p>（3）方式三：创建文件夹后load数据到分区</p>
<p>创建目录</p>
<blockquote>
<p>  hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=10;</p>
</blockquote>
<p>上传数据</p>
<blockquote>
<p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/dept.txt’ into table dept_partition2 partition(month=’201709’,day=’10’);</p>
</blockquote>
<p>查询数据</p>
<blockquote>
<p>  hive (default)&gt; select * from dept_partition2 where month=’201709’ and day=’10’;</p>
</blockquote>
<h2 id="4-7-修改表"><a href="#4-7-修改表" class="headerlink" title="4.7 修改表"></a>4.7 修改表</h2><h3 id="4-7-1-重命名表"><a href="#4-7-1-重命名表" class="headerlink" title="4.7.1 重命名表"></a>4.7.1 重命名表</h3><p>1．语法</p>
<blockquote>
<p>  ALTER TABLE table_name RENAME TO new_table_name</p>
</blockquote>
<p>2．实操案例</p>
<blockquote>
<p>  hive (default)&gt; alter table dept_partition2 rename to dept_partition3;</p>
</blockquote>
<h3 id="4-7-2-增加、修改和删除表分区"><a href="#4-7-2-增加、修改和删除表分区" class="headerlink" title="4.7.2 增加、修改和删除表分区"></a>4.7.2 增加、修改和删除表分区</h3><p>详见4.6.1分区表基本操作。</p>
<h3 id="4-7-3-增加-修改-替换列信息"><a href="#4-7-3-增加-修改-替换列信息" class="headerlink" title="4.7.3 增加/修改/替换列信息"></a>4.7.3 增加/修改/替换列信息</h3><p>1．语法</p>
<p>更新列</p>
<blockquote>
<p>  ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type<br>  [COMMENT col_comment] [FIRST|AFTER column_name]</p>
</blockquote>
<p>增加和替换列</p>
<blockquote>
<p>  ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT<br>  col_comment], …)</p>
</blockquote>
<p>注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。</p>
<p>2．实操案例</p>
<p>（1）查询表结构</p>
<blockquote>
<p>  hive&gt; desc dept_partition;</p>
</blockquote>
<p>（2）添加列</p>
<blockquote>
<p>  hive (default)&gt; alter table dept_partition add columns(deptdesc string);</p>
</blockquote>
<p>（3）查询表结构</p>
<blockquote>
<p>  hive&gt; desc dept_partition;</p>
</blockquote>
<p>（4）更新列</p>
<blockquote>
<p>  hive (default)&gt; alter table dept_partition change column deptdesc desc int;</p>
</blockquote>
<p>（5）查询表结构</p>
<blockquote>
<p>  hive&gt; desc dept_partition;</p>
</blockquote>
<p>（6）替换列</p>
<blockquote>
<p>  hive (default)&gt; alter table dept_partition replace columns(deptno string,dname string, loc string);</p>
</blockquote>
<p>（7）查询表结构</p>
<blockquote>
<p>  hive&gt; desc dept_partition;</p>
</blockquote>
<h2 id="4-8-删除表"><a href="#4-8-删除表" class="headerlink" title="4.8 删除表"></a>4.8 删除表</h2><blockquote>
<p>  hive (default)&gt; drop table dept_partition;</p>
</blockquote>
<h1 id="第5章-DML数据操作"><a href="#第5章-DML数据操作" class="headerlink" title="第5章 DML数据操作"></a>第5章 DML数据操作</h1><h2 id="5-1-数据导入"><a href="#5-1-数据导入" class="headerlink" title="5.1 数据导入"></a>5.1 数据导入</h2><h3 id="5-1-1-向表中装载数据（Load）"><a href="#5-1-1-向表中装载数据（Load）" class="headerlink" title="5.1.1 向表中装载数据（Load）"></a>5.1.1 向表中装载数据（Load）</h3><p>1．语法</p>
<blockquote>
<p>  hive&gt; load data [local] inpath ‘/opt/module/datas/student.txt’ overwrite |<br>  into table student [partition (partcol1=val1,…)];</p>
</blockquote>
<blockquote>
<p>  （1）load data:表示加载数据</p>
</blockquote>
<blockquote>
<p>  （2）local:表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</p>
</blockquote>
<blockquote>
<p>  （3）inpath:表示加载数据的路径</p>
</blockquote>
<blockquote>
<p>  （4）overwrite:表示覆盖表中已有数据，否则表示追加</p>
</blockquote>
<blockquote>
<p>  （5）into table:表示加载到哪张表</p>
</blockquote>
<blockquote>
<p>  （6）student:表示具体的表</p>
</blockquote>
<blockquote>
<p>  （7）partition:表示上传到指定分区</p>
</blockquote>
<p>2．实操案例</p>
<p>（0）创建一张表</p>
<blockquote>
<p>  hive (default)&gt; create table student(id string, name string) row format<br>  delimited fields terminated by ‘\t’;</p>
</blockquote>
<p>（1）加载本地文件到hive</p>
<blockquote>
<p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/student.txt’ into table default.student;</p>
</blockquote>
<p>（2）加载HDFS文件到hive中</p>
<p>上传文件到HDFS</p>
<blockquote>
<p>  hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/xing/hive;</p>
</blockquote>
<p>加载HDFS上数据</p>
<blockquote>
<p>  hive (default)&gt; load data inpath ‘/user/xing/hive/student.txt’ into table default.student;</p>
</blockquote>
<p>（3）加载数据覆盖表中已有的数据</p>
<p>上传文件到HDFS</p>
<blockquote>
<p>  hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/xing/hive;</p>
</blockquote>
<p>加载数据覆盖表中已有的数据</p>
<blockquote>
<p>  hive (default)&gt; load data inpath ‘/user/xing/hive/student.txt’ overwrite into table default.student;</p>
</blockquote>
<h3 id="5-1-2-通过查询语句向表中插入数据（Insert）"><a href="#5-1-2-通过查询语句向表中插入数据（Insert）" class="headerlink" title="5.1.2 通过查询语句向表中插入数据（Insert）"></a>5.1.2 通过查询语句向表中插入数据（Insert）</h3><p>1．创建一张分区表</p>
<blockquote>
<p>  hive (default)&gt; create table student(id int, name string) partitioned by (month string) row format delimited fields terminated by ‘\t’;</p>
</blockquote>
<blockquote>
<p>  2．基本插入数据</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; insert into table student partition(month=’201709’) values(1,’wangwu’);</p>
</blockquote>
<p>3．基本模式插入（根据单张表查询结果）</p>
<blockquote>
<p>  hive (default)&gt; insert overwrite table student partition(month=’201708’) select id, name from student where month=’201709’;</p>
</blockquote>
<p>4．多插入模式（根据多张表查询结果）</p>
<blockquote>
<p>  hive (default)&gt; from student insert overwrite table student partition(month=’201707’) select id, name where month=’201709’ insert overwrite table student partition(month=’201706’) select id, name where month=’201709’;</p>
</blockquote>
<h3 id="5-1-3-查询语句中创建表并加载数据（As-Select）"><a href="#5-1-3-查询语句中创建表并加载数据（As-Select）" class="headerlink" title="5.1.3 查询语句中创建表并加载数据（As Select）"></a>5.1.3 查询语句中创建表并加载数据（As Select）</h3><p>详见4.5.1章创建表。</p>
<p>根据查询结果创建表（查询的结果会添加到新创建的表中）</p>
<blockquote>
<p>  create table if not exists student3 as select id, name from student;</p>
</blockquote>
<h3 id="5-1-4-创建表时通过Location指定加载数据路径"><a href="#5-1-4-创建表时通过Location指定加载数据路径" class="headerlink" title="5.1.4 创建表时通过Location指定加载数据路径"></a>5.1.4 创建表时通过Location指定加载数据路径</h3><p>1．创建表，并指定在hdfs上的位置</p>
<blockquote>
<p>  hive (default)&gt; create table if not exists student5(</p>
<p>  ​       id int, name string</p>
<p>  )</p>
<p>   row format delimited fields terminated by ‘\t’</p>
<pre><code>location &#39;/user/hive/warehouse/student5&#39;;</code></pre>
</blockquote>
<p>2．上传数据到hdfs上</p>
<blockquote>
<p>  hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/hive/warehouse/student5;</p>
</blockquote>
<p>3．查询数据</p>
<blockquote>
<p>  hive (default)&gt; select * from student5;</p>
</blockquote>
<h3 id="5-1-5-Import数据到指定Hive表中"><a href="#5-1-5-Import数据到指定Hive表中" class="headerlink" title="5.1.5 Import数据到指定Hive表中"></a>5.1.5 Import数据到指定Hive表中</h3><p><strong>==注意：先用export导出后，再将数据导入。==</strong></p>
<blockquote>
<p>  hive (default)&gt; import table student2 partition(month=’201709’) from ‘/user/hive/warehouse/export/student’;</p>
</blockquote>
<h2 id="5-2-数据导出"><a href="#5-2-数据导出" class="headerlink" title="5.2 数据导出"></a>5.2 数据导出</h2><h3 id="5-2-1-Insert导出"><a href="#5-2-1-Insert导出" class="headerlink" title="5.2.1 Insert导出"></a>5.2.1 Insert导出</h3><p>1．将查询的结果导出到本地</p>
<blockquote>
<p>  hive (default)&gt; insert overwrite local directory ‘/opt/module/datas/export/student’ select * from student;</p>
</blockquote>
<blockquote>
<p>  2．将查询的结果格式化导出到本地</p>
</blockquote>
<blockquote>
<p>  hive(default)&gt;insert overwrite local directory ‘/opt/module/datas/export/student1’ ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’ select * from student;</p>
</blockquote>
<p>3．将查询的结果导出到HDFS上(没有local)</p>
<blockquote>
<p>  hive (default)&gt; insert overwrite directory ‘/user/xing/student2’ ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’ select * from student;</p>
</blockquote>
<h3 id="5-2-2-Hadoop命令导出到本地"><a href="#5-2-2-Hadoop命令导出到本地" class="headerlink" title="5.2.2 Hadoop命令导出到本地"></a>5.2.2 Hadoop命令导出到本地</h3><blockquote>
<p>  hive (default)&gt; dfs -get /user/hive/warehouse/student/month=201709/000000_0 /opt/module/datas/export/student3.txt;</p>
</blockquote>
<h3 id="5-2-3-Hive-Shell-命令导出"><a href="#5-2-3-Hive-Shell-命令导出" class="headerlink" title="5.2.3 Hive Shell 命令导出"></a>5.2.3 Hive Shell 命令导出</h3><p>基本语法：（hive -f/-e 执行语句或者脚本 &gt; file）</p>
<blockquote>
<p>  [xing@hadoop102 hive]$ bin/hive -e ‘select * from default.student;’ &gt; /opt/module/datas/export/student4.txt;</p>
</blockquote>
<h3 id="5-2-4-Export导出到HDFS上"><a href="#5-2-4-Export导出到HDFS上" class="headerlink" title="5.2.4 Export导出到HDFS上"></a>5.2.4 Export导出到HDFS上</h3><blockquote>
<p>  (defahiveult)&gt; export table default.student to ‘/user/hive/warehouse/export/student’;</p>
</blockquote>
<h3 id="5-2-5-Sqoop导出"><a href="#5-2-5-Sqoop导出" class="headerlink" title="5.2.5 Sqoop导出"></a>5.2.5 Sqoop导出</h3><p>详见Sqooop</p>
<h2 id="5-3-清除表中数据（Truncate）"><a href="#5-3-清除表中数据（Truncate）" class="headerlink" title="5.3 清除表中数据（Truncate）"></a>5.3 清除表中数据（Truncate）</h2><p>注意：Truncate只能删除管理表，不能删除外部表中数据</p>
<blockquote>
<p>  hive (default)&gt; truncate table student;</p>
</blockquote>
<h1 id="第6章-查询"><a href="#第6章-查询" class="headerlink" title="第6章 查询"></a>第6章 查询</h1><p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select</a></p>
<p>查询语句语法：</p>
<blockquote>
<p>[WITH CommonTableExpression (, CommonTableExpression)*]  (Note: Only available</p>
<p>starting with Hive 0.13.0)</p>
<p>SELECT [ALL | DISTINCT] select_expr, select_expr, …</p>
<p>FROM table_reference</p>
<p>[WHERE where_condition]</p>
<p>[GROUP BY col_list]</p>
<p>[ORDER BY col_list]</p>
<p>[CLUSTER BY col_list</p>
<p> | [DISTRIBUTE BY col_list] [SORT BY col_list]</p>
<p>]</p>
<p>[LIMIT number]</p>
</blockquote>
<h2 id="6-1-基本查询（Select…From）"><a href="#6-1-基本查询（Select…From）" class="headerlink" title="6.1 基本查询（Select…From）"></a>6.1 基本查询（Select…From）</h2><h3 id="6-1-1-全表和特定列查询"><a href="#6-1-1-全表和特定列查询" class="headerlink" title="6.1.1 全表和特定列查询"></a>6.1.1 全表和特定列查询</h3><p>1．全表查询</p>
<blockquote>
<p>  hive (default)&gt; select * from emp;</p>
</blockquote>
<p>2．选择特定列查询</p>
<blockquote>
<p>  hive (default)&gt; select empno, ename from emp;</p>
</blockquote>
<blockquote>
<p>  注意：</p>
</blockquote>
<blockquote>
<p>  （1）SQL 语言**==大小写不敏感。==**</p>
</blockquote>
<blockquote>
<p>  （2）SQL 可以写在一行或者多行</p>
</blockquote>
<blockquote>
<p>  （3）**==关键字不能被缩写也不能分行==**</p>
</blockquote>
<blockquote>
<p>  （4）各子句一般要分行写。</p>
</blockquote>
<blockquote>
<p>  （5）使用缩进提高语句的可读性。</p>
</blockquote>
<h3 id="6-1-2-列别名"><a href="#6-1-2-列别名" class="headerlink" title="6.1.2 列别名"></a>6.1.2 列别名</h3><p>1．重命名一个列</p>
<p>2．便于计算</p>
<p>3．紧跟列名，**==也可以在列名和别名之间加入关键字‘AS’==**</p>
<p>4．案例实操</p>
<blockquote>
<p>  查询名称和部门</p>
<p>  hive (default)&gt; select ename AS name, deptno dn from emp;</p>
</blockquote>
<h3 id="6-1-3-算术运算符"><a href="#6-1-3-算术运算符" class="headerlink" title="6.1.3 算术运算符"></a>6.1.3 算术运算符</h3><table>
<thead>
<tr>
<th>运算符</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>A+B</td>
<td>A和B 相加</td>
</tr>
<tr>
<td>A-B</td>
<td>A减去B</td>
</tr>
<tr>
<td>A*B</td>
<td>A和B 相乘</td>
</tr>
<tr>
<td>A/B</td>
<td>A除以B</td>
</tr>
<tr>
<td>A%B</td>
<td>A对B取余</td>
</tr>
<tr>
<td>A&amp;B</td>
<td>A和B按位取与</td>
</tr>
<tr>
<td>A|B</td>
<td>A和B按位取或</td>
</tr>
<tr>
<td>A^B</td>
<td>A和B按位取异或</td>
</tr>
<tr>
<td>~A</td>
<td>A按位取反</td>
</tr>
</tbody></table>
<p>案例实操</p>
<p>查询出所有员工的薪水后加1显示。</p>
<blockquote>
<p>  hive (default)&gt; select sal +1 from emp;</p>
</blockquote>
<h3 id="6-1-4-常用函数"><a href="#6-1-4-常用函数" class="headerlink" title="6.1.4 常用函数"></a>6.1.4 常用函数</h3><p>1．求总行数（count）</p>
<blockquote>
<p>  hive (default)&gt; select count(*) cnt from emp;</p>
</blockquote>
<p>2．求工资的最大值（max）</p>
<blockquote>
<p>  hive (default)&gt; select max(sal) max_sal from emp;</p>
</blockquote>
<p>3．求工资的最小值（min）</p>
<blockquote>
<p>  hive (default)&gt; select min(sal) min_sal from emp;</p>
</blockquote>
<p>4．求工资的总和（sum）</p>
<blockquote>
<p>  hive (default)&gt; select sum(sal) sum_sal from emp;</p>
</blockquote>
<p>5．求工资的平均值（avg）</p>
<blockquote>
<p>  hive (default)&gt; select avg(sal) avg_sal from emp;</p>
</blockquote>
<h3 id="6-1-5-Limit语句"><a href="#6-1-5-Limit语句" class="headerlink" title="6.1.5 Limit语句"></a>6.1.5 Limit语句</h3><p>典型的查询会返回多行数据。LIMIT子句用于限制返回的行数。</p>
<blockquote>
<p>  hive (default)&gt; select * from emp limit 5;</p>
</blockquote>
<h2 id="6-2-Where语句"><a href="#6-2-Where语句" class="headerlink" title="6.2 Where语句"></a>6.2 Where语句</h2><p>1．使用WHERE子句，将不满足条件的行过滤掉</p>
<p>2．WHERE子句紧随FROM子句</p>
<p>3．案例实操</p>
<p>查询出薪水大于1000的所有员工</p>
<blockquote>
<p>  hive (default)&gt; select * from emp where sal &gt;1000;</p>
</blockquote>
<h3 id="6-2-1-比较运算符（Between-In-Is-Null）"><a href="#6-2-1-比较运算符（Between-In-Is-Null）" class="headerlink" title="6.2.1 比较运算符（Between/In/ Is Null）"></a>6.2.1 比较运算符（Between/In/ Is Null）</h3><p>1）下面表中描述了谓词操作符，这些操作符同样可以用于JOIN…ON和HAVING语句中。</p>
<p>表6-4</p>
<table>
<thead>
<tr>
<th>操作符</th>
<th>支持的数据类型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>A=B</td>
<td>基本数据类型</td>
<td>如果A等于B则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&lt;=&gt;B</td>
<td>基本数据类型</td>
<td>如果A和B都为NULL，则返回TRUE，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL</td>
</tr>
<tr>
<td>A&lt;&gt;B, A!=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&lt;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&lt;=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&gt;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&gt;=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A [NOT] BETWEEN B AND C</td>
<td>基本数据类型</td>
<td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td>
</tr>
<tr>
<td>A IS NULL</td>
<td>所有数据类型</td>
<td>如果A等于NULL，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A IS NOT NULL</td>
<td>所有数据类型</td>
<td>如果A不等于NULL，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>IN(数值1, 数值2)</td>
<td>所有数据类型</td>
<td>使用 IN运算显示列表中的值</td>
</tr>
<tr>
<td>A [NOT] LIKE B</td>
<td>STRING 类型</td>
<td>B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td>
</tr>
<tr>
<td>A RLIKE B, A REGEXP B</td>
<td>STRING 类型</td>
<td>B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td>
</tr>
</tbody></table>
<p>2）案例实操</p>
<p>（1）查询出薪水等于5000的所有员工</p>
<blockquote>
<p>  hive (default)&gt; select * from emp where sal =5000;</p>
</blockquote>
<p>（2）查询工资在500到1000的员工信息</p>
<blockquote>
<p>  hive (default)&gt; select * from emp where sal between 500 and 1000;</p>
</blockquote>
<p>（3）查询comm为空的所有员工信息</p>
<blockquote>
<p>  hive (default)&gt; select * from emp where comm is null;</p>
</blockquote>
<p>（4）查询工资是1500或5000的员工信息</p>
<blockquote>
<p>  hive (default)&gt; select * from emp where sal IN (1500, 5000);</p>
</blockquote>
<h3 id="6-2-2-Like和RLike"><a href="#6-2-2-Like和RLike" class="headerlink" title="6.2.2 Like和RLike"></a>6.2.2 Like和RLike</h3><blockquote>
<p>  1）使用LIKE运算选择类似的值</p>
<p>  2）选择条件可以包含字符或数字:</p>
<p>  % 代表零个或多个字符(任意个字符)。</p>
<p>  _ 代表一个字符。</p>
<p>  3）RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。</p>
</blockquote>
<blockquote>
<p>4）案例实操</p>
</blockquote>
<p>（1）查找以2开头薪水的员工信息</p>
<blockquote>
<p>  hive (default)&gt; select * from emp where sal LIKE ‘2%’;</p>
</blockquote>
<p>（2）查找第二个数值为2的薪水的员工信息</p>
<blockquote>
<p>  hive (default)&gt; select * from emp where sal LIKE ‘_2%’;</p>
</blockquote>
<p>（3）查找薪水中含有2的员工信息</p>
<blockquote>
<p>  hive (default)&gt; select * from emp where sal RLIKE ‘[2]’;</p>
</blockquote>
<h3 id="6-2-3-逻辑运算符（And-Or-Not）"><a href="#6-2-3-逻辑运算符（And-Or-Not）" class="headerlink" title="6.2.3 逻辑运算符（And/Or/Not）"></a>6.2.3 逻辑运算符（And/Or/Not）</h3><table>
<thead>
<tr>
<th>操作符</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>AND</td>
<td>逻辑并</td>
</tr>
<tr>
<td>OR</td>
<td>逻辑或</td>
</tr>
<tr>
<td>NOT</td>
<td>逻辑否</td>
</tr>
</tbody></table>
<p>案例实操</p>
<p>（1）查询薪水大于1000，部门是30</p>
<blockquote>
<p>  hive (default)&gt; select * from emp where sal&gt;1000 and deptno=30;</p>
</blockquote>
<p>（2）查询薪水大于1000，或者部门是30</p>
<blockquote>
<p>  hive (default)&gt; select * from emp where sal&gt;1000 or deptno=30;</p>
</blockquote>
<p>（3）查询除了20部门和30部门以外的员工信息</p>
<blockquote>
<p>  hive (default)&gt; select * from emp where deptno not IN(30, 20);</p>
</blockquote>
<h2 id="6-3-分组"><a href="#6-3-分组" class="headerlink" title="6.3 分组"></a>6.3 分组</h2><h3 id="6-3-1-Group-By语句"><a href="#6-3-1-Group-By语句" class="headerlink" title="6.3.1 Group By语句"></a>6.3.1 Group By语句</h3><p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。</p>
<p>案例实操：</p>
<p>（1）计算emp表每个部门的平均工资</p>
<blockquote>
<p>  hive (default)&gt; select t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno;</p>
</blockquote>
<p>（2）计算emp每个部门中每个岗位的最高薪水</p>
<blockquote>
<p>  hive (default)&gt; select t.deptno, t.job, max(t.sal) max_sal from emp t group by t.deptno, t.job;</p>
</blockquote>
<h3 id="6-3-2-Having语句"><a href="#6-3-2-Having语句" class="headerlink" title="6.3.2 Having语句"></a>6.3.2 Having语句</h3><p>1．having与where不同点</p>
<p>（1）where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据。</p>
<p>（2）where后面不能写分组函数，而having后面可以使用分组函数。</p>
<p>（3）having只用于group by分组统计语句。</p>
<p>2．案例实操</p>
<p>（1）求每个部门的平均薪水大于2000的部门</p>
<blockquote>
<p>  求每个部门的平均工资</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; select deptno, avg(sal) from emp group by deptno;</p>
</blockquote>
<p>求每个部门的平均薪水大于2000的部门</p>
<blockquote>
<p>  hive (default)&gt; select deptno, avg(sal) avg_sal from emp group by deptno having avg_sal &gt; 2000;</p>
</blockquote>
<h2 id="6-4-Join语句"><a href="#6-4-Join语句" class="headerlink" title="6.4 Join语句"></a>6.4 Join语句</h2><h3 id="6-4-1-等值Join"><a href="#6-4-1-等值Join" class="headerlink" title="6.4.1 等值Join"></a>6.4.1 等值Join</h3><p>Hive支持通常的SQL JOIN语句，但是**==只支持等值连接，不支持非等值连接。==**</p>
<p>案例实操</p>
<p>（1）根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称；</p>
<blockquote>
<p>  hive (default)&gt; select e.empno, e.ename, d.deptno, d.dname from emp e join dept d on e.deptno = d.deptno;</p>
</blockquote>
<h3 id="6-4-2-表的别名"><a href="#6-4-2-表的别名" class="headerlink" title="6.4.2 表的别名"></a>6.4.2 表的别名</h3><p>1．好处</p>
<p>（1）使用别名可以简化查询。</p>
<p>（2）使用表名前缀可以提高执行效率。</p>
<p>2．案例实操</p>
<p>合并员工表和部门表</p>
<blockquote>
<p>  hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on<br>  e.deptno</p>
</blockquote>
<blockquote>
<p>  = d.deptno;</p>
</blockquote>
<h3 id="6-4-3-内连接"><a href="#6-4-3-内连接" class="headerlink" title="6.4.3 内连接"></a>6.4.3 内连接</h3><p>内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p>
<blockquote>
<p>  hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno;</p>
</blockquote>
<h3 id="6-4-4-左外连接"><a href="#6-4-4-左外连接" class="headerlink" title="6.4.4 左外连接"></a>6.4.4 左外连接</h3><p>左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。</p>
<blockquote>
<p>  hive (default)&gt; select e.empno, e.ename, d.deptno from emp e left join dept<br>  d on e.deptno = d.deptno;</p>
</blockquote>
<h3 id="6-4-5-右外连接"><a href="#6-4-5-右外连接" class="headerlink" title="6.4.5 右外连接"></a>6.4.5 右外连接</h3><p>右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。</p>
<blockquote>
<p>  hive (default)&gt; select e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno = d.deptno;</p>
</blockquote>
<h3 id="6-4-6-满外连接"><a href="#6-4-6-满外连接" class="headerlink" title="6.4.6 满外连接"></a>6.4.6 满外连接</h3><p>满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代。</p>
<blockquote>
<p>  hive (default)&gt; select e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno = d.deptno;</p>
</blockquote>
<h3 id="6-4-7-多表连接"><a href="#6-4-7-多表连接" class="headerlink" title="6.4.7 多表连接"></a>6.4.7 多表连接</h3><p><strong>==注意：连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。==</strong></p>
<p>数据准备 <a href="./location.txt">location.txt</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1700	Beijing</span><br><span class="line">1800	London</span><br><span class="line">1900	Tokyo</span><br></pre></td></tr></table></figure>

<p>1．创建位置表 </p>
<blockquote>
<p>create table if not exists default.location(</p>
<p>loc int,</p>
<p>loc_name string</p>
<p>)</p>
<p>row format delimited fields terminated by ‘\t’;</p>
</blockquote>
<p>2．导入数据</p>
<blockquote>
<p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/location.txt’<br>  into table default.location;</p>
</blockquote>
<p>3．多表连接查询</p>
<blockquote>
<p>  hive (default)&gt;SELECT e.ename, d.deptno, l. loc_name</p>
<p>  FROM  emp e </p>
<p>  JOIN  dept d</p>
<p>  ON   d.deptno = e.deptno </p>
<p>  JOIN  location l</p>
<p>  ON   d.loc = l.loc;</p>
</blockquote>
<p>大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce<br>job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce<br>job的输出和表l;进行连接操作。</p>
<p><strong>==注意：为什么不是表d和表l先进行连接操作呢？这是因为Hive总是按照从左到右的顺序执行的。==</strong></p>
<h3 id="6-4-8-笛卡尔积"><a href="#6-4-8-笛卡尔积" class="headerlink" title="6.4.8 笛卡尔积"></a>6.4.8 笛卡尔积</h3><p>1．笛卡尔集会在下面条件下产生</p>
<p>（1）省略连接条件</p>
<p>（2）连接条件无效</p>
<p>（3）所有表中的所有行互相连接</p>
<p>2．案例实操</p>
<blockquote>
<p>  hive (default)&gt; select empno, dname from emp, dept;</p>
</blockquote>
<h3 id="6-4-9-连接谓词中不支持or"><a href="#6-4-9-连接谓词中不支持or" class="headerlink" title="6.4.9 连接谓词中不支持or"></a>6.4.9 连接谓词中不支持or</h3><blockquote>
<p>  hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on<br>  e.deptno</p>
</blockquote>
<blockquote>
<p>  = d.deptno or e.ename=d.ename; 错误的</p>
</blockquote>
<h2 id="6-5-排序"><a href="#6-5-排序" class="headerlink" title="6.5 排序"></a>6.5 排序</h2><h3 id="6-5-1-全局排序（Order-By）"><a href="#6-5-1-全局排序（Order-By）" class="headerlink" title="6.5.1 全局排序（Order By）"></a>6.5.1 全局排序（Order By）</h3><p><strong>==Order By：全局排序，一个Reducer==</strong></p>
<p>1．使用 ORDER BY 子句排序</p>
<p><strong>==SC（ascend）: 升序（默认）==</strong></p>
<p><strong>==DESC（descend）: 降序==</strong></p>
<p>2．ORDER BY 子句在SELECT语句的结尾</p>
<p>3．案例实操</p>
<p>（1）查询员工信息按工资升序排列</p>
<blockquote>
<p>  hive (default)&gt; select * from emp order by sal;</p>
</blockquote>
<p>（2）查询员工信息按工资降序排列</p>
<blockquote>
<p>  hive (default)&gt; select * from emp order by sal desc;</p>
</blockquote>
<h3 id="6-5-2-按照别名排序"><a href="#6-5-2-按照别名排序" class="headerlink" title="6.5.2 按照别名排序"></a>6.5.2 按照别名排序</h3><p>按照员工薪水的2倍排序</p>
<blockquote>
<p>  hive (default)&gt; select ename, sal*2 twosal from emp order by twosal;</p>
</blockquote>
<h3 id="6-5-3-多个列排序"><a href="#6-5-3-多个列排序" class="headerlink" title="6.5.3 多个列排序"></a>6.5.3 多个列排序</h3><p>按照部门和工资升序排序</p>
<blockquote>
<p>  hive (default)&gt; select ename, deptno, sal from emp order by deptno, sal ;</p>
</blockquote>
<h3 id="6-5-4-每个MapReduce内部排序（Sort-By）"><a href="#6-5-4-每个MapReduce内部排序（Sort-By）" class="headerlink" title="6.5.4 每个MapReduce内部排序（Sort By）"></a>6.5.4 每个MapReduce内部排序（Sort By）</h3><p>Sort By：每个Reducer内部进行排序，对全局结果集来说不是排序。</p>
<p>1．设置reduce个数</p>
<blockquote>
<p>  hive (default)&gt; set mapreduce.job.reduces=3;</p>
</blockquote>
<p>2．查看设置reduce个数</p>
<blockquote>
<p>  hive (default)&gt; set mapreduce.job.reduces;</p>
</blockquote>
<p>3．根据部门编号降序查看员工信息</p>
<blockquote>
<p>  hive (default)&gt; select * from emp sort by empno desc;</p>
</blockquote>
<p>4．将查询结果导入到文件中（按照部门编号降序排序）</p>
<blockquote>
<p>  hive (default)&gt; insert overwrite local directory ‘/opt/module/datas/sortby-result’</p>
</blockquote>
<blockquote>
<p>  select * from emp sort by deptno desc;</p>
</blockquote>
<h3 id="6-5-5-分区排序（Distribute-By）"><a href="#6-5-5-分区排序（Distribute-By）" class="headerlink" title="6.5.5 分区排序（Distribute By）"></a>6.5.5 分区排序（Distribute By）</h3><p>Distribute By：类似MR中partition，进行分区，结合sort by使用。</p>
<p><strong>==注意，Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。==</strong></p>
<p>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute<br>by的效果。</p>
<p>案例实操：</p>
<p>（1）先按照部门编号分区，再按照员工编号降序排序。</p>
<blockquote>
<p>  hive (default)&gt; set mapreduce.job.reduces=3;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; insert overwrite local directory<br>  ‘/opt/module/datas/distribute-result’ select * from emp distribute by<br>  deptno sort by empno desc;</p>
</blockquote>
<h3 id="6-5-6-Cluster-By"><a href="#6-5-6-Cluster-By" class="headerlink" title="6.5.6 Cluster By"></a>6.5.6 Cluster By</h3><p>当distribute by和sorts by字段相同时，可以使用cluster by方式。</p>
<p>cluster by除了具有distribute by的功能外还兼具sort<br>by的功能。但是排序**==只能是升序排序==**，不能指定排序规则为ASC或者DESC。</p>
<p>1）以下两种写法等价</p>
<blockquote>
<p>  hive (default)&gt; select * from emp cluster by deptno;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; select * from emp distribute by deptno sort by deptno;</p>
</blockquote>
<p>注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</p>
<h2 id="6-6-分桶及抽样查询"><a href="#6-6-分桶及抽样查询" class="headerlink" title="6.6 分桶及抽样查询"></a>6.6 <a name="6.6">分桶</a>及抽样查询</h2><h3 id="6-6-1-分桶表数据存储"><a href="#6-6-1-分桶表数据存储" class="headerlink" title="6.6.1 分桶表数据存储"></a>6.6.1 分桶表数据存储</h3><p><strong>分区针对的是数据的存储路径；==分桶针对的是数据文件==。</strong></p>
<p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区，特别是之前所提到过的要确定合适的划分大小这个疑虑。</p>
<p>分桶是将数据集分解成更容易管理的若干部分的另一个技术。</p>
<p>1．先创建分桶表，通过直接导入数据文件的方式</p>
<p>（1）数据准备 <a href="./student.txt">student.txt</a></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1001	ss1</span><br><span class="line">1002	ss2</span><br><span class="line">1003	ss3</span><br><span class="line">1004	ss4</span><br><span class="line">1005	ss5</span><br><span class="line">1006	ss6</span><br><span class="line">1007	ss7</span><br><span class="line">1008	ss8</span><br><span class="line">1009	ss9</span><br><span class="line">1010	ss10</span><br><span class="line">1011	ss11</span><br><span class="line">1012	ss12</span><br><span class="line">1013	ss13</span><br><span class="line">1014	ss14</span><br><span class="line">1015	ss15</span><br><span class="line">1016	ss16</span><br></pre></td></tr></table></figure>

<p>（2）创建分桶表</p>
<blockquote>
<p>create table stu_buck(id int, name string)</p>
<p>clustered by(id) </p>
<p>into 4 buckets</p>
<p>row format delimited fields terminated by ‘\t’;</p>
</blockquote>
<p>（3）查看表结构</p>
<blockquote>
<p>  hive (default)&gt; desc formatted stu_buck;</p>
</blockquote>
<blockquote>
<p>  Num Buckets: 4</p>
</blockquote>
<p>（4）导入数据到分桶表中</p>
<blockquote>
<p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/student.txt’ into table stu_buck;</p>
</blockquote>
<p>（5）查看创建的分桶表中是否分成4个桶,发现并没有分成4个桶。是什么原因呢？</p>
<p>2．创建分桶表时，数据通过子查询的方式导入</p>
<p>（1）先建一个普通的stu表</p>
<blockquote>
<p>create table stu(id int, name string) </p>
<p>row format delimited fields terminated by ‘\t’; </p>
</blockquote>
<p>（2）向普通的stu表中导入数据</p>
<blockquote>
<p>load data local inpath ‘/opt/module/datas/student.txt’ into table stu; </p>
</blockquote>
<p>（3）清空stu_buck表中数据</p>
<blockquote>
<p>truncate table stu_buck; select * from stu_buck; </p>
</blockquote>
<p>（4）导入数据到分桶表，通过子查询的方式</p>
<blockquote>
<p>insert into table stu_buck select id, name from stu; </p>
</blockquote>
<p>（5）发现还是只有一个分桶</p>
<p>（6）需要设置一个属性</p>
<blockquote>
<p>hive (default)&gt; set hive.enforce.bucketing=true;</p>
<p> hive (default)&gt; set mapreduce.job.reduces=-1; </p>
<p>hive (default)&gt; insert into table stu_buck select id, name from stu; </p>
</blockquote>
<p>（7）查询分桶的数据,在hdfs上查看已经分桶</p>
<blockquote>
<p>hive (default)&gt; select * from stu_buck; </p>
<p>OK</p>
<p>stu_buck.id   stu_buck.name</p>
<p>1004  ss4</p>
<p>1008  ss8</p>
<p>1012  ss12</p>
<p>1016  ss16</p>
<p>1001  ss1</p>
<p>1005  ss5</p>
<p>1009  ss9</p>
<p>1013  ss13</p>
<p>1002  ss2</p>
<p>1006  ss6</p>
<p>1010  ss10</p>
<p>1014  ss14</p>
<p>1003  ss3</p>
<p>1007  ss7</p>
<p>1011  ss11</p>
<p>1015  ss15</p>
</blockquote>
<h3 id="6-6-2-分桶抽样查询"><a href="#6-6-2-分桶抽样查询" class="headerlink" title="6.6.2 分桶抽样查询"></a>6.6.2 分桶抽样查询</h3><p>对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。</p>
<p>查询表stu_buck中的数据。</p>
<blockquote>
<p>hive (default)&gt; select * from stu_buck tablesample(bucket 1 out of 4 on id); </p>
</blockquote>
<p>注：tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y) 。</p>
<p>y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。</p>
<p>**==x表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。==**例如，table总bucket数为4，tablesample(bucket1 out of 2)，表示总共抽取（4/2=）2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据。</p>
<p><strong>==注意：x的值必须小于等于y的值，否则==</strong></p>
<p>FAILED: SemanticException [Error 10061]: Numerator should not be bigger than<br>denominator in sample clause for table stu_buck</p>
<h2 id="6-7-其他常用查询函数"><a href="#6-7-其他常用查询函数" class="headerlink" title="6.7 其他常用查询函数"></a>6.7 其他常用查询函数</h2><h3 id="6-7-1-空字段赋值"><a href="#6-7-1-空字段赋值" class="headerlink" title="6.7.1 空字段赋值"></a>6.7.1 空字段赋值</h3><ol>
<li>函数说明</li>
</ol>
<p>NVL：给值为NULL的数据赋值，它的格式是NVL( string1, replace_with)。它的功能是如果string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都为NULL ，则返回NULL。</p>
<ol start="2">
<li>数据准备：采用员工表</li>
</ol>
<p>3.查询：如果员工的comm为NULL，则用-1代替</p>
<blockquote>
<p>hive (default)&gt; select nvl(comm,-1) from emp;</p>
<p>OK</p>
<p>_c0</p>
<p>20.0</p>
<p>300.0</p>
<p>500.0</p>
<p>-1.0</p>
<p>1400.0</p>
<p>-1.0</p>
<p>-1.0</p>
<p>-1.0</p>
<p>-1.0</p>
<p>0.0</p>
<p>-1.0</p>
<p>-1.0</p>
<p>-1.0</p>
<p>-1.0</p>
</blockquote>
<ol start="4">
<li>查询：如果员工的comm为NULL，则用领导id代替</li>
</ol>
<blockquote>
<p>hive (default)&gt; select nvl(comm,mgr) from emp;</p>
<p>OK</p>
<p>_c0</p>
<p>20.0</p>
<p>300.0</p>
<p>500.0</p>
<p>7839.0</p>
<p>1400.0</p>
<p>7839.0</p>
<p>7839.0</p>
<p>7566.0</p>
<p>NULL</p>
<p>0.0</p>
<p>7788.0</p>
<p>7698.0</p>
<p>7566.0</p>
</blockquote>
<h3 id="6-7-2-CASE-WHEN"><a href="#6-7-2-CASE-WHEN" class="headerlink" title="6.7.2 CASE WHEN"></a>6.7.2 CASE WHEN</h3><ol>
<li>数据准备</li>
</ol>
<table>
<thead>
<tr>
<th>name</th>
<th>dept_id</th>
<th>sex</th>
</tr>
</thead>
<tbody><tr>
<td>悟空</td>
<td>A</td>
<td>男</td>
</tr>
<tr>
<td>大海</td>
<td>A</td>
<td>男</td>
</tr>
<tr>
<td>宋宋</td>
<td>B</td>
<td>男</td>
</tr>
<tr>
<td>凤姐</td>
<td>A</td>
<td>女</td>
</tr>
<tr>
<td>婷姐</td>
<td>B</td>
<td>女</td>
</tr>
<tr>
<td>婷婷</td>
<td>B</td>
<td>女</td>
</tr>
</tbody></table>
<p>2．需求</p>
<blockquote>
<p>  求出不同部门男女各多少人。结果如下：</p>
<p>  A 2 1</p>
</blockquote>
<blockquote>
<p>  B 1 2</p>
</blockquote>
<p>3．创建本地emp_sex.txt，导入数据</p>
<blockquote>
<p>  [xing@hadoop102 datas]$ vi emp_sex.txt</p>
<p>  悟空 A  男</p>
<p>  大海 A  男</p>
<p>  宋宋 B  男</p>
<p>  凤姐 A  女</p>
<p>  婷姐 B  女</p>
<p>  婷婷 B  女</p>
</blockquote>
<p>4．创建hive表并导入数据</p>
<blockquote>
<p>  create table emp_sex(</p>
<p>  name string, </p>
<p>  dept_id string, </p>
<p>  sex string) </p>
<p>  row format delimited fields terminated by “\t”;</p>
<p>  load data local inpath ‘/opt/module/datas/emp_sex.txt’ into table emp_sex;</p>
</blockquote>
<p>5．按需求查询数据</p>
<blockquote>
<p>select </p>
<p>dept_id,</p>
<p>sum(case sex when ‘男’ then 1 else 0 end) male_count,</p>
<p>sum(case sex when ‘女’ then 1 else 0 end) female_count</p>
<p>from </p>
<p>emp_sex</p>
<p>group by</p>
<p>dept_id;</p>
</blockquote>
<h3 id="6-7-2-行转列"><a href="#6-7-2-行转列" class="headerlink" title="6.7.2 行转列"></a>6.7.2 行转列</h3><p>1．相关函数说明</p>
<blockquote>
<p>  CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;</p>
</blockquote>
<blockquote>
<p>  CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL和空字符串。分隔符将被加到被连接的字符串之间;</p>
</blockquote>
<blockquote>
<p>  COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。</p>
</blockquote>
<p>2．数据准备</p>
<table>
<thead>
<tr>
<th>name</th>
<th>constellation</th>
<th>blood_type</th>
</tr>
</thead>
<tbody><tr>
<td>孙悟空</td>
<td>白羊座</td>
<td>A</td>
</tr>
<tr>
<td>大海</td>
<td>射手座</td>
<td>A</td>
</tr>
<tr>
<td>宋宋</td>
<td>白羊座</td>
<td>B</td>
</tr>
<tr>
<td>猪八戒</td>
<td>白羊座</td>
<td>A</td>
</tr>
<tr>
<td>凤姐</td>
<td>射手座</td>
<td>A</td>
</tr>
</tbody></table>
<p>3．需求</p>
<blockquote>
<p>  把星座和血型一样的人归类到一起。结果如下：</p>
</blockquote>
<blockquote>
<p>  射手座,A      大海|凤姐</p>
<p>  白羊座,A      孙悟空|猪八戒</p>
<p>  白羊座,B       宋宋</p>
</blockquote>
<p>4．创建本地constellation.txt，导入数据</p>
<blockquote>
<p>  [xing@hadoop102 datas]$ vi constellation.txt</p>
<p>  孙悟空  白羊座  A</p>
<p>  大海    射手座  A</p>
<p>  宋宋    白羊座  B</p>
<p>  猪八戒  白羊座  A</p>
<p>  凤姐    射手座  A</p>
</blockquote>
<p>5．创建hive表并导入数据</p>
<blockquote>
<p>  create table person_info(</p>
<p>  name string, </p>
<p>  constellation string, </p>
<p>  blood_type string) </p>
<p>  row format delimited fields terminated by “\t”;</p>
<p>  load data local inpath “/opt/module/datas/person_info.txt” into table person_info;</p>
</blockquote>
<p>6．按需求查询数据</p>
<blockquote>
<p>select</p>
<p> t1.base,</p>
<p> concat_ws(‘|’, collect_set(t1.name)) name</p>
<p>from</p>
<p> (select</p>
<p>​    name,</p>
<p>​    concat(constellation, “,”, blood_type) base</p>
<p> from</p>
<p>​    person_info) t1</p>
<p>group by</p>
<p> t1.base;</p>
</blockquote>
<h3 id="6-7-3-列转行"><a href="#6-7-3-列转行" class="headerlink" title="6.7.3 列转行"></a>6.7.3 列转行</h3><p>1．函数说明</p>
<p>EXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行。</p>
<p>LATERAL VIEW</p>
<p>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</p>
<p>解释：用于和split,<br>explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p>
<p>2．数据准备</p>
<table>
<thead>
<tr>
<th>movie</th>
<th>category</th>
</tr>
</thead>
<tbody><tr>
<td>《疑犯追踪》</td>
<td>悬疑,动作,科幻,剧情</td>
</tr>
<tr>
<td>《Lie to me》</td>
<td>悬疑,警匪,动作,心理,剧情</td>
</tr>
<tr>
<td>《战狼2》</td>
<td>战争,动作,灾难</td>
</tr>
</tbody></table>
<p>3．需求</p>
<blockquote>
<p>  将电影分类中的数组数据展开。结果如下：</p>
</blockquote>
<blockquote>
<p>  《疑犯追踪》   悬疑</p>
<p>  《疑犯追踪》   动作</p>
<p>  《疑犯追踪》   科幻</p>
<p>  《疑犯追踪》   剧情</p>
<p>  《Lie to me》  悬疑</p>
<p>  《Lie to me》  警匪</p>
<p>  《Lie to me》  动作</p>
<p>  《Lie to me》  心理</p>
<p>  《Lie to me》  剧情</p>
<p>  《战狼2》     战争</p>
<p>  《战狼2》     动作</p>
<p>  《战狼2》     灾难</p>
</blockquote>
<p>4．创建本地movie.txt，导入数据</p>
<blockquote>
<p>  [xing@hadoop102 datas]$ vi movie.txt</p>
<p>  《疑犯追踪》  悬疑,动作,科幻,剧情</p>
<p>  《Lie to me》 悬疑,警匪,动作,心理,剧情</p>
<p>  《战狼2》 战争,动作,灾难</p>
</blockquote>
<p>5．创建hive表并导入数据</p>
<blockquote>
<p>create table movie_info(</p>
<p> movie string, </p>
<p> category array<string>) </p>
<p>row format delimited fields terminated by “\t”</p>
<p>collection items terminated by “,”;</p>
<p>load data local inpath “/opt/module/datas/movie.txt” into table movie_info;</p>
</blockquote>
<p>6．按需求查询数据</p>
<blockquote>
<p>select</p>
<p> movie,</p>
<p> category_name</p>
<p>from </p>
<p> movie_info lateral view explode(category) table_tmp as category_name;</p>
</blockquote>
<h3 id="6-7-4-窗口函数"><a href="#6-7-4-窗口函数" class="headerlink" title="6.7.4 窗口函数"></a>6.7.4 窗口函数</h3><p>1．相关函数说明</p>
<blockquote>
<p>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化</p>
</blockquote>
<blockquote>
<p>  CURRENT ROW：当前行</p>
</blockquote>
<blockquote>
<p>  n PRECEDING：往前n行数据</p>
</blockquote>
<blockquote>
<p>  n FOLLOWING：往后n行数据</p>
</blockquote>
<blockquote>
<p>UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点</p>
</blockquote>
<blockquote>
<p>  LAG(col,n)：往前第n行数据</p>
</blockquote>
<blockquote>
<p>  LEAD(col,n)：往后第n行数据</p>
</blockquote>
<blockquote>
<p>  NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。<strong>注意：n必须为int类型。</strong></p>
</blockquote>
<p>2．数据准备：name，orderdate，cost</p>
<blockquote>
<p>  jack,2017-01-01,10</p>
<p>  tony,2017-01-02,15</p>
<p>  jack,2017-02-03,23</p>
<p>  tony,2017-01-04,29</p>
<p>  jack,2017-01-05,46</p>
<p>  jack,2017-04-06,42</p>
<p>  tony,2017-01-07,50</p>
<p>  jack,2017-01-08,55</p>
<p>  mart,2017-04-08,62</p>
<p>  mart,2017-04-09,68</p>
<p>  neil,2017-05-10,12</p>
<p>  mart,2017-04-11,75</p>
<p>  neil,2017-06-12,80</p>
<p>  mart,2017-04-13,94</p>
</blockquote>
<p>3．需求</p>
<ol>
<li><p>查询在2017年4月份购买过的顾客及总人数</p>
</li>
<li><p>查询顾客的购买明细及月购买总额</p>
</li>
<li><p>上述的场景,要将cost按照日期进行累加</p>
</li>
<li><p>查询顾客上次的购买时间</p>
</li>
<li><p>查询前20%时间的订单信息</p>
</li>
</ol>
<p>4．创建本地business.txt，导入数据</p>
<blockquote>
<p>  [xing@hadoop102 datas]$ vi business.txt</p>
</blockquote>
<p>5．创建hive表并导入数据</p>
<blockquote>
<p>create table business(</p>
<p>name string, </p>
<p>orderdate string,</p>
<p>cost int</p>
<p>) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’;</p>
<p>load data local inpath “/opt/module/datas/business.txt” into table business;</p>
</blockquote>
<p>6．按需求查询数据</p>
<p>（1）查询在2017年4月份购买过的顾客及总人数</p>
<blockquote>
<p>select name,count(*) over ()   from business   where substring(orderdate,1,7) = ‘2017-04’   group by name;  </p>
</blockquote>
<p>（2）查询顾客的购买明细及月购买总额</p>
<blockquote>
<p>select name,orderdate,cost,sum(cost) over(partition  by month(orderdate)) from   business;  </p>
</blockquote>
<p>（3）上述的场景,要将cost按照日期进行累加</p>
<blockquote>
<p>select name,orderdate,cost,  </p>
<p> sum(cost) over() as sample1,–所有行相加 </p>
<p> sum(cost) over(partition by name) as sample2,–按name分组，组内数据相加   </p>
<p>sum(cost) over(partition by name order by  orderdate) as sample3,–按name分组，组内数据累加   sum(cost) over(partition by name order by orderdate  rows between UNBOUNDED PRECEDING and current row ) as sample4 ,–和sample3一样,由起点到当前行的聚合   </p>
<p>sum(cost) over(partition by name order by orderdate  rows between 1 PRECEDING and current row) as sample5, –当前行和前面一行做聚合   </p>
<p>sum(cost) over(partition by name order by orderdate  rows between 1 PRECEDING AND 1 FOLLOWING ) as sample6,–当前行和前边一行及后面一行   </p>
<p>sum(cost) over(partition by name order by orderdate  rows between current row and UNBOUNDED FOLLOWING ) as sample7 –当前行及后面所有行   from business;  </p>
</blockquote>
<p>（4）查看顾客上次的购买时间</p>
<blockquote>
<p>select name,orderdate,cost,   lag(orderdate,1,’1900-01-01’) over(partition by  name order by orderdate ) as time1, lag(orderdate,2) over (partition by name  order by orderdate) as time2   from business;  </p>
</blockquote>
<p>（5）查询前20%时间的订单信息</p>
<blockquote>
<p>select * from ( select name,orderdate,cost, ntile(5) over(order by orderdate) sorted</p>
</blockquote>
<h3 id="6-7-5-Rank"><a href="#6-7-5-Rank" class="headerlink" title="6.7.5 Rank"></a>6.7.5 Rank</h3><p>1．函数说明</p>
<blockquote>
<p>  RANK() 排序相同时会重复，总数不会变</p>
</blockquote>
<blockquote>
<p>  DENSE_RANK() 排序相同时会重复，总数会减少</p>
</blockquote>
<blockquote>
<p>  ROW_NUMBER() 会根据顺序计算</p>
</blockquote>
<p>2．数据准备</p>
<table>
<thead>
<tr>
<th>name</th>
<th>subject</th>
<th>score</th>
</tr>
</thead>
<tbody><tr>
<td>孙悟空</td>
<td>语文</td>
<td>87</td>
</tr>
<tr>
<td>孙悟空</td>
<td>数学</td>
<td>95</td>
</tr>
<tr>
<td>孙悟空</td>
<td>英语</td>
<td>68</td>
</tr>
<tr>
<td>大海</td>
<td>语文</td>
<td>94</td>
</tr>
<tr>
<td>大海</td>
<td>数学</td>
<td>56</td>
</tr>
<tr>
<td>大海</td>
<td>英语</td>
<td>84</td>
</tr>
<tr>
<td>宋宋</td>
<td>语文</td>
<td>64</td>
</tr>
<tr>
<td>宋宋</td>
<td>数学</td>
<td>86</td>
</tr>
<tr>
<td>宋宋</td>
<td>英语</td>
<td>84</td>
</tr>
<tr>
<td>婷婷</td>
<td>语文</td>
<td>65</td>
</tr>
<tr>
<td>婷婷</td>
<td>数学</td>
<td>85</td>
</tr>
<tr>
<td>婷婷</td>
<td>英语</td>
<td>78</td>
</tr>
</tbody></table>
<p>3．需求</p>
<blockquote>
<p>  计算每门学科成绩排名。</p>
</blockquote>
<p>4．创建本地movie.txt，导入数据</p>
<blockquote>
<p>  [xing@hadoop102 datas]$ vi score.txt</p>
</blockquote>
<p>5．创建hive表并导入数据</p>
<blockquote>
<p>create table score(</p>
<p>name string,</p>
<p>subject string, </p>
<p>score int) </p>
<p>row format delimited fields terminated by “\t”;</p>
<p>load data local inpath ‘/opt/module/datas/score.txt’ into table score;</p>
</blockquote>
<p>6．按需求查询数据</p>
<blockquote>
<p>select name,</p>
<p>subject,</p>
<p>score,</p>
<p>rank() over(partition by subject order by score desc) rp,</p>
<p>dense_rank() over(partition by subject order by score desc) drp,</p>
<p>row_number() over(partition by subject order by score desc) rmp</p>
<p>from score;</p>
</blockquote>
<blockquote>
<p>name  subject score  rp   drp   rmp</p>
<p>孙悟空 数学  95   1    1    1</p>
<p>宋宋  数学  86   2    2    2</p>
<p>婷婷  数学  85   3    3    3</p>
<p>大海  数学  56   4    4    4</p>
<p><strong>宋宋  英语  84   1    1    1</strong></p>
<p><strong>大海  英语  84   1    1    2</strong></p>
<p><strong>婷婷  英语  78   3    2    3</strong></p>
<p><strong>孙悟空 英语  68   4    3    4</strong></p>
<p>大海  语文  94   1    1    1</p>
<p>孙悟空 语文  87   2    2    2</p>
<p>婷婷  语文  65   3    3    3</p>
<p>宋宋  语文  64   4    4    4</p>
</blockquote>
<h1 id="第7章-函数"><a href="#第7章-函数" class="headerlink" title="第7章 函数"></a>第7章 函数</h1><h2 id="7-1-系统内置函数"><a href="#7-1-系统内置函数" class="headerlink" title="7.1 系统内置函数"></a>7.1 系统内置函数</h2><p>1．查看系统自带的函数</p>
<blockquote>
<p>  hive&gt; show functions;</p>
</blockquote>
<p>2．显示自带的函数的用法</p>
<blockquote>
<p>  hive&gt; desc function upper;</p>
</blockquote>
<p>3．详细显示自带的函数的用法</p>
<blockquote>
<p>  hive&gt; desc function extended upper;</p>
</blockquote>
<h2 id="7-2-自定义函数"><a href="#7-2-自定义函数" class="headerlink" title="7.2 自定义函数"></a>7.2 自定义函数</h2><blockquote>
<p>  1）Hive自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。</p>
</blockquote>
<blockquote>
<p>  2）当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-definedfunction）。</p>
</blockquote>
<blockquote>
<p>  3）根据用户自定义函数类别分为以下三种：</p>
</blockquote>
<blockquote>
<p>  （1）<strong>UDF</strong>（User-Defined-Function）一进一出</p>
</blockquote>
<blockquote>
<p>  （2）UDAF（User-Defined Aggregation Function）聚集函数，多进一出 类似于：count/max/min</p>
</blockquote>
<blockquote>
<p>  （3）UDTF（User-Defined Table-Generating Functions）一进多出 如lateral view explore()</p>
</blockquote>
<blockquote>
<p>  4）官方文档地址</p>
</blockquote>
<blockquote>
<p>  <a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">https://cwiki.apache.org/confluence/display/Hive/HivePlugins</a></p>
</blockquote>
<blockquote>
<p>  5）<strong>编程步骤：</strong></p>
</blockquote>
<blockquote>
<p>  （1）继承org.apache.hadoop.hive.ql.UDF</p>
</blockquote>
<blockquote>
<p>  （2）需要实现evaluate函数；evaluate函数支持重载；</p>
</blockquote>
<blockquote>
<p>  （3）在hive的命令行窗口创建函数</p>
</blockquote>
<blockquote>
<p>  ​        <strong>a）添加jar</strong></p>
</blockquote>
<blockquote>
<p>  ​            add jar linux_jar_path</p>
</blockquote>
<blockquote>
<p>  ​        <strong>b）创建function，</strong></p>
</blockquote>
<blockquote>
<p>  ​            create [temporary] function [dbname.]function_name AS class_name;</p>
</blockquote>
<blockquote>
<p>  （4）在hive的命令行窗口删除函数</p>
</blockquote>
<blockquote>
<p>  ​        Drop [temporary] function [if exists] [dbname.]function_name;</p>
</blockquote>
<blockquote>
<p>  6）注意事项</p>
</blockquote>
<blockquote>
<p>  （1）UDF必须要有返回类型，可以返回null，但是返回类型不能为void；</p>
</blockquote>
<h2 id="7-3-自定义UDF函数"><a href="#7-3-自定义UDF函数" class="headerlink" title="7.3 自定义UDF函数"></a>7.3 自定义UDF函数</h2><p>1．创建一个Maven工程Hive</p>
<p>2．导入依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">		<span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>


<p>3．创建一个类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing.hive;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Lower</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span> <span class="params">(<span class="keyword">final</span> String s)</span> </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> (s == <span class="keyword">null</span>) &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">return</span> s.toLowerCase();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>




<p>4．打成jar包上传到服务器/opt/module/jars/udf.jar</p>
<p>5．将jar包添加到hive的classpath</p>
<blockquote>
<p>  hive (default)&gt; add jar /opt/module/datas/udf.jar;</p>
</blockquote>
<p>6．创建临时函数与开发好的java class关联</p>
<blockquote>
<p>  hive (default)&gt; create temporary function mylower as “com.xing.hive.Lower”;</p>
</blockquote>
<p>7．即可在hql中使用自定义的函数strip</p>
<blockquote>
<p>  hive (default)&gt; select ename, mylower(ename) lowername from emp;</p>
</blockquote>
<h1 id="第8章-压缩和存储"><a href="#第8章-压缩和存储" class="headerlink" title="第8章 压缩和存储"></a>第8章 <a name="8">压缩和存储</a></h1><h2 id="8-1-Hadoop源码编译支持Snappy压缩"><a href="#8-1-Hadoop源码编译支持Snappy压缩" class="headerlink" title="8.1 Hadoop源码编译支持Snappy压缩"></a>8.1 Hadoop源码编译支持Snappy压缩</h2><h3 id="8-1-1-资源准备"><a href="#8-1-1-资源准备" class="headerlink" title="8.1.1 资源准备"></a>8.1.1 资源准备</h3><p>1．CentOS联网</p>
<p>配置CentOS能连接外网。Linux虚拟机ping <a target="_blank" rel="noopener" href="http://www.baidu.com/">www.baidu.com</a><br>是畅通的</p>
<p><strong>注意：采用root角色编译，减少文件夹权限出现问题</strong></p>
<p>2．jar包准备(hadoop源码、JDK8 、maven、protobuf)</p>
<p>（1）hadoop-2.7.2-src.tar.gz</p>
<p>（2）jdk-8u144-linux-x64.tar.gz</p>
<p>（3）snappy-1.1.3.tar.gz</p>
<p>（4）apache-maven-3.0.5-bin.tar.gz</p>
<p>（5）protobuf-2.5.0.tar.gz</p>
<h3 id="8-1-2-jar包安装"><a href="#8-1-2-jar包安装" class="headerlink" title="8.1.2 jar包安装"></a>8.1.2 jar包安装</h3><p><strong>注意：所有操作必须在root用户下完成</strong></p>
<p>1．JDK解压、配置环境变量JAVA_HOME和PATH，验证<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/javase">java</a>-version(如下都需要验证是否配置成功)</p>
<blockquote>
<p>[root@hadoop101 software] # tar -zxf jdk-8u144-linux-x64.tar.gz -C /opt/module/</p>
<p>[root@hadoop101 software]# vi /etc/profile</p>
<p> #JAVA_HOME  export  JAVA_HOME=/opt/module/jdk1.8.0_144  export  PATH=$PATH:$JAVA_HOME/bin  </p>
<p>[root@hadoop101 software]#source /etc/profile</p>
</blockquote>
<p><strong>验证命令：java -version</strong></p>
<p>2．Maven解压、配置 MAVEN_HOME和PATH</p>
<blockquote>
<p>[root@hadoop101 software]# tar -zxvf apache-maven-3.0.5-bin.tar.gz -C /opt/module/</p>
<p>[root@hadoop101 apache-maven-3.0.5]# vi /etc/profile</p>
<p> #MAVEN_HOME  export  MAVEN_HOME=/opt/module/apache-maven-3.0.5  export  PATH=$PATH:$MAVEN_HOME/bin  </p>
<p>[root@hadoop101 software]#source /etc/profile</p>
</blockquote>
<p><strong>验证命令：mvn -version</strong></p>
<h3 id="8-1-3-编译源码"><a href="#8-1-3-编译源码" class="headerlink" title="8.1.3 编译源码"></a>8.1.3 编译源码</h3><p>1．准备编译环境</p>
<blockquote>
<p>  [root@hadoop101 software]# yum install svn</p>
</blockquote>
<blockquote>
<p>  [root@hadoop101 software]# yum install autoconf automake libtool cmake</p>
</blockquote>
<blockquote>
<p>  [root@hadoop101 software]# yum install ncurses-devel</p>
</blockquote>
<blockquote>
<p>  [root@hadoop101 software]# yum install openssl-devel</p>
</blockquote>
<blockquote>
<p>  [root@hadoop101 software]# yum install gcc*</p>
</blockquote>
<p>2．编译安装snappy</p>
<blockquote>
<p>  [root@hadoop101 software]# tar -zxvf snappy-1.1.3.tar.gz -C /opt/module/</p>
</blockquote>
<blockquote>
<p>  [root@hadoop101 module]# cd snappy-1.1.3/</p>
</blockquote>
<blockquote>
<p>  [root@hadoop101 snappy-1.1.3]# ./configure</p>
</blockquote>
<blockquote>
<p>  [root@hadoop101 snappy-1.1.3]# make</p>
</blockquote>
<blockquote>
<p>  [root@hadoop101 snappy-1.1.3]# make install</p>
</blockquote>
<blockquote>
<p>  # 查看snappy库文件</p>
</blockquote>
<blockquote>
<p>  [root@hadoop101 snappy-1.1.3]# ls -lh /usr/local/lib |grep snappy</p>
</blockquote>
<p>3．编译安装protobuf</p>
<blockquote>
<p>  [root@hadoop101 software]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/</p>
</blockquote>
<blockquote>
<p>  [root@hadoop101 module]# cd protobuf-2.5.0/</p>
</blockquote>
<blockquote>
<p>  [root@hadoop101 protobuf-2.5.0]# ./configure</p>
</blockquote>
<blockquote>
<p>  [root@hadoop101 protobuf-2.5.0]# make</p>
</blockquote>
<blockquote>
<p>  [root@hadoop101 protobuf-2.5.0]# make install</p>
</blockquote>
<blockquote>
<p>  # 查看protobuf版本以测试是否安装成功<br>  [root@hadoop101 protobuf-2.5.0]# protoc –version</p>
</blockquote>
<p>4．编译hadoop native</p>
<blockquote>
<p>  [root@hadoop101 software]# tar -zxvf hadoop-2.7.2-src.tar.gz</p>
</blockquote>
<blockquote>
<p>  [root@hadoop101 software]# cd hadoop-2.7.2-src/</p>
</blockquote>
<blockquote>
<p>  [root@hadoop101 software]# mvn clean package -DskipTests -Pdist,native<br>  -Dtar -Dsnappy.lib=/usr/local/lib -Dbundle.snappy</p>
</blockquote>
<blockquote>
<p>  执行成功后，/opt/software/hadoop-2.7.2-src/hadoop-dist/target/<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/hadoop">hadoop</a>-2.7.2.tar.gz即为新生成的支持snappy压缩的二进制安装包。</p>
</blockquote>
<h2 id="8-2-Hadoop压缩配置"><a href="#8-2-Hadoop压缩配置" class="headerlink" title="8.2 Hadoop压缩配置"></a>8.2 Hadoop压缩配置</h2><h3 id="8-2-1-MR支持的压缩编码"><a href="#8-2-1-MR支持的压缩编码" class="headerlink" title="8.2.1 MR支持的压缩编码"></a>8.2.1 MR支持的压缩编码</h3><table>
<thead>
<tr>
<th>压缩格式</th>
<th>工具</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
</tr>
</thead>
<tbody><tr>
<td>DEFAULT</td>
<td>无</td>
<td>DEFAULT</td>
<td>.deflate</td>
<td>否</td>
</tr>
<tr>
<td>Gzip</td>
<td>gzip</td>
<td>DEFAULT</td>
<td>.gz</td>
<td>否</td>
</tr>
<tr>
<td>bzip2</td>
<td>bzip2</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
</tr>
<tr>
<td>LZO</td>
<td>lzop</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
</tr>
<tr>
<td>Snappy</td>
<td>无</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
</tr>
</tbody></table>
<p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示：</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码/解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
<p>压缩性能的比较：</p>
<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB/s</td>
<td>74.6MB/s</td>
</tr>
</tbody></table>
<p><a target="_blank" rel="noopener" href="http://google.github.io/snappy/">http://google.github.io/snappy/</a></p>
<p>On a single core of a Core i7 processor in 64-bit mode, Snappy <strong>compresses</strong> at<br>about <strong>250 MB/sec</strong> or more and <strong>decompresses</strong> at about <strong>500 MB/sec</strong> or more.</p>
<h3 id="8-2-2-压缩参数配置"><a href="#8-2-2-压缩参数配置" class="headerlink" title="8.2.2 压缩参数配置"></a>8.2.2 压缩参数配置</h3><p>要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody><tr>
<td>io.compression.codecs  （在core-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.Lz4Codec</td>
<td>输入压缩</td>
<td>Hadoop使用文件扩展名判断是否支持某种编解码器</td>
</tr>
<tr>
<td>mapreduce.map.output.compress</td>
<td>false</td>
<td>mapper输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.map.output.compress.codec</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>mapper输出</td>
<td>使用LZO、LZ4或snappy编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress</td>
<td>false</td>
<td>reducer输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.codec</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>reducer输出</td>
<td>使用标准工具或者编解码器，如gzip和bzip2</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.type</td>
<td>RECORD</td>
<td>reducer输出</td>
<td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td>
</tr>
</tbody></table>
<h2 id="8-3-开启Map输出阶段压缩"><a href="#8-3-开启Map输出阶段压缩" class="headerlink" title="8.3 开启Map输出阶段压缩"></a>8.3 开启Map输出阶段压缩</h2><p>开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下：</p>
<p><strong>案例实操：</strong></p>
<p>1．开启hive中间传输数据压缩功能</p>
<blockquote>
<p>  hive (default)&gt;set hive.exec.compress.intermediate=true;</p>
</blockquote>
<p>2．开启mapreduce中map输出压缩功能</p>
<blockquote>
<p>  hive (default)&gt;set mapreduce.map.output.compress=true;</p>
</blockquote>
<p>3．设置mapreduce中map输出数据的压缩方式</p>
<blockquote>
<p>  hive (default)&gt;set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;</p>
</blockquote>
<p>4．执行查询语句</p>
<blockquote>
<p>  hive (default)&gt; select count(ename) name from emp;</p>
</blockquote>
<h2 id="8-4-开启Reduce输出阶段压缩"><a href="#8-4-开启Reduce输出阶段压缩" class="headerlink" title="8.4 开启Reduce输出阶段压缩"></a>8.4 开启Reduce输出阶段压缩</h2><p>当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。</p>
<p><strong>案例实操：</strong></p>
<p>1．开启hive最终输出数据压缩功能</p>
<blockquote>
<p>  hive (default)&gt;set hive.exec.compress.output=true;</p>
</blockquote>
<p>2．开启mapreduce最终输出数据压缩</p>
<blockquote>
<p>  hive (default)&gt;set mapreduce.output.fileoutputformat.compress=true;</p>
</blockquote>
<p>3．设置mapreduce最终数据输出压缩方式</p>
<blockquote>
<p>  hive (default)&gt; set mapreduce.output.fileoutputformat.compress.codec =org.apache.hadoop.io.compress.SnappyCodec;</p>
</blockquote>
<p>4．设置mapreduce最终数据输出压缩为块压缩</p>
<blockquote>
<p>  hive (default)&gt; set mapreduce.output.fileoutputformat.compress.type=BLOCK;</p>
</blockquote>
<p>5．测试一下输出结果是否是压缩文件</p>
<blockquote>
<p>  hive (default)&gt; insert overwrite local directory ‘/opt/module/datas/distribute-result’ select * from emp distribute by deptno sort by empno desc;</p>
</blockquote>
<h2 id="8-5-文件存储格式"><a href="#8-5-文件存储格式" class="headerlink" title="8.5 文件存储格式"></a>8.5 文件存储格式</h2><p>Hive支持的存储数的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。</p>
<h3 id="8-5-1-列式存储和行式存储"><a href="#8-5-1-列式存储和行式存储" class="headerlink" title="8.5.1 列式存储和行式存储"></a>8.5.1 列式存储和行式存储</h3><p>列式存储和行式存储</p>
<p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113931.png" alt="image-20200918011108143"></p>
<p>所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p>
<p>1．行存储的特点</p>
<p>查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p>
<p>2．列存储的特点</p>
<p>因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</p>
<p><strong>==TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；==</strong></p>
<p><strong>==ORC和PARQUET是基于列式存储的。==</strong></p>
<h3 id="8-5-2-TextFile格式"><a href="#8-5-2-TextFile格式" class="headerlink" title="8.5.2 TextFile格式"></a>8.5.2 TextFile格式</h3><p>默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。</p>
<h3 id="8-5-3-Orc格式"><a href="#8-5-3-Orc格式" class="headerlink" title="8.5.3 Orc格式"></a>8.5.3 Orc格式</h3><p>Orc (Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式。</p>
<p>如图所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当RowGroup概念，不过大小由4MB-&gt;250MB，这样应该能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index<br>Data，Row Data，Stripe Footer：</p>
<p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922163426.png" alt="image-20200922163424885"></p>
<p> Orc格式</p>
<p>1）Index Data：一个轻量级的index，默认是**==每隔1W行做一个索引==**。这里做的索引应该只是记录某行的各字段在RowData中的offset。</p>
<p>2）RowData：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。</p>
<p>3）Stripe Footer：存的是各个Stream的类型，长度等信息。</p>
<p>每个文件有一个FileFooter，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到FileFooter长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。</p>
<h3 id="8-5-4-Parquet格式"><a href="#8-5-4-Parquet格式" class="headerlink" title="8.5.4 Parquet格式"></a>8.5.4 Parquet格式</h3><p>Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。</p>
<p>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，**==因此Parquet格式文件是自解析的。==**</p>
<p>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把**==每一个行组由一个Mapper任务处理，增大任务执行并行度。==**Parquet文件的格式如图所示。</p>
<p><img src="d:\桌面\冉辰星总结\03-hive\Hive(2)详细.assets\20200918113932.png" alt="image-20200918011216597"></p>
<p> Parquet格式</p>
<p>上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic<br>Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：**==数据页、字典页和索引页==**。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。</p>
<h3 id="8-5-5-主流文件存储格式对比实验"><a href="#8-5-5-主流文件存储格式对比实验" class="headerlink" title="8.5.5 主流文件存储格式对比实验"></a>8.5.5 主流文件存储格式对比实验</h3><p>从存储文件的压缩比和查询速度两个角度对比。</p>
<p><strong>存储文件的压缩比测试：</strong></p>
<ol>
<li><p>测试数据<a href="./log.data">log.data</a></p>
<p>2．TextFile</p>
</li>
</ol>
<p>（1）创建表，存储数据格式为TEXTFILE</p>
<blockquote>
<p>create  table log_text ( </p>
<p> track_time  string,  url  string,  session_id  string,  referer  string,  ip  string,  end_user_id  string,  city_id  string  )  </p>
<p>row  format delimited fields terminated by ‘\t’  stored  as textfile ;  </p>
</blockquote>
<p>（2）向表中加载数据</p>
<blockquote>
<p>hive  (default)&gt; load data local inpath ‘/opt/module/datas/log.data’ into table  log_text ;  </p>
</blockquote>
<p>（3）查看表中数据大小</p>
<blockquote>
<p>hive  (default)&gt; dfs -du -h /user/hive/warehouse/log_text;  </p>
</blockquote>
<blockquote>
<p><strong>==18.1 M==</strong> /user/hive/warehouse/log_text/log.data</p>
</blockquote>
<p>3．ORC</p>
<p>​    （1）创建表，存储数据格式为ORC</p>
<blockquote>
<p>create  table log_orc(  track_time  string,  url  string,  session_id  string,  referer  string,  ip  string,  end_user_id  string,  city_id  string  ) </p>
<p> row  format delimited fields terminated by ‘\t’  stored  as orc ;  </p>
</blockquote>
<p>（2）向表中加载数据</p>
<blockquote>
<p>hive  (default)&gt; insert into table log_orc select * from log_text ;  </p>
</blockquote>
<p>（3）查看表中数据大小</p>
<blockquote>
<p>hive  (default)&gt; dfs -du -h /user/hive/warehouse/log_orc/ ;  </p>
</blockquote>
<blockquote>
<p>**==2.8 M== **/user/hive/warehouse/log_orc/000000_0</p>
</blockquote>
<p>4．Parquet</p>
<p>（1）创建表，存储数据格式为parquet</p>
<blockquote>
<p>create  table log_parquet(  track_time  string,  url  string,  session_id  string,  referer  string,  ip  string,  end_user_id  string,  city_id  string  )  row  format delimited fields terminated by ‘\t’  stored  as parquet ;    </p>
</blockquote>
<p>（2）向表中加载数据</p>
<blockquote>
<p>hive  (default)&gt; insert into table log_parquet select * from log_text ;  </p>
</blockquote>
<p>（3）查看表中数据大小</p>
<blockquote>
<p>hive  (default)&gt; dfs -du -h /user/hive/warehouse/log_parquet/ ;  </p>
</blockquote>
<blockquote>
<p>**==13.1 M== ** /user/hive/warehouse/log_parquet/000000_0</p>
</blockquote>
<p>存储文件的压缩比总结：</p>
<blockquote>
<p>ORC &gt; Parquet &gt; textFile</p>
</blockquote>
<p><strong>存储文件的查询速度测试：</strong></p>
<p>1．TextFile</p>
<blockquote>
<p>  hive (default)&gt; select count(*) from log_text;</p>
</blockquote>
<blockquote>
<p>  _c0</p>
</blockquote>
<blockquote>
<p>  100000</p>
</blockquote>
<blockquote>
<p>  Time taken: 21.54 seconds, Fetched: 1 row(s)</p>
</blockquote>
<blockquote>
<p>  Time taken: 21.08 seconds, Fetched: 1 row(s)</p>
</blockquote>
<blockquote>
<p>  Time taken: 19.298 seconds, Fetched: 1 row(s)</p>
</blockquote>
<p>2．ORC</p>
<blockquote>
<p>  hive (default)&gt; select count(*) from log_orc;</p>
</blockquote>
<blockquote>
<p>  _c0</p>
</blockquote>
<blockquote>
<p>  100000</p>
</blockquote>
<blockquote>
<p>  Time taken: 20.867 seconds, Fetched: 1 row(s)</p>
</blockquote>
<blockquote>
<p>  Time taken: 22.667 seconds, Fetched: 1 row(s)</p>
</blockquote>
<blockquote>
<p>  Time taken: 18.36 seconds, Fetched: 1 row(s)</p>
</blockquote>
<p>3．Parquet</p>
<blockquote>
<p>  hive (default)&gt; select count(*) from log_parquet;</p>
</blockquote>
<blockquote>
<p>  _c0</p>
</blockquote>
<blockquote>
<p>  100000</p>
</blockquote>
<blockquote>
<p>  Time taken: 22.922 seconds, Fetched: 1 row(s)</p>
</blockquote>
<blockquote>
<p>  Time taken: 21.074 seconds, Fetched: 1 row(s)</p>
</blockquote>
<blockquote>
<p>  Time taken: 18.384 seconds, Fetched: 1 row(s)</p>
</blockquote>
<blockquote>
<p>  存储文件的查询速度总结：**==查询速度相近==。**</p>
</blockquote>
<h2 id="8-6-存储和压缩结合"><a href="#8-6-存储和压缩结合" class="headerlink" title="8.6 存储和压缩结合"></a>8.6 存储和压缩结合</h2><h3 id="8-6-1-修改Hadoop集群具有Snappy压缩方式"><a href="#8-6-1-修改Hadoop集群具有Snappy压缩方式" class="headerlink" title="8.6.1 修改Hadoop集群具有Snappy压缩方式"></a>8.6.1 修改Hadoop集群具有Snappy压缩方式</h3><p>1．查看hadoop checknative命令使用</p>
<blockquote>
<p>  [xing@hadoop104 hadoop-2.7.2]$ hadoop checknative [-a|-h] check native hadoop and compression libraries availability</p>
</blockquote>
<p>2．查看hadoop支持的压缩方式</p>
<blockquote>
<p>  [xing@hadoop104 hadoop-2.7.2]$ hadoop checknative</p>
<p>  17/12/24 20:32:52 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version</p>
<p>  17/12/24 20:32:52 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</p>
<p>  Native library checking:</p>
<p>  hadoop: true /opt/module/hadoop-2.7.2/lib/native/libhadoop.so</p>
<p>  zlib:  true /lib64/libz.so.1</p>
<p>  snappy: false </p>
<p>  lz4:   true revision:99</p>
<p>  bzip2:  false</p>
</blockquote>
<p>3．将编译好的支持Snappy压缩的hadoop-2.7.2.tar.gz包导入到hadoop102的/opt/software中</p>
<p>4．解压hadoop-2.7.2.tar.gz到当前路径</p>
<blockquote>
<p>  [xing@hadoop102 software]$ tar -zxvf hadoop-2.7.2.tar.gz</p>
</blockquote>
<p>5．进入到/opt/software/hadoop-2.7.2/lib/native路径可以看到支持Snappy压缩的动态链接库</p>
<blockquote>
<p>  [xing@hadoop102 native]$ pwd</p>
</blockquote>
<blockquote>
<p>  /opt/software/hadoop-2.7.2/lib/native</p>
</blockquote>
<blockquote>
<p>  [xing@hadoop102 native]$ ll</p>
</blockquote>
<blockquote>
<p>  -rw-r–r–. 1 xing xing 472950 9月 1 10:19 libsnappy.a</p>
</blockquote>
<blockquote>
<p>  -rwxr-xr-x. 1 xing xing 955 9月 1 10:19 libsnappy.la</p>
</blockquote>
<blockquote>
<p>  lrwxrwxrwx. 1 xing xing 18 12月 24 20:39 libsnappy.so -&gt;<br>  libsnappy.so.1.3.0</p>
</blockquote>
<blockquote>
<p>  lrwxrwxrwx. 1 xing xing 18 12月 24 20:39 libsnappy.so.1 -&gt;<br>  libsnappy.so.1.3.0</p>
</blockquote>
<blockquote>
<p>  -rwxr-xr-x. 1 xing xing 228177 9月 1 10:19 libsnappy.so.1.3.0</p>
</blockquote>
<p>6．拷贝/opt/software/hadoop-2.7.2/lib/native里面的所有内容到开发集群的/opt/module/hadoop-2.7.2/lib/native路径上</p>
<blockquote>
<p>  [xing@hadoop102 native]$ cp ../native/*<br>  /opt/module/hadoop-2.7.2/lib/native/</p>
</blockquote>
<p>7．分发集群</p>
<blockquote>
<p>  [xing@hadoop102 lib]$ xsync native/</p>
</blockquote>
<p>8．再次查看hadoop支持的压缩类型</p>
<blockquote>
<p>  [xing@hadoop102 hadoop-2.7.2]$ hadoop checknative</p>
<p>  17/12/24 20:45:02 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version</p>
<p>  17/12/24 20:45:02 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</p>
<p>  Native library checking:</p>
<p>  hadoop: true /opt/module/hadoop-2.7.2/lib/native/libhadoop.so</p>
<p>  zlib:  true /lib64/libz.so.1</p>
<p>  snappy: true /opt/module/hadoop-2.7.2/lib/native/libsnappy.so.1</p>
<p>  lz4:   true revision:99</p>
<p>  bzip2:  false</p>
</blockquote>
<p>9．重新启动hadoop集群和hive</p>
<h3 id="8-6-2-测试存储和压缩"><a href="#8-6-2-测试存储和压缩" class="headerlink" title="8.6.2 测试存储和压缩"></a>8.6.2 测试存储和压缩</h3><p>官网：<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC</a></p>
<p>ORC存储方式的压缩：</p>
<table>
<thead>
<tr>
<th>Key</th>
<th>Default</th>
<th>Notes</th>
</tr>
</thead>
<tbody><tr>
<td>orc.compress</td>
<td>ZLIB</td>
<td>high level compression (one of NONE, ZLIB, SNAPPY)</td>
</tr>
<tr>
<td>orc.compress.size</td>
<td>262,144</td>
<td>number of bytes in each compression chunk</td>
</tr>
<tr>
<td>orc.stripe.size</td>
<td>67,108,864</td>
<td>number of bytes in each stripe</td>
</tr>
<tr>
<td>orc.row.index.stride</td>
<td>10,000</td>
<td>number of rows between index entries (must be &gt;= 1000)</td>
</tr>
<tr>
<td>orc.create.index</td>
<td>true</td>
<td>whether to create row indexes</td>
</tr>
<tr>
<td>orc.bloom.filter.columns</td>
<td>“”</td>
<td>comma separated list of column names for which bloom filter should be created</td>
</tr>
<tr>
<td>orc.bloom.filter.fpp</td>
<td>0.05</td>
<td>false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)</td>
</tr>
</tbody></table>
<p>1．创建一个非压缩的的ORC存储方式</p>
<p>（1）建表语句</p>
<blockquote>
<p>create table log_orc_none(</p>
<p>track_time string,</p>
<p>url string,</p>
<p>session_id string,</p>
<p>referer string,</p>
<p>ip string,</p>
<p>end_user_id string,</p>
<p>city_id string</p>
<p>)</p>
<p>row format delimited fields terminated by ‘\t’</p>
<p>stored as orc tblproperties (“orc.compress”=”NONE”);</p>
</blockquote>
<p>（2）插入数据</p>
<blockquote>
<p>hive (default)&gt; insert into table log_orc_none select * from log_text ; </p>
</blockquote>
<p>（3）查看插入后数据</p>
<blockquote>
<p>hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_none/ ; </p>
</blockquote>
<blockquote>
<p><strong>==7.7 M /user/hive/warehouse/log_orc_none/000000_0==</strong></p>
</blockquote>
<p>2．创建一个SNAPPY压缩的ORC存储方式</p>
<p>（1）建表语句</p>
<blockquote>
<p>create table log_orc_snappy( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string ) row format delimited fields terminated by ‘\t’ stored as orc tblproperties (“orc.compress”=”SNAPPY”); </p>
</blockquote>
<p>（2）插入数据</p>
<blockquote>
<p>hive (default)&gt; insert into table log_orc_snappy select * from log_text ; </p>
</blockquote>
<p>（3）查看插入后数据</p>
<blockquote>
<p>hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_snappy/ ; </p>
</blockquote>
<blockquote>
<p><strong>==3.8 M==</strong> /user/hive/warehouse/log_orc_snappy/000000_0</p>
</blockquote>
<p>3．上一节中默认创建的ORC存储方式，导入数据后的大小为</p>
<blockquote>
<p>**==2.8 M== **/user/hive/warehouse/log_orc/000000_0</p>
</blockquote>
<blockquote>
<p>比Snappy压缩的还小。**==原因是orc存储文件默认采用ZLIB压缩。比snappy压缩的小。==**</p>
</blockquote>
<p>4．存储方式和压缩总结</p>
<p><strong>==在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。==</strong></p>
<h1 id="第9章-企业级调优"><a href="#第9章-企业级调优" class="headerlink" title="第9章 企业级调优"></a>第9章 企业级调优</h1><h2 id="9-1-Fetch抓取"><a href="#9-1-Fetch抓取" class="headerlink" title="9.1 Fetch抓取"></a>9.1 Fetch抓取</h2><p>Fetch抓取是指，**==Hive中对某些情况的查询可以不必使用MapReduce计算==**。例如：SELECT * FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。</p>
<p>在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，**==老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。==**</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.fetch.task.conversion<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>more<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      Expects one of [none, minimal, more].</span><br><span class="line">      Some select queries can be converted to single FETCH task minimizing latency.</span><br><span class="line">      Currently the query should be single sourced not having any subquery and should not have</span><br><span class="line">      any aggregations or distincts (which incurs RS), lateral views and joins.</span><br><span class="line">      0. none : disable hive.fetch.task.conversion</span><br><span class="line">      1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only</span><br><span class="line">      2. more  : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>案例</p>
<p>1）把hive.fetch.task.conversion设置成none，然后执行查询语句，都会执行mapreduce程序。</p>
<blockquote>
<p>  hive (default)&gt; set hive.fetch.task.conversion=none;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; select * from emp;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; select ename from emp;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; select ename from emp limit 3;</p>
</blockquote>
<p>2）把hive.fetch.task.conversion设置成more，然后执行查询语句，如下查询方式都不会执行mapreduce程序。</p>
<blockquote>
<p>  hive (default)&gt; set hive.fetch.task.conversion=more;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; select * from emp;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; select ename from emp;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; select ename from emp limit 3;</p>
</blockquote>
<h2 id="9-2-本地模式"><a href="#9-2-本地模式" class="headerlink" title="9.2 本地模式"></a>9.2 本地模式</h2><p>大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于大多数这种情况，**==Hive可以通过本地模式在单台机器上处理所有的任务==。==对于小数据集，执行时间可以明显被缩短。==**</p>
<p>用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。</p>
<blockquote>
<p>set hive.exec.mode.local.auto=true; //开启本地mr</p>
<p>设置local mr的最大输入数据量，当输入数据量小于这个值时采用local mr的方式，默认为134217728，即128M</p>
<p>set hive.exec.mode.local.auto.inputbytes.max=50000000;</p>
<p>设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4</p>
<p>set hive.exec.mode.local.auto.input.files.max=10;</p>
</blockquote>
<p>案例实操：</p>
<p>1）开启本地模式，并执行查询语句</p>
<blockquote>
<p>  hive (default)&gt; set hive.exec.mode.local.auto=true;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; select * from emp cluster by deptno;</p>
</blockquote>
<blockquote>
<p>  Time taken: 1.328 seconds, Fetched: 14 row(s)</p>
</blockquote>
<p>2）关闭本地模式，并执行查询语句</p>
<blockquote>
<p>  hive (default)&gt; set hive.exec.mode.local.auto=false;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; select * from emp cluster by deptno;</p>
</blockquote>
<blockquote>
<p>  Time taken: 20.09 seconds, Fetched: 14 row(s)</p>
</blockquote>
<h2 id="9-3-表的优化"><a href="#9-3-表的优化" class="headerlink" title="9.3 表的优化"></a>9.3 表的优化</h2><h3 id="9-3-1-小表、大表Join"><a href="#9-3-1-小表、大表Join" class="headerlink" title="9.3.1 小表、大表Join"></a>9.3.1 小表、大表Join</h3><p>将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。</p>
<p><strong>==实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。==</strong></p>
<p><strong>案例实操</strong></p>
<ol>
<li>需求</li>
</ol>
<p>测试大表JOIN小表和小表JOIN大表的效率</p>
<p>2．建大表、小表和JOIN后表的语句</p>
<blockquote>
<p>// 创建大表</p>
<p>create table bigtable(</p>
<p>id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) </p>
<p>row format delimited fields terminated by ‘\t’;</p>
<p>// 创建小表</p>
<p>create table smalltable(</p>
<p>id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) </p>
<p>row format delimited fields terminated by ‘\t’;</p>
<p>// 创建join后表的语句</p>
<p>create table jointable(</p>
<p>id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) </p>
<p>row format delimited fields terminated by ‘\t’;</p>
</blockquote>
<p>3．分别向大表和小表中导入数据</p>
<blockquote>
<p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/bigtable’ intotable bigtable;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt;load data local inpath ‘/opt/module/datas/smalltable’ into table smalltable;</p>
</blockquote>
<p>4．关闭mapjoin功能（默认是打开的）</p>
<blockquote>
<p>  set hive.auto.convert.join = false;</p>
</blockquote>
<p>5．执行小表JOIN大表语句</p>
<blockquote>
<p>  insert overwrite table jointable</p>
<p>  select b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</p>
<p>  from smalltable s</p>
<p>  left join bigtable b</p>
<p>  on b.id = s.id;</p>
</blockquote>
<p><strong>==Time taken: 35.921 seconds==</strong></p>
<p><strong>==No rows affected (44.456 seconds)==</strong></p>
<p>6．执行大表JOIN小表语句</p>
<blockquote>
<p>  insert overwrite table jointable</p>
<p>  select b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</p>
<p>  from bigtable b</p>
<p>  left join smalltable s</p>
<p>  on s.id = b.id;</p>
</blockquote>
<p>Time taken: 34.196 seconds</p>
<p><strong>==No rows affected (26.287 seconds)==</strong></p>
<h3 id="9-3-2-大表Join大表"><a href="#9-3-2-大表Join大表" class="headerlink" title="9.3.2 大表Join大表"></a>9.3.2 大表Join大表</h3><p>1．空KEY过滤</p>
<p>有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空，操作如下：</p>
<p>案例实操</p>
<p>（1）配置历史服务器,配置mapred-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>启动历史服务器</p>
<blockquote>
<p>  sbin/mr-jobhistory-daemon.sh start historyserver</p>
</blockquote>
<p>查看jobhistory</p>
<p><a target="_blank" rel="noopener" href="http://hadoop102:19888/jobhistory">http://hadoop102:19888/jobhistory</a></p>
<p>（2）创建原始数据表、空id表、合并后数据表</p>
<blockquote>
<p>// 创建原始表</p>
<p>create table ori(</p>
<p>id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) </p>
<p>row format delimited fields terminated by ‘\t’;</p>
<p>// 创建空id表</p>
<p>create table nullidtable(</p>
<p>id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) </p>
<p>row format delimited fields terminated by ‘\t’;</p>
<p>// 创建join后表的语句</p>
<p>create table jointable(</p>
<p>id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) </p>
<p>row format delimited fields terminated by ‘\t’;</p>
</blockquote>
<p>（3）分别加载原始数据和空id数据到对应表中</p>
<blockquote>
<p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/ori’ into table ori;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/nullid’ into table nullidtable;</p>
</blockquote>
<p>（4）测试不过滤空id</p>
<blockquote>
<p>  hive (default)&gt; insert overwrite table jointable</p>
</blockquote>
<blockquote>
<p>  select n.* from nullidtable n left join ori o on n.id = o.id;</p>
</blockquote>
<p><strong>==Time taken: 42.038 seconds==</strong></p>
<p><strong>==Time taken: 37.284 seconds==</strong></p>
<p>（5）测试过滤空id</p>
<blockquote>
<p>  hive (default)&gt; insert overwrite table jointable</p>
</blockquote>
<blockquote>
<p>  select n.* from (select * from nullidtable where id is not null ) n left join ori o on n.id = o.id;</p>
</blockquote>
<p><strong>==Time taken: 31.725 seconds==</strong></p>
<p><strong>==Time taken: 28.876 seconds==</strong></p>
<p>2．空key转换</p>
<p>有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。例如：</p>
<p><strong>案例实操：</strong></p>
<p><strong>不随机分布空null值：</strong></p>
<p>（1）设置5个reduce个数</p>
<blockquote>
<p>  <strong>==set mapreduce.job.reduces = 5;==</strong></p>
</blockquote>
<p>（2）JOIN两张表</p>
<blockquote>
<p>  insert overwrite table jointable select n.* from nullidtable n left join ori b on n.id = b.id;</p>
</blockquote>
<p><strong>结果：如图所示，可以看出来，出现了数据倾斜，某些reducer的资源消耗远大于其他reducer。</strong></p>
<p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113934.png" alt="image-20200918103254514"></p>
<p><strong>随机分布空null值</strong></p>
<p>（1）设置5个reduce个数</p>
<blockquote>
<p><strong>==set mapreduce.job.reduces = 5;==</strong></p>
</blockquote>
<p>（2）JOIN两张表</p>
<blockquote>
<p>  insert overwrite table jointable</p>
<p>  select n.* from nullidtable n full join ori o on</p>
<p>  case when n.id is null then concat(‘hive’, rand()) else n.id end = o.id;</p>
</blockquote>
<p><strong>结果：如图所示，可以看出来，消除了数据倾斜，负载均衡reducer的资源消耗</strong></p>
<p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113935.png" alt="image-20200918103502930"></p>
<h3 id="9-3-3-MapJoin"><a href="#9-3-3-MapJoin" class="headerlink" title="9.3.3 MapJoin"></a>9.3.3 MapJoin</h3><p>如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。</p>
<p>1．开启MapJoin参数设置</p>
<p>（1）设置自动选择Mapjoin</p>
<blockquote>
<p>  set hive.auto.convert.join = true; 默认为true</p>
</blockquote>
<p>（2）大表小表的阈值设置（默认25M以下认为是小表）：</p>
<blockquote>
<p>  set hive.mapjoin.smalltable.filesize=25000000;</p>
</blockquote>
<p>2．MapJoin工作机制</p>
<p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113936.png" alt="image-20200918104519263"></p>
<p><strong>案例实操：</strong></p>
<p>（1）开启Mapjoin功能</p>
<blockquote>
<p>  set hive.auto.convert.join = true; 默认为true</p>
</blockquote>
<p>（2）执行小表JOIN大表语句</p>
<blockquote>
<p>insert overwrite table jointable</p>
<p>select b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</p>
<p>from smalltable s</p>
<p>join bigtable b</p>
<p>on s.id = b.id;</p>
</blockquote>
<p>Time taken: 24.594 seconds</p>
<p>（3）执行大表JOIN小表语句</p>
<blockquote>
<p>insert overwrite table jointable</p>
<p>select b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</p>
<p>from bigtable b</p>
<p>join smalltable s</p>
<p>on s.id = b.id;</p>
</blockquote>
<p>Time taken: 24.315 seconds</p>
<h3 id="9-3-4-Group-By"><a href="#9-3-4-Group-By" class="headerlink" title="9.3.4 Group By"></a>9.3.4 Group By</h3><p>默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。</p>
<p>并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。</p>
<p>1．开启Map端聚合参数设置</p>
<p>（1）是否在Map端进行聚合，默认为True</p>
<blockquote>
<p>  hive.map.aggr = true</p>
</blockquote>
<p>（2）在Map端进行聚合操作的条目数目</p>
<blockquote>
<p>  hive.groupby.mapaggr.checkinterval = 100000</p>
</blockquote>
<p>（3）有数据倾斜的时候进行负载均衡（默认是false）</p>
<blockquote>
<p>  hive.groupby.skewindata = true</p>
</blockquote>
<p><strong>==当选项设定为 true，生成的查询计划会有两个MR Job==**。第一个MRJob中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的</strong>==Group By Key有可能被分发到不同的Reduce中==**，从而达到负载均衡的目的；第二个MRJob再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</p>
<h3 id="9-3-5-Count-Distinct-去重统计"><a href="#9-3-5-Count-Distinct-去重统计" class="headerlink" title="9.3.5 Count(Distinct) 去重统计"></a>9.3.5 Count(Distinct) 去重统计</h3><p>数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个ReduceTask来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换：</p>
<p><strong>案例实操</strong></p>
<p>1． 创建一张大表</p>
<blockquote>
<p>hive (default)&gt; create table bigtable(</p>
<p>id bigint, time bigint, uid string, keyword</p>
<p>string, url_rank int, click_num int, click_url string)</p>
<p>row format delimitedfields terminated by ‘\t’;</p>
</blockquote>
<p>2．加载数据</p>
<blockquote>
<p>hive (default)&gt; load data local inpath ‘/opt/module/datas/bigtable’ into table bigtable;</p>
</blockquote>
<p>3．设置5个reduce个数</p>
<blockquote>
<p>set mapreduce.job.reduces = 5;</p>
</blockquote>
<p>4．执行去重id查询</p>
<blockquote>
<p>hive (default)&gt; select count(distinct id) from bigtable;</p>
<p>Stage-Stage-1: Map: 1 Reduce: 1  Cumulative CPU: 7.12 sec  HDFS Read: 120741990 HDFS Write: 7 SUCCESS</p>
<p>Total MapReduce CPU Time Spent: 7 seconds 120 msec</p>
<p>OK</p>
<p>c0</p>
<p>100001</p>
<p>Time taken: 23.607 seconds, Fetched: 1 row(s)</p>
</blockquote>
<p>5．采用GROUP by去重id</p>
<blockquote>
<p>hive (default)&gt; select count(id) from (select id from bigtable group by id) a;</p>
<p>Stage-Stage-1: Map: 1 Reduce: 5  Cumulative CPU: 17.53 sec  HDFS Read: 120752703 HDFS Write: 580 SUCCESS</p>
<p>Stage-Stage-2: Map: 1 Reduce: 1  Cumulative CPU: 4.29 sec  HDFS Read: 9409 HDFS Write: 7 SUCCESS</p>
<p>Total MapReduce CPU Time Spent: 21 seconds 820 msec</p>
<p>OK</p>
<p>_c0</p>
<p>100001</p>
<p>Time taken: 50.795 seconds, Fetched: 1 row(s)</p>
</blockquote>
<p><strong>==虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。==</strong></p>
<h3 id="9-3-6-笛卡尔积"><a href="#9-3-6-笛卡尔积" class="headerlink" title="9.3.6 笛卡尔积"></a>9.3.6 笛卡尔积</h3><p>尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。</p>
<h3 id="9-3-7-行列过滤"><a href="#9-3-7-行列过滤" class="headerlink" title="9.3.7 行列过滤"></a>9.3.7 行列过滤</h3><p>列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。</p>
<p>行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤，比如：</p>
<p><strong>案例实操：</strong></p>
<p>1．测试先关联两张表，再用where条件过滤</p>
<blockquote>
<p>  hive (default)&gt; select o.id from bigtable b</p>
<p>  join ori o on o.id = b.id</p>
<p>  where o.id &lt;= 10;</p>
</blockquote>
<p>Time taken: **==34.406==**seconds, Fetched: 100 row(s)</p>
<p>2．通过子查询后，再关联表</p>
<blockquote>
<p>  hive (default)&gt; select b.id from bigtable b</p>
<p>  join (select id from ori where id &lt;= 10 ) o </p>
<p>  on b.id = o.id;</p>
</blockquote>
<p>Time taken: <strong>==30.058==</strong> seconds, Fetched: 100 row(s)</p>
<h3 id="9-3-8-动态分区调整"><a href="#9-3-8-动态分区调整" class="headerlink" title="9.3.8 动态分区调整"></a>9.3.8 动态分区调整</h3><p>关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置。</p>
<p>1．开启动态分区参数设置</p>
<p>（1）开启动态分区功能（默认true，开启）</p>
<blockquote>
<p>  <strong>==hive.exec.dynamic.partition=true==</strong></p>
</blockquote>
<p>（2）设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。）</p>
<blockquote>
<p>  hive.exec.dynamic.partition.mode=nonstrict</p>
</blockquote>
<p>（3）在所有执行MR的节点上，最大一共可以创建多少个动态分区。</p>
<blockquote>
<p>  hive.exec.max.dynamic.partitions=1000</p>
</blockquote>
<p>（4）**==在每个执行MR的节点上，最大可以创建多少个动态分区==**。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。</p>
<blockquote>
<p>  hive.exec.max.dynamic.partitions.pernode=100</p>
</blockquote>
<p>（5）整个MR Job中，最大可以创建多少个HDFS文件。</p>
<blockquote>
<p>  hive.exec.max.created.files=100000</p>
</blockquote>
<p>（6）当有空分区生成时，是否抛出异常。一般不需要设置。</p>
<blockquote>
<p>  hive.error.on.empty.partition=false</p>
</blockquote>
<p>2．案例实操</p>
<p>需求：将ori中的数据按照时间(如：20111230000008)，插入到目标表ori_partitioned_target的相应分区中。</p>
<p>（1）创建分区表</p>
<blockquote>
<p>create table ori_partitioned(</p>
<p>id bigint, time bigint, uid string, keyword string,  url_rank int, click_num int, click_url string)  </p>
<p>partitioned by (p_time bigint)  </p>
<p>row format delimited fields terminated by ‘\t’; |</p>
</blockquote>
<p>（2）加载数据到分区表中</p>
<blockquote>
<p>hive (default)&gt; load data local inpath ‘/home/xing/ds1’ into table  ori_partitioned </p>
<p>partition(p_time=’20111230000010’) ; </p>
<p>hive (default)&gt; load data local inpath ‘/home/xing/ds2’ into table ori_partitioned partition(p_time=’20111230000011’) ; </p>
</blockquote>
<p>（3）创建目标分区表</p>
<blockquote>
<p>create table ori_partitioned_target(</p>
<p>id bigint, time bigint, uid string,  keyword string, url_rank int, click_num int, click_url string) PARTITIONED BY (p_time STRING)</p>
<p>row format delimited fields terminated by ‘\t’; |</p>
</blockquote>
<p>（4）设置动态分区</p>
<blockquote>
<p> set hive.exec.dynamic.partition = true; </p>
<p>set hive.exec.dynamic.partition.mode = nonstrict; </p>
<p>set hive.exec.max.dynamic.partitions = 1000; </p>
<p>set hive.exec.max.dynamic.partitions.pernode = 100; </p>
<p>set hive.exec.max.created.files = 100000; </p>
<p>set hive.error.on.empty.partition = false; </p>
<p>hive (default)&gt; insert overwrite table ori_partitioned_target partition (p_time)  </p>
<p>select id, time, uid, keyword, url_rank, click_num, click_url, p_time from ori_partitioned;</p>
</blockquote>
<p>（5）查看目标分区表的分区情况</p>
<blockquote>
<p>  hive (default)&gt; show partitions ori_partitioned_target;</p>
</blockquote>
<h3 id="9-3-9-分桶"><a href="#9-3-9-分桶" class="headerlink" title="9.3.9 分桶"></a>9.3.9 分桶</h3><p><a href="#6.6">详见6.6章</a>。</p>
<h3 id="9-3-10-分区"><a href="#9-3-10-分区" class="headerlink" title="9.3.10 分区"></a>9.3.10 分区</h3><p><a href="#4.6">详见4.6章</a>。</p>
<h2 id="9-4-数据倾斜"><a href="#9-4-数据倾斜" class="headerlink" title="9.4 数据倾斜"></a>9.4 数据倾斜</h2><h3 id="9-4-1-合理设置Map数"><a href="#9-4-1-合理设置Map数" class="headerlink" title="9.4.1 合理设置Map数"></a>9.4.1 合理设置Map数</h3><p><strong>1）通常情况下，作业会通过input的目录产生一个或者多个map任务。</strong></p>
<p>主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。</p>
<p><strong>2）是不是map数越多越好？</strong></p>
<p>答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。</p>
<p><strong>3）是不是保证每个map处理接近128m的文件块，就高枕无忧了？</strong></p>
<p>答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。</p>
<p>针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数；</p>
<h3 id="9-4-2-小文件进行合并"><a href="#9-4-2-小文件进行合并" class="headerlink" title="9.4.2 小文件进行合并"></a>9.4.2 小文件进行合并</h3><p>在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。</p>
<blockquote>
<p>set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</p>
</blockquote>
<h3 id="9-4-3-复杂文件增加Map数"><a href="#9-4-3-复杂文件增加Map数" class="headerlink" title="9.4.3 复杂文件增加Map数"></a>9.4.3 复杂文件增加Map数</h3><p>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。</p>
<p>增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。</p>
<p><strong>案例实操：</strong></p>
<p>1．执行查询</p>
<blockquote>
<p>  hive (default)&gt; select count(*) from emp;</p>
</blockquote>
<blockquote>
<p>  Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</p>
</blockquote>
<p>2．设置最大切片值为100个字节</p>
<blockquote>
<p>  hive (default)&gt; set mapreduce.input.fileinputformat.split.maxsize=100;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; select count(*) from emp;</p>
</blockquote>
<blockquote>
<p>  Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1</p>
</blockquote>
<h3 id="9-4-4-合理设置Reduce数"><a href="#9-4-4-合理设置Reduce数" class="headerlink" title="9.4.4 合理设置Reduce数"></a>9.4.4 合理设置Reduce数</h3><p>1．调整reduce个数方法一</p>
<p>（1）每个Reduce处理的数据量默认是256MB</p>
<blockquote>
<p>  hive.exec.reducers.bytes.per.reducer=256000000</p>
</blockquote>
<p>（2）每个任务最大的reduce数，默认为1009</p>
<blockquote>
<p>  hive.exec.reducers.max=1009</p>
</blockquote>
<p>（3）计算reducer数的公式</p>
<blockquote>
<p>  N=min(参数2，总输入数据量/参数1)</p>
</blockquote>
<p>2．调整reduce个数方法二</p>
<p>在hadoop的mapred-default.xml文件中修改</p>
<p>设置每个job的Reduce个数</p>
<blockquote>
<p>  set mapreduce.job.reduces = 15;</p>
</blockquote>
<p>3．reduce个数并不是越多越好</p>
<p>1）过多的启动和初始化reduce也会消耗时间和资源；</p>
<p>2）另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</p>
<p>在设置reduce个数的时候也需要考虑这两个原则：**==处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适；==**</p>
<h2 id="9-5-并行执行"><a href="#9-5-并行执行" class="headerlink" title="9.5 并行执行"></a>9.5 并行执行</h2><p>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。</p>
<p>通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。</p>
<blockquote>
<p>set hive.exec.parallel=true; //打开任务并行执行</p>
</blockquote>
<blockquote>
<p>set hive.exec.parallel.thread.number=16; //同一个sql允许最大并行度，默认为8。</p>
</blockquote>
<p><strong>==当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。==</strong></p>
<h2 id="9-6-严格模式"><a href="#9-6-严格模式" class="headerlink" title="9.6 严格模式"></a>9.6 严格模式</h2><p>Hive提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询。</p>
<p>通过设置属性hive.mapred.mode值为默认是非严格模式**==nonstrict==**。开启严格模式需要修改hive.mapred.mode值为strict，开启严格模式可以禁止3种类型的查询。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.mapred.mode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>strict<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      The mode in which the Hive operations are being performed. </span><br><span class="line">      In strict mode, some risky queries are not allowed to run. They include:</span><br><span class="line">        Cartesian Product.</span><br><span class="line">        No partition being picked up for a query.</span><br><span class="line">        Comparing bigints and strings.</span><br><span class="line">        Comparing bigints and doubles.</span><br><span class="line">        Orderby without limit.</span><br><span class="line">	<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol>
<li><p>对于分区表，**==除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行==**。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。</p>
</li>
<li><p>对于**==使用了order by语句的查询，要求必须使用limit语句==**。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。</p>
</li>
<li><p>**==限制笛卡尔积的查询==**。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。</p>
</li>
</ol>
<h2 id="9-7-JVM重用"><a href="#9-7-JVM重用" class="headerlink" title="9.7 JVM重用"></a>9.7 JVM重用</h2><p>JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是**==对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。==**</p>
<p>Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。**==JVM重用可以使得JVM实例在同一个job中重新使用N次==**。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.jvm.numtasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      How many tasks to run per jvm. If set to -1, there is no limit. </span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。</p>
<h2 id="9-8-推测执行"><a href="#9-8-推测执行" class="headerlink" title="9.8 推测执行"></a>9.8 推测执行</h2><p>在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。</p>
<p>设置开启推测执行参数：Hadoop的mapred-site.xml文件中进行配置</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      If true, then multiple instances of some map tasks  may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      If true, then multiple instances of some reduce tasks  may be executed in parallel.	<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>不过hive本身也提供了配置项来控制reduce-side的推测执行：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.mapred.reduce.tasks.speculative.execution<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">        Whether speculative execution for reducers should be turned on. 					<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<p>关于调优这些推测执行变量，还很难给一个具体的建议。**==如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉==**。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大大。</p>
<h2 id="9-9-压缩"><a href="#9-9-压缩" class="headerlink" title="9.9 压缩"></a>9.9 压缩</h2><p><a href="#8">详见第8章。</a></p>
<h2 id="9-10-执行计划（Explain）"><a href="#9-10-执行计划（Explain）" class="headerlink" title="9.10 执行计划（Explain）"></a>9.10 执行计划（Explain）</h2><p>1．基本语法</p>
<p>EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query</p>
<p>2．案例实操</p>
<p>（1）查看下面这条语句的执行计划</p>
<blockquote>
<p>  hive (default)&gt; explain select * from emp;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; explain select deptno, avg(sal) avg_sal from emp group by<br>  deptno;</p>
</blockquote>
<p>（2）查看详细执行计划</p>
<blockquote>
<p>  hive (default)&gt; explain extended select * from emp;</p>
</blockquote>
<blockquote>
<p>  hive (default)&gt; explain extended select deptno, avg(sal) avg_sal from emp group by deptno;</p>
</blockquote>
<h1 id="第10章-Hive实战之谷粒影音"><a href="#第10章-Hive实战之谷粒影音" class="headerlink" title="第10章 Hive实战之谷粒影音"></a>第10章 Hive实战之谷粒影音</h1><h2 id="10-1-需求描述"><a href="#10-1-需求描述" class="headerlink" title="10.1 需求描述"></a>10.1 需求描述</h2><p>统计硅谷影音视频网站的常规指标，各种TopN指标：</p>
<p>--统计视频观看数Top10</p>
<p>--统计视频类别热度Top10</p>
<p>--统计视频观看数Top20所属类别</p>
<p>--统计视频观看数Top50所关联视频的所属类别Rank</p>
<p>--统计每个类别中的视频热度Top10</p>
<p>--统计每个类别中视频流量Top10</p>
<p>--统计上传视频最多的用户Top10以及他们上传的视频</p>
<p>--统计每个类别视频观看数Top10</p>
<h2 id="10-2-项目"><a href="#10-2-项目" class="headerlink" title="10.2 项目"></a>10.2 项目</h2><h3 id="10-2-1-数据结构"><a href="#10-2-1-数据结构" class="headerlink" title="10.2.1 数据结构"></a>10.2.1 数据结构</h3><p>1．视频表</p>
<table>
<thead>
<tr>
<th>字段</th>
<th>备注</th>
<th>详细描述</th>
</tr>
</thead>
<tbody><tr>
<td>video id</td>
<td>视频唯一id</td>
<td>11位字符串</td>
</tr>
<tr>
<td>uploader</td>
<td>视频上传者</td>
<td>上传视频的用户名String</td>
</tr>
<tr>
<td>age</td>
<td>视频年龄</td>
<td>视频在平台上的整数天</td>
</tr>
<tr>
<td>category</td>
<td>视频类别</td>
<td>上传视频指定的视频分类</td>
</tr>
<tr>
<td>length</td>
<td>视频长度</td>
<td>整形数字标识的视频长度</td>
</tr>
<tr>
<td>views</td>
<td>观看次数</td>
<td>视频被浏览的次数</td>
</tr>
<tr>
<td>rate</td>
<td>视频评分</td>
<td>满分5分</td>
</tr>
<tr>
<td>ratings</td>
<td>流量</td>
<td>视频的流量，整型数字</td>
</tr>
<tr>
<td>conments</td>
<td>评论数</td>
<td>一个视频的整数评论数</td>
</tr>
<tr>
<td>related ids</td>
<td>相关视频id</td>
<td>相关视频的id，最多20个</td>
</tr>
</tbody></table>
<p>2．用户表</p>
<table>
<thead>
<tr>
<th>字段</th>
<th>备注</th>
<th>字段类型</th>
</tr>
</thead>
<tbody><tr>
<td>uploader</td>
<td>上传者用户名</td>
<td>string</td>
</tr>
<tr>
<td>videos</td>
<td>上传视频数</td>
<td>int</td>
</tr>
<tr>
<td>friends</td>
<td>朋友数量</td>
<td>int</td>
</tr>
</tbody></table>
<h3 id="10-2-2-ETL原始数据"><a href="#10-2-2-ETL原始数据" class="headerlink" title="10.2.2 ETL原始数据"></a>10.2.2 ETL原始数据</h3><p>通过观察原始数据形式，可以发现，视频可以有多个所属分类，每个所属分类用&amp;符号分割，且分割的两边有空格字符，同时相关视频也是可以有多个元素，多个相关视频又用“\t”进行分割。为了分析数据时方便对存在多个子元素的数据进行操作，我们首先进行数据重组清洗操作。即：将所有的类别用“&amp;”分割，同时去掉两边空格，多个相关视频id也使用“&amp;”进行分割。</p>
<p>1．ETL之ETLUtil</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ETLUtil</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">oriString2ETLString</span><span class="params">(String ori)</span></span>&#123;</span><br><span class="line">		StringBuilder etlString = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">		String[] splits = ori.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">		<span class="keyword">if</span>(splits.length &lt; <span class="number">9</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">		splits[<span class="number">3</span>] = splits[<span class="number">3</span>].replace(<span class="string">&quot; &quot;</span>, <span class="string">&quot;&quot;</span>);</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; splits.length; i++)&#123;</span><br><span class="line">			<span class="keyword">if</span>(i &lt; <span class="number">9</span>)&#123;</span><br><span class="line">				<span class="keyword">if</span>(i == splits.length - <span class="number">1</span>)&#123;</span><br><span class="line">					etlString.append(splits[i]);					</span><br><span class="line">				&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">					etlString.append(splits[i] + <span class="string">&quot;\t&quot;</span>);	</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">				<span class="keyword">if</span>(i == splits.length - <span class="number">1</span>)&#123;</span><br><span class="line">					etlString.append(splits[i]);</span><br><span class="line">				&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">					etlString.append(splits[i] + <span class="string">&quot;&amp;&quot;</span>);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">return</span> etlString.toString();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>




<p>2．ETL之Mapper</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.xing.util.ETLUtil;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">VideoETLMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">	Text text = <span class="keyword">new</span> Text();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		String etlString = ETLUtil.oriString2ETLString(value.toString());</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span>(StringUtils.isBlank(etlString)) <span class="keyword">return</span>;</span><br><span class="line">		</span><br><span class="line">		text.set(etlString);</span><br><span class="line">		context.write(NullWritable.get(), text);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>




<p>3．ETL之Runner</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">VideoETLRunner</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> Configuration conf = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setConf</span><span class="params">(Configuration conf)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.conf = conf;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> Configuration <span class="title">getConf</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">this</span>.conf;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		conf = <span class="keyword">this</span>.getConf();</span><br><span class="line">		conf.set(<span class="string">&quot;inpath&quot;</span>, args[<span class="number">0</span>]);</span><br><span class="line">		conf.set(<span class="string">&quot;outpath&quot;</span>, args[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">		Job job = Job.getInstance(conf);</span><br><span class="line">		</span><br><span class="line">		job.setJarByClass(VideoETLRunner.class);</span><br><span class="line">		</span><br><span class="line">		job.setMapperClass(VideoETLMapper.class);</span><br><span class="line">		job.setMapOutputKeyClass(NullWritable.class);</span><br><span class="line">		job.setMapOutputValueClass(Text.class);</span><br><span class="line">		job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">this</span>.initJobInputPath(job);</span><br><span class="line">		<span class="keyword">this</span>.initJobOutputPath(job);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">return</span> job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initJobOutputPath</span><span class="params">(Job job)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		Configuration conf = job.getConfiguration();</span><br><span class="line">		String outPathString = conf.get(<span class="string">&quot;outpath&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		FileSystem fs = FileSystem.get(conf);</span><br><span class="line">		</span><br><span class="line">		Path outPath = <span class="keyword">new</span> Path(outPathString);</span><br><span class="line">		<span class="keyword">if</span>(fs.exists(outPath))&#123;</span><br><span class="line">			fs.delete(outPath, <span class="keyword">true</span>);</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		FileOutputFormat.setOutputPath(job, outPath);</span><br><span class="line">		</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initJobInputPath</span><span class="params">(Job job)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		Configuration conf = job.getConfiguration();</span><br><span class="line">		String inPathString = conf.get(<span class="string">&quot;inpath&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		FileSystem fs = FileSystem.get(conf);</span><br><span class="line">		</span><br><span class="line">		Path inPath = <span class="keyword">new</span> Path(inPathString);</span><br><span class="line">		<span class="keyword">if</span>(fs.exists(inPath))&#123;</span><br><span class="line">			FileInputFormat.addInputPath(job, inPath);</span><br><span class="line">		&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;HDFS中该文件目录不存在：&quot;</span> + inPathString);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			<span class="keyword">int</span> resultCode = ToolRunner.run(<span class="keyword">new</span> VideoETLRunner(), args);</span><br><span class="line">			<span class="keyword">if</span>(resultCode == <span class="number">0</span>)&#123;</span><br><span class="line">				System.out.println(<span class="string">&quot;Success!&quot;</span>);</span><br><span class="line">			&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">				System.out.println(<span class="string">&quot;Fail!&quot;</span>);</span><br><span class="line">			&#125;</span><br><span class="line">			System.exit(resultCode);</span><br><span class="line">		&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">			e.printStackTrace();</span><br><span class="line">			System.exit(<span class="number">1</span>);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>




<p>4．执行ETL</p>
<blockquote>
<p>$ bin/yarn jar ~/softwares/jars/gulivideo-0.0.1-SNAPSHOT.jar \ </p>
<p>com.xing.etl.ETLVideosRunner  \  </p>
<p>/gulivideo/video/2008/0222  \  </p>
<p>/gulivideo/output/video/2008/0222  </p>
</blockquote>
<h2 id="10-3-准备工作"><a href="#10-3-准备工作" class="headerlink" title="10.3 准备工作"></a>10.3 准备工作</h2><h3 id="10-3-1-创建表"><a href="#10-3-1-创建表" class="headerlink" title="10.3.1 创建表"></a>10.3.1 创建表</h3><p>创建表：gulivideo_ori，gulivideo_user_ori，</p>
<p>创建表：gulivideo_orc，gulivideo_user_orc</p>
<p>gulivideo_ori：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_ori(</span><br><span class="line">    videoId <span class="keyword">string</span>, </span><br><span class="line">    uploader <span class="keyword">string</span>, </span><br><span class="line">    age <span class="built_in">int</span>, </span><br><span class="line">    <span class="keyword">category</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;, </span><br><span class="line">    <span class="keyword">length</span> <span class="built_in">int</span>, </span><br><span class="line">    views <span class="built_in">int</span>, </span><br><span class="line">    rate <span class="built_in">float</span>, </span><br><span class="line">    ratings <span class="built_in">int</span>, </span><br><span class="line">    comments <span class="built_in">int</span>,</span><br><span class="line">    relatedId <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;&amp;&quot;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure>



<p>gulivideo_user_ori：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_user_ori(</span><br><span class="line">    uploader <span class="keyword">string</span>,</span><br><span class="line">    videos <span class="built_in">int</span>,</span><br><span class="line">    friends <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span> </span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure>




<p>然后把原始数据插入到orc表中</p>
<p>gulivideo_orc：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_orc(</span><br><span class="line">    videoId <span class="keyword">string</span>, </span><br><span class="line">    uploader <span class="keyword">string</span>, </span><br><span class="line">    age <span class="built_in">int</span>, </span><br><span class="line">    <span class="keyword">category</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;, </span><br><span class="line">    <span class="keyword">length</span> <span class="built_in">int</span>, </span><br><span class="line">    views <span class="built_in">int</span>, </span><br><span class="line">    rate <span class="built_in">float</span>, </span><br><span class="line">    ratings <span class="built_in">int</span>, </span><br><span class="line">    comments <span class="built_in">int</span>,</span><br><span class="line">    relatedId <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;)</span><br><span class="line">clustered <span class="keyword">by</span> (uploader) <span class="keyword">into</span> <span class="number">8</span> buckets </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span> </span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;&amp;&quot;</span> </span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc;</span><br></pre></td></tr></table></figure>




<p>gulivideo_user_orc：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_user_orc(</span><br><span class="line">    uploader <span class="keyword">string</span>,</span><br><span class="line">    videos <span class="built_in">int</span>,</span><br><span class="line">    friends <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span> </span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc;</span><br></pre></td></tr></table></figure>




<h3 id="10-3-2-导入ETL后的数据"><a href="#10-3-2-导入ETL后的数据" class="headerlink" title="10.3.2 导入ETL后的数据"></a>10.3.2 导入ETL后的数据</h3><p>gulivideo_ori：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">&quot;/gulivideo/output/video/2008/0222&quot;</span> <span class="keyword">into</span> <span class="keyword">table</span> gulivideo_ori; </span><br></pre></td></tr></table></figure>

<p>gulivideo_user_ori：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">&quot;/gulivideo/user/2008/0903&quot;</span> <span class="keyword">into</span> <span class="keyword">table</span> gulivideo_user_ori; </span><br></pre></td></tr></table></figure>




<h3 id="10-3-3-向ORC表插入数据"><a href="#10-3-3-向ORC表插入数据" class="headerlink" title="10.3.3 向ORC表插入数据"></a>10.3.3 向ORC表插入数据</h3><p>gulivideo_orc：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> gulivideo_orc <span class="keyword">select</span> * <span class="keyword">from</span> gulivideo_ori; </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>gulivideo_user_orc：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> gulivideo_user_orc <span class="keyword">select</span> * <span class="keyword">from</span> gulivideo_user_ori; </span><br></pre></td></tr></table></figure>




<h2 id="10-4-业务分析"><a href="#10-4-业务分析" class="headerlink" title="10.4 业务分析"></a>10.4 业务分析</h2><h3 id="10-4-1-统计视频观看数Top10"><a href="#10-4-1-统计视频观看数Top10" class="headerlink" title="10.4.1 统计视频观看数Top10"></a>10.4.1 统计视频观看数Top10</h3><p>思路：使用order by按照views字段做一个全局排序即可，同时我们设置只显示前10条。</p>
<p>最终代码：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>   </span><br><span class="line">videoId,  uploader,   age,   <span class="keyword">category</span>,   <span class="keyword">length</span>,   views,   rate,   ratings,   comments <span class="keyword">from</span>   gulivideo_orc  </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span>   views  <span class="keyword">desc</span> <span class="keyword">limit</span>   <span class="number">10</span>; </span><br></pre></td></tr></table></figure>


<h3 id="10-4-2-统计视频类别热度Top10"><a href="#10-4-2-统计视频类别热度Top10" class="headerlink" title="10.4.2 统计视频类别热度Top10"></a>10.4.2 统计视频类别热度Top10</h3><p>思路：</p>
<ol>
<li><p>即统计每个类别有多少个视频，显示出包含视频最多的前10个类别。</p>
</li>
<li><p>我们需要按照类别group by聚合，然后count组内的videoId个数即可。</p>
</li>
<li><p>因为当前表结构为：一个视频对应一个或多个类别。所以如果要group<br>by类别，需要先将类别进行列转行(展开)，然后再进行count即可。</p>
</li>
<li><p>最后按照热度排序，显示前10条。</p>
</li>
</ol>
<p>最终代码：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>  </span><br><span class="line">category_name <span class="keyword">as</span> <span class="keyword">category</span>, <span class="keyword">count</span>(t1.videoId) <span class="keyword">as</span> hot  </span><br><span class="line"><span class="keyword">from</span> (<span class="keyword">select</span> videoId, category_name </span><br><span class="line">      <span class="keyword">from</span> gulivideo_orc <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) t_catetory <span class="keyword">as</span> category_name) t1 <span class="keyword">group</span> <span class="keyword">by</span>   t1.category_name  </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span>   hot  <span class="keyword">desc</span> <span class="keyword">limit</span>   <span class="number">10</span>; </span><br></pre></td></tr></table></figure>




<h3 id="10-4-3-统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数"><a href="#10-4-3-统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数" class="headerlink" title="10.4.3 统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数"></a>10.4.3 统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数</h3><p>思路：</p>
<ol>
<li><p>先找到观看数最高的20个视频所属条目的所有信息，降序排列</p>
</li>
<li><p>把这20条信息中的category分裂出来(列转行)</p>
</li>
<li><p>最后查询视频分类名称和该分类下有多少个Top20的视频</p>
</li>
</ol>
<p>最终代码：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>   </span><br><span class="line">	category_name <span class="keyword">as</span> <span class="keyword">category</span>, <span class="keyword">count</span>(t2.videoId) <span class="keyword">as</span> hot_with_views  </span><br><span class="line"><span class="keyword">from</span> (<span class="keyword">select</span> </span><br><span class="line">      	videoId, category_name   </span><br><span class="line">      <span class="keyword">from</span> (<span class="keyword">select</span> *   <span class="keyword">from</span>   gulivideo_orc   <span class="keyword">order</span> <span class="keyword">by</span>   views   <span class="keyword">desc</span> <span class="keyword">limit</span>   <span class="number">20</span>) t1 		  <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) t_catetory <span class="keyword">as</span> category_name</span><br><span class="line">     ) t2  </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span>  category_name  </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span>  hot_with_views  <span class="keyword">desc</span>; </span><br></pre></td></tr></table></figure>




<h3 id="10-4-4-统计视频观看数Top50所关联视频的所属类别Rank"><a href="#10-4-4-统计视频观看数Top50所关联视频的所属类别Rank" class="headerlink" title="10.4.4 统计视频观看数Top50所关联视频的所属类别Rank"></a>10.4.4 统计视频观看数Top50所关联视频的所属类别Rank</h3><p>思路：</p>
<ol>
<li>查询出观看数最多的前50个视频的所有信息(当然包含了每个视频对应的关联视频)，记为临时表t1</li>
</ol>
<p>t1：观看数前50的视频</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>  *  <span class="keyword">from</span>   gulivideo_orc  <span class="keyword">order</span> <span class="keyword">by</span>   views  <span class="keyword">desc</span> <span class="keyword">limit</span>   <span class="number">50</span>; </span><br></pre></td></tr></table></figure>



<ol start="2">
<li>将找到的50条视频信息的相关视频relatedId列转行，记为临时表t2</li>
</ol>
<p>t2：将相关视频的id进行列转行操作</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>   <span class="keyword">explode</span>(relatedId) <span class="keyword">as</span> videoId  <span class="keyword">from</span>   t1; </span><br></pre></td></tr></table></figure>

<ol start="3">
<li>将相关视频的id和gulivideo_orc表进行inner join操作</li>
</ol>
<p>t5：得到两列数据，一列是category，一列是之前查询出来的相关视频id</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(</span><br><span class="line">    <span class="keyword">select</span>  </span><br><span class="line">    	<span class="keyword">distinct</span>(t2.videoId), t3.category  </span><br><span class="line">    <span class="keyword">from</span>  t2 </span><br><span class="line">    <span class="keyword">inner</span> <span class="keyword">join</span>  gulivideo_orc t3 </span><br><span class="line">    <span class="keyword">on</span> t2.videoId = t3.videoId</span><br><span class="line">) t4 <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) t_catetory <span class="keyword">as</span> category_name;  </span><br></pre></td></tr></table></figure>



<ol start="4">
<li>按照视频类别进行分组，统计每组视频个数，然后排行</li>
</ol>
<p>最终代码：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>   </span><br><span class="line">	category_name <span class="keyword">as</span> <span class="keyword">category</span>, <span class="keyword">count</span>(t5.videoId) <span class="keyword">as</span> hot  </span><br><span class="line"><span class="keyword">from</span> ( <span class="keyword">select</span> videoId, category_name <span class="keyword">from</span> (  </span><br><span class="line">    	<span class="keyword">select</span>  <span class="keyword">distinct</span>(t2.videoId), t3.category  <span class="keyword">from</span> (  </span><br><span class="line">            <span class="keyword">select</span>   <span class="keyword">explode</span>(relatedId) <span class="keyword">as</span> videoId  <span class="keyword">from</span> (  </span><br><span class="line">                <span class="keyword">select</span>  *  <span class="keyword">from</span>  gulivideo_orc  <span class="keyword">order</span> <span class="keyword">by</span>  views  <span class="keyword">desc</span> <span class="keyword">limit</span>   <span class="number">50</span></span><br><span class="line">            ) t1</span><br><span class="line">        ) t2  <span class="keyword">inner</span> <span class="keyword">join</span>  gulivideo_orc t3 <span class="keyword">on</span> t2.videoId = t3.videoId</span><br><span class="line">	 ) t4 <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) t_catetory <span class="keyword">as</span> category_name) t5 </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span>   category_name  <span class="keyword">order</span> <span class="keyword">by</span>   hot  <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>





<h3 id="10-4-5-统计每个类别中的视频热度Top10，以Music为例"><a href="#10-4-5-统计每个类别中的视频热度Top10，以Music为例" class="headerlink" title="10.4.5 统计每个类别中的视频热度Top10，以Music为例"></a>10.4.5 统计每个类别中的视频热度Top10，以Music为例</h3><p>思路：</p>
<p>1)要想统计Music类别中的视频热度Top10，需要先找到Music类别，那么就需要将category展开，所以可以创建一张表用于存放categoryId展开的数据。</p>
<ol start="2">
<li><p>向category展开的表中插入数据。</p>
</li>
<li><p>统计对应类别（Music）中的视频热度。</p>
</li>
</ol>
<p>最终代码：</p>
<p>创建表类别表：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_category(  </span><br><span class="line">    videoId <span class="keyword">string</span>,  uploader <span class="keyword">string</span>,  age <span class="built_in">int</span>,  categoryId <span class="keyword">string</span>,  <span class="keyword">length</span> <span class="built_in">int</span>,   </span><br><span class="line">    views <span class="built_in">int</span>,   rate <span class="built_in">float</span>,   ratings <span class="built_in">int</span>,   comments <span class="built_in">int</span>,  relatedId <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span>  </span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;&amp;&quot;</span>  </span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc; </span><br></pre></td></tr></table></figure>

<p>向类别表中插入数据：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> gulivideo_category  </span><br><span class="line"><span class="keyword">select</span>   </span><br><span class="line">	videoId, uploader, age, categoryId, <span class="keyword">length</span>, views, rate, ratings, comments, relatedId <span class="keyword">from</span>  gulivideo_orc </span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) catetory <span class="keyword">as</span> categoryId;</span><br></pre></td></tr></table></figure>

<p>统计Music类别的Top10（也可以统计其他）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>   </span><br><span class="line">	videoId,  views </span><br><span class="line"><span class="keyword">from</span>  gulivideo_category  </span><br><span class="line"><span class="keyword">where</span>  categoryId = <span class="string">&quot;Music&quot;</span>  </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span>  views  <span class="keyword">desc</span> <span class="keyword">limit</span>  <span class="number">10</span>;</span><br></pre></td></tr></table></figure>




<h3 id="10-4-6-统计每个类别中视频流量Top10，以Music为例"><a href="#10-4-6-统计每个类别中视频流量Top10，以Music为例" class="headerlink" title="10.4.6 统计每个类别中视频流量Top10，以Music为例"></a>10.4.6 统计每个类别中视频流量Top10，以Music为例</h3><p>思路：</p>
<ol>
<li><p>创建视频类别展开表（categoryId列转行后的表）</p>
</li>
<li><p>按照ratings排序即可</p>
</li>
</ol>
<p>最终代码：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>   </span><br><span class="line">	videoId,  views,  ratings  </span><br><span class="line"><span class="keyword">from</span>   gulivideo_category  </span><br><span class="line"><span class="keyword">where</span>   categoryId = <span class="string">&quot;Music&quot;</span>  </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span>   ratings  <span class="keyword">desc</span> <span class="keyword">limit</span>   <span class="number">10</span>; </span><br></pre></td></tr></table></figure>


<h3 id="10-4-7-统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频"><a href="#10-4-7-统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频" class="headerlink" title="10.4.7 统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频"></a>10.4.7 统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频</h3><p>思路：</p>
<ol>
<li>先找到上传视频最多的10个用户的用户信息</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>  *  </span><br><span class="line"><span class="keyword">from</span>   gulivideo_user_orc  </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span>   videos  <span class="keyword">desc</span> <span class="keyword">limit</span>   <span class="number">10</span>; </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>2)通过uploader字段与gulivideo_orc表进行join，得到的信息按照views观看次数进行排序即可。</p>
<p>最终代码：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>   </span><br><span class="line"></span><br><span class="line">	t2.videoId,   t2.views,  t2.ratings,  t1.videos,  t1.friends  </span><br><span class="line">	</span><br><span class="line"><span class="keyword">from</span> ( <span class="keyword">select</span>  *  <span class="keyword">from</span>  gulivideo_user_orc  <span class="keyword">order</span> <span class="keyword">by</span>  videos <span class="keyword">desc</span>   <span class="keyword">limit</span>   <span class="number">10</span>) t1 </span><br><span class="line"></span><br><span class="line"><span class="keyword">join</span>  gulivideo_orc t2 <span class="keyword">on</span>  t1.uploader = t2.uploader  </span><br><span class="line"></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span>  views <span class="keyword">desc</span>  <span class="keyword">limit</span>   <span class="number">20</span>; </span><br></pre></td></tr></table></figure>




<h3 id="10-4-8-统计每个类别视频观看数Top10"><a href="#10-4-8-统计每个类别视频观看数Top10" class="headerlink" title="10.4.8 统计每个类别视频观看数Top10"></a>10.4.8 统计每个类别视频观看数Top10</h3><p>思路：</p>
<ol>
<li>先得到categoryId展开的表数据</li>
</ol>
<p>2)子查询按照categoryId进行分区，然后分区内排序，并生成递增数字，该递增数字这一列起名为rank列</p>
<ol start="3">
<li>通过子查询产生的临时表，查询rank值小于等于10的数据行即可。</li>
</ol>
<p>最终代码：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>   t1.*  <span class="keyword">from</span> (  </span><br><span class="line">    <span class="keyword">select</span> </span><br><span class="line">    	videoId, categoryId,  views, </span><br><span class="line">    	row_number() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> categoryId <span class="keyword">order</span> <span class="keyword">by</span> views <span class="keyword">desc</span>) <span class="keyword">rank</span> </span><br><span class="line">    <span class="keyword">from</span> gulivideo_category</span><br><span class="line">) t1 <span class="keyword">where</span>  <span class="keyword">rank</span> &lt;= <span class="number">10</span>; </span><br></pre></td></tr></table></figure>




<h1 id="第11章-常见错误及解决方案"><a href="#第11章-常见错误及解决方案" class="headerlink" title="第11章 常见错误及解决方案"></a>第11章 常见错误及解决方案</h1><p>1）SecureCRT 7.3出现乱码或者删除不掉数据，免安装版的SecureCRT<br>卸载或者用虚拟机直接操作或者换安装版的SecureCRT</p>
<p>2）连接不上mysql数据库</p>
<p>（1）导错驱动包，应该把mysql-connector-java-5.1.27-bin.jar导入/opt/module/hive/lib的不是这个包。错把mysql-connector-java-5.1.27.tar.gz导入hive/lib包下。</p>
<p>（2）修改user表中的主机名称没有都修改为%，而是修改为localhost</p>
<p>3）hive默认的输入格式处理是CombineHiveInputFormat，会对小文件进行合并。</p>
<p>hive (default)&gt; set hive.input.format;</p>
<p>hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</p>
<blockquote>
<p>  可以采用HiveInputFormat就会根据分区数输出相应的文件。</p>
</blockquote>
<p>hive (default)&gt; set<br>hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</p>
<p>4）不能执行mapreduce程序</p>
<p>可能是hadoop的yarn没开启。</p>
<p>5）启动mysql服务时，报MySQL server PID file could not be found! 异常。</p>
<p>在/var/lock/subsys/mysql路径下创建hadoop102.pid，并在文件中添加内容：4396</p>
<p>6）报service mysql status MySQL is not running, but lock file<br>(/var/lock/subsys/mysql[失败])异常。</p>
<p>解决方案：在/var/lib/mysql 目录下创建： -rw-rw—-. 1 mysql mysql 5 12月 22<br>16:41 hadoop102.pid 文件，并修改权限为 777。</p>
<p>7）JVM堆内存溢出</p>
<p>描述：java.lang.OutOfMemoryError: Java heap space</p>
<p>解决：在yarn-site.xml中加入如下代码</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>2.1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.child.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xmx1024m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>


      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color2">Detail</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color5">Hive</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/Hive//" class="article-tag-list-link color5">Hive</a>
        		</li>
      		
		</ul>
	</div>


      

      
        
<div class="share-btn share-icons tooltip-left">
  <div class="tooltip tooltip-east">
    <span class="tooltip-item">
      <a href="javascript:;" class="share-sns share-outer">
        <i class="icon icon-share"></i>
      </a>
    </span>
    <span class="tooltip-content">
      <div class="share-wrap">
        <div class="share-icons">
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="icon icon-weibo"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="icon icon-weixin"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="icon icon-qq"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="icon icon-douban"></i>
          </a>
          <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a>
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="icon icon-facebook"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="icon icon-twitter"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="icon icon-google"></i>
          </a>
        </div>
      </div>
    </span>
  </div>
</div>

<div class="page-modal wx-share js-wx-box">
    <a class="close js-modal-close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//pan.baidu.com/share/qrcode?url=http://iscurry.com/2020/01/01/Hive(2)%E8%AF%A6%E7%BB%86/" alt="微信分享二维码">
    </div>
</div>

<div class="mask js-mask"></div>
      
      <div class="clearfix"></div>
    </div>
  </div>

</article>


  
<nav id="article-nav">
  
    <a href="/2020/01/01/Hive(1)/" id="article-nav-newer" class="article-nav-link-wrap">
      <i class="icon-circle-left"></i>
      <div class="article-nav-title">
        
          Hive(1)
        
      </div>
    </a>
  
  
    <a href="/2020/01/01/Hive(3)%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Hive开窗函数</div>
      <i class="icon-circle-right"></i>
    </a>
  
</nav>


<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>





  
<div id="gitalk-container" style="padding: 0px 30px 0px 30px;"></div> 

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script type="text/javascript">

if(true){
	var gitalk = new Gitalk({
  	clientID: '82453c023e54ba4f5997',
  	clientSecret: 'cf442cc48cb74834bae9f39b7f5cf8b25d117387',
  	repo: 'gitalk-comment',
  	owner: 'curryfor369',
  	admin: ['curryfor369'],
  	id: 'Wed Jan 01 2020 19:20:20 GMT+0800',
  	distractionFreeMode: 'true'
})
gitalk.render('gitalk-container') 
}
</script>


  
  
  

  

  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2020 curry
    	</div>
      	<div class="footer-right">
      		<a href="javascript:;" onclick="alert('Email: nba@iscurry.com')"  target="_blank">Email: nba@iscurry.com</a>
      	</div>
    </div>
  </div>
  
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/love.js"></script>
</footer>


    </div>
    <script>
	var yiliaConfig = {
		mathjax: false,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: true,
		root: "/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&laquo; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &raquo;</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/slider.e37972.js")}()</script>



    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">所有文章</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">友链</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">关于我</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Flume</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Hbase</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Hadoop</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Detail</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Spark</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Hive</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Linux</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Sqoop</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">hexo</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Kafka</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
        <ul class="search-ul">
          
            <li class="search-li">
              <a href="https://www.fszb8.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>飞速直播</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.feisuzhibo.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>飞速直播2</a>
            </li>
          
            <li class="search-li">
              <a href="http://www.sjzfgw.cn/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>微直播吧</a>
            </li>
          
            <li class="search-li">
              <a href="https://smzb.cn/room/19082" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>山猫直播</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.dbwf88.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>JRS直播</a>
            </li>
          
            <li class="search-li">
              <a href="http://www.zhuafan.live/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>抓饭直播</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.cnmysoft.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>黑土直播</a>
            </li>
          
            <li class="search-li">
              <a href="http://www.kanqiuba.net/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>看球吧(录像)</a>
            </li>
          
            <li class="search-li">
              <a href="http://www.zhiboba.tv/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>直播吧(录像)</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.uusnba.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>搜球吧(录像)</a>
            </li>
          
            <li class="search-li">
              <a href="http://ziyuannba.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>NBA视频资源(录像)</a>
            </li>
          
            <li class="search-li">
              <a href="/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>------------</a>
            </li>
          
            <li class="search-li">
              <a href="https://typecho.in/xing" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>笔记</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.processon.com/diagrams" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>脑图</a>
            </li>
          
            <li class="search-li">
              <a href="https://withpinbox.com/items" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>书签</a>
            </li>
          
            <li class="search-li">
              <a href="https://mail.88.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>88邮箱</a>
            </li>
          
        </ul>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">Email： nba@iscurry.com</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
<!-- 代码块复制功能 -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.js"></script>
<script type="text/javascript" src="https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
<script type="text/javascript" src="/js/clipboard_use.js"></script>

</body>
<script>
    var hide = false;
    function myFunction(x) {
        x.classList.toggle("change");
        if(hide == false){
            $(".left-col").css('display', 'none');
            $(".mid-col").css("left", 6);
            $(".tools-col").css('display', 'none');
            $(".tools-col.hide").css('display', 'none');
            hide = true;
        }else{
            $(".left-col").css('display', '');
            $(".mid-col").css("left", 300);
            $(".tools-col").css('display', '');
            $(".tools-col.hide").css('display', '');
            hide = false;
        }
    }
</script>
</html>