<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Curry&#39;s Blog</title>
  
  
  <link href="http://iscurry.com/atom.xml" rel="self"/>
  
  <link href="http://iscurry.com/"/>
  <updated>2020-12-22T05:32:39.124Z</updated>
  <id>http://iscurry.com/</id>
  
  <author>
    <name>curry</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CNN-NLP-基础</title>
    <link href="http://iscurry.com/2020/12/22/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CNLP-%E5%9F%BA%E7%A1%80/"/>
    <id>http://iscurry.com/2020/12/22/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CNLP-%E5%9F%BA%E7%A1%80/</id>
    <published>2020-12-22T05:28:18.000Z</published>
    <updated>2020-12-22T05:32:39.124Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>1、百度百科、维基百科 - 基本介绍</p><p>2、Magi - 相关内容 【Magi是一个自然语言搜索引擎，通过自然语言算法处理，直接给出问题答案】</p><p>4、google、 百度、技术论坛、博客</p><p>5、 知乎专栏 - 通俗理解</p><p>6、paper</p></blockquote><h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><h2 id="1、是什么"><a href="#1、是什么" class="headerlink" title="1、是什么"></a>1、是什么</h2><ul><li>一种算法，基于卷积的数学运算</li></ul><h2 id="2、干什么的"><a href="#2、干什么的" class="headerlink" title="2、干什么的"></a>2、干什么的</h2><ul><li>对输入对象做模式匹配</li></ul><h2 id="3、有什么用"><a href="#3、有什么用" class="headerlink" title="3、有什么用"></a>3、有什么用</h2><ul><li>分类</li></ul><h2 id="4、用在哪"><a href="#4、用在哪" class="headerlink" title="4、用在哪"></a>4、用在哪</h2><ul><li>图片分类，语音识别，NLP</li></ul><a id="more"></a><h1 id="CNN–卷积神经网络"><a href="#CNN–卷积神经网络" class="headerlink" title="CNN–卷积神经网络"></a>CNN–卷积神经网络</h1><h2 id="1、-定义"><a href="#1、-定义" class="headerlink" title="1、 定义"></a>1、 定义</h2><blockquote><p>Convolution Neural Network，简称 CNN，是一种算法，是==深度学习==的一种代表==算法==.</p><p>在人工智能领域，有一个方法叫机器学习。在机器学习这个方法里，有一种算法叫深度学习。CNN是一类包含==卷积计算==的且具有==深度结构==的==前馈神经网络==，具有==表征学习==能力，能按阶层对输入信息进行==平移不变分类==，称为==平移不变人工神经网络==，研究与二十世纪80-90年代，二十一世纪后，快速发展，属于人工智能学科，应用于计算机视觉、==自然语言处理==等领域，时间延迟网络和LeNet-5是最早出现的卷积神经网络</p></blockquote><h2 id="2、-定义的延伸"><a href="#2、-定义的延伸" class="headerlink" title="2、 定义的延伸"></a>2、 定义的延伸</h2><ol><li><p>什么是深度学习？</p><blockquote><p>深度学习是一种学习方式，指的是采用深度模型进行学习，不是模型.</p><p>深度学习是学习样本数据的内在规律和表示层次，最终目标是让机器能够像人一样具有分析学习能力，能够识别文字、图像和声音等数据</p></blockquote></li><li><p>深度学习还有什么算法？</p><blockquote><p>深度学习典型模型：</p><ol><li>卷积神经网络模型</li><li>深度信任网络模型</li><li>堆栈自编码网络模型</li></ol></blockquote></li><li><p>什么是机器学习？</p><blockquote><p>专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，所谓人工智能并不是真的智能，而是经过大量数据进行训练后，依照经验进行判断</p></blockquote></li><li><p>机器学习算法还有什么？</p><blockquote><p>决策树算法，朴素贝斯算法，支持向量机，人工神经网络，深度学习 等</p></blockquote></li><li><p>什么是神经网络？</p><blockquote><p>这个词来自生物学，数十亿个神经元细胞以不同方式连接形成网络 这是神经网络，而我们所目前所指的应该是“人工神经网络”</p></blockquote></li><li><p>什么是人工神经网络</p><blockquote><p>人工神经网络是指 尝试模拟 生物学上的神经网络，模拟生物学上的这种 体系结构及操作</p><p>不同类型的神经网络体系结构有很大不同，我们知道的只是神经元的基本结构</p></blockquote></li><li><p>什么是卷积计算？</p><blockquote><p>是一种线性计算, 卷积核在原始矩阵进行滑动，做加权求和运算</p></blockquote></li><li><p>怎样体现深度结构?</p><blockquote><p>多个隐藏层</p></blockquote></li><li><p>什么是前馈神经网络？</p><blockquote><ol><li>是==最简单==的一种==神经网络==，单向多层，各层无反馈</li><li>==单向多层==，各神经元分层排列，每个神经元只与前一层神经元相连，接收前一层的输出，并输出给下一层，各层==无反馈==</li><li>包含==3类节点==<ol><li>输入节点：外层信息输入，不进行任何计算，仅向下一层传递。相当于头</li><li>隐藏节点：接收上一层节点输入，进行计算，信息传下一层。 相当于体</li><li>输出节点：接收上一层节点输入，计算并将结果输出。相当于尾</li></ol></li><li>输入输出节点必须有，可以没有隐藏节点，例如==单层感知器==。有多个==隐藏层==的是==多层感知器==</li></ol></blockquote></li><li><p>什么是反馈神经网络</p><blockquote><p>反馈神经网络（FeedBack NN ):又称递归网络、回归网络，是一种将输出经过一步时移再接入到输入层的神经网络系统。这类网络中，神经元可以互连，有些神经元的输出会被反馈至同层甚至前层的神经元。常见的有Hopfield神经网络、Elman神经网络、Boltzmann机等。</p></blockquote></li><li><p>前馈神经网络和反馈神经网络的主要区别：</p><blockquote><p>前馈神经网络各层神经元之间无连接，神经元只接受上层传来的数据，处理后传入下一层，数据正向流动；反馈神经网络层间神经元有连接，数据可以在同层间流动或反馈至前层。</p><p>前馈神经网络不考虑输出与输入在时间上的滞后效应，只表达输出与输入的映射关系；反馈神经网络考虑输出与输入之间在时间上的延迟，需要用动态方程来描述系统的模型。</p><p>前馈神经网络的学习主要采用误差修正法（如BP算法），计算过程一般比较慢，收敛速度也比较慢；反馈神经网络主要采用Hebb学习规则，一般情况下计算的收敛速度很快。</p><p>相比前馈神经网络，反馈神经网络更适合应用在联想记忆和优化计算等领域。</p></blockquote></li><li><p>什么是感知机</p><blockquote><p>感知器（Perceptron): 用于线性客服模式分类的最简单的神经网络模型。由一个具有可调树突权值和偏置的神经元组成。1958年Frank Rosenblatt提出一种具有单层计算单元的神经网络，即为Perception。其本质是一个非线性前馈网络，同层内无互联，不同层间无反馈，由下层向上层传递。其输入、输出均为离散值，神经元对输入加权求和后，由阈值函数决定其输出。感知器实际上是一个简单的单层神经网络模型。</p></blockquote></li><li><p>什么是表征学习？为什么说它具有这个能力，从哪体现</p><blockquote><p>能从输入信息中提取高阶特征。在卷积核池化层都能响应输入特征的平移不变性</p></blockquote></li><li><p>什么是按阶层处理信息？</p><blockquote><p>CNN 分为 输入，隐藏，输出层</p><p>隐藏层 分为 卷积，池化，全连接层</p></blockquote></li><li><p>什么是平移不变</p><blockquote><p>原始图像某个像素在局部又微小变化，不影响池化的结果</p></blockquote></li><li><p>什么是时间延迟网络，有什么用？</p><blockquote><p>是一个应用于语音识别的卷积神经网络，基于BP算法进行学习</p></blockquote></li><li><p>什么是过拟合(over-fitting)</p><blockquote><p>模型在新数据集上的泛化能力很低，称为过拟合</p></blockquote></li><li><p>什么是泛化能力</p><blockquote><p>模型在数据全集上的预测准确度。包括对新数据和已经训练的数据的预测。泛化能力是对模型性能的评价</p></blockquote></li><li><p>什么是欠拟合</p><blockquote><p>与过拟合相对，可以说是模型复杂度太低，没法很好的学到数据背后的规律。开普勒在总结天体运行规律之前，他的老师第谷记录了很多的运行数据，但是都没法用数据去解释天体运行的规律并预测，这就是在天体运行数据上,人们一直处于欠拟合的状态，只知道记录过的过去是这样运行的，但是不知道道理是什么</p></blockquote><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20201221144408.png" alt="image-20201221144407083"></p></li><li><p>随机梯度下降法</p><blockquote><p>梯度就是 偏导数。当导数为0，有极值，左正右负，极大值，反之，极小值。当单调递增，函数最小值在左边，减小x降低函数值，当函数单调递减，导数为负，梯度为负，函数最小值在右边，增大x值，这个调整x值找极小值的过程是梯度下降</p><p>梯度下降法： 就是找极小值9</p><p>当自变量为几时，导数值最小，导数就是梯度，梯度下降，梯度最小。当梯度最小时，自变量是多少？求出自变量，更新权重和参数值，损失函数值减小，误差就小了</p><p><a href="https://blog.csdn.net/qq_38890412/article/details/109193294">https://blog.csdn.net/qq_38890412/article/details/109193294</a></p></blockquote></li><li><p>BP算法</p><blockquote><p>正向传播 （求损失）</p><p>反向传播  (误差回传)，更新权值和参数，重新正向传播，如此反复知道 误差小于指定值 &lt; 0.0001 则收敛</p></blockquote></li></ol><h2 id="3、CNN基础结构"><a href="#3、CNN基础结构" class="headerlink" title="3、CNN基础结构"></a>3、CNN基础结构</h2><ul><li>CNN目的：输入对象做模式匹配：分类</li></ul><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20201221100606.png" alt="image-20201221100547072"></p><ul><li>一个简单的CNN模型：<ul><li>输入层</li><li>隐藏层(卷积层，池化层，全连接层)</li><li>输出层</li></ul></li><li>基于简单模型处理的过程：输入，卷积，激活函数，池化，全连接，输出</li></ul><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20201222132508.png" alt="image-20201221100553015"></p><h3 id="1、输入层"><a href="#1、输入层" class="headerlink" title="1、输入层"></a>1、输入层</h3><ol><li>本质上图片是一组矩阵，下面图片是一个灰色图像，只有一个通道，RGB彩色的是三个通道</li></ol><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20201221100217.gif"></p><ol start="2"><li>通道：用于指代图像的某个组成部分。来自标准数码相机的图像将具有三个通道-红色，绿色和蓝色-您可以将它们想象成彼此堆叠的三个2d矩阵（每种颜色一个），每个像素值的范围为0到255。</li></ol><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20201221101419.png" alt="image-20201221101418269"></p><h3 id="2、卷积层"><a href="#2、卷积层" class="headerlink" title="2、卷积层"></a>2、卷积层</h3><p>卷积层： 卷积核在输入对象上滑动做加权求和运算 生成特征图 Feature Map</p><h4 id="1-卷积核-Kernel-Filter"><a href="#1-卷积核-Kernel-Filter" class="headerlink" title="1. 卷积核(Kernel,Filter)"></a>1. 卷积核(Kernel,Filter)</h4><blockquote><p>又名 卷积核，过滤器，滤波器，内核，特征检测器，值是权重。是一个矩阵，在原始矩阵上滑动，过滤特定数据。是原始图像的特征检测器。过滤器需要4个参数确定 宽度×高度×通道数×过滤器个数，通道数与上层通道数一致，过滤器个数和本层输出通道数一致。RGB三原色就是三个通道,黑白图片1个通道，彩色的三个通道</p><p>所谓权值共享、参数共享说的是 所有被扫描的像素每一次都是用这个滤波器进行计算，这个Filter里的值为权值，所以权值共享</p><p>多个卷积核 可发现不同角度的特征， 多个卷积层可以捕捉更全面的特征。处于卷积神经网络更深的层的单元接受域比浅层的接受域更大</p></blockquote><ol><li>Sobel 滤波器： 边缘检测器：用输出图像中更亮的像素表示原始图像中存在的边缘</li></ol><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20201222132518.png"></p><ol start="2"><li>通过Sobel滤波器卷积后的效果</li></ol><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20201221095351.png" alt="image-20201221095350621"></p><ol start="3"><li><p>通过不同的卷积核对原始图像提取不同的特征图</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20201221103225.png" alt="image-20201221103224000"></p></li></ol><h4 id="2-卷积-Convolution"><a href="#2-卷积-Convolution" class="headerlink" title="2. 卷积(Convolution)"></a>2. 卷积(Convolution)</h4><blockquote><p>卷积的主要目的是从输入图像中提取特征。通过卷积核在原始图像上滑动，做加权求和运算</p><p>这里不讨论卷积的数学细节，只尝试了解卷积在处理图像时的原理</p></blockquote><p><strong>1、  单输入通道卷积运算 (黑白图)</strong></p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20201221090224.png" alt="image-20201221090223401"></p><p><strong>2、 多输入通道卷积运算 （彩图）</strong></p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20201221085832.png" alt="image-20201221085830867"></p><p><strong>3、 多个卷积核 卷积运算（单通道）</strong></p><ul><li><strong><font color="#FF0000">下层通道数取决于上层Filter数 </font></strong></li></ul><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20201221091236.png" alt="image-20201221091233996"></p><p><strong>4、 填充</strong></p><blockquote><p>Full模式: 从卷积核（fileter）和输入刚相交开始做卷积。没有元素的部分做补0操作</p><p>Valid模式: 卷积核和输入完全相交开始做卷积，这种模式不需要补0</p><p>Same模式：当卷积核的中心C和输入开始相交时做卷积。没有元素的部分做补0操作。</p><p>注： 卷积核Size一般为奇数，因为有中心像素点，便于定位卷积核</p></blockquote><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20201221090626.png" alt="image-20201221090625260"></p><p><strong>5、步长</strong>:输入矩阵上滑动滤镜矩阵的像素数</p><h4 id="3-引入非线性-激活函数"><a href="#3-引入非线性-激活函数" class="headerlink" title="3. 引入非线性(激活函数)"></a>3. 引入非线性(激活函数)</h4><blockquote><p>为什么要引入非线性？</p><p>卷积运算是线性运算，元素=矩阵乘法和加法运算，在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了</p><p>使用非线性函数(激活函数)来解决非线性问题。常用的例如ReLU， tanh或 Sigmoid</p><p>ReLU：线性整流函数（Rectified Linear Unit）这个函数是按照元素操作的，将特征图中所有的负值元素替换为0。</p></blockquote><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20201221105635.png" alt="image-20201221105634367"></p><h3 id="3、池化层-汇合层"><a href="#3、池化层-汇合层" class="headerlink" title="3、池化层(汇合层)"></a>3、池化层(汇合层)</h3><blockquote><p>空间池化（也称为子采样或下采样）降低了每个特征图的维数，但保留了最重要的信息，逐渐减小输入表示的空间大小，在一定程度上避免过拟合</p><p>空间池不同类型：常用：最大值，最小值，平均值，总和，随机</p></blockquote><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20201222132529.png" alt="image-20201221092632319"></p><ol><li><p>最大池化： 取局部最大值，可更好的保留纹理特征，只关心物体是否出现而不关系具体出现位置</p></li><li><p>平均池化：取局部平均值，保留整体特征，突出背景信息</p></li><li><p>随机池化：最大值被选中概率最大，最大化保证了Max值的取值，又确保不完全是Max值起作用，造成过度失真，一定程度上避免过拟合</p></li><li><p>重叠池化</p></li></ol><ul><li>一般CNN中使用的池化都是不重叠的，但是池化是可重叠的。卷积类似，可定义步长等参数；不同的是，卷积是窗口元素和卷积核内积运算，池化是取最大值等</li></ul><p><strong>==到目前为止，这些层 在图像中提取有用的特征，引入非线性并减小特征尺寸，同时旨在使特征在一定程度上与比例尺和平移相等，也就是平移不变性==</strong></p><h3 id="4、全连接层"><a href="#4、全连接层" class="headerlink" title="4、全连接层"></a>4、全连接层</h3><blockquote><p>作用：对输入图像分类和 对输入图像的高级功能(卷积和汇合层输出)的 非线性组合</p><p>完全连接层是传统的多层感知器, 在输出层中使用softmax激活功能（也可以使用其他分类器（例如SVM）)</p><p>“完全连接”: 表示上一层中的每个神经元都连接到下一层中的每个神经元</p><p>卷积和汇总层的输出 表示输入图像的高级功能。完全连接层的目的是使用这些功能 根据训练数据集 将输入图像分类为各种类别</p><p>除了分类之外，添加完全连接的层也是学习这些功能的非线性组合的一种通用方法。来自卷积层和池化层的大多数特征可能对分类任务很有用，但是这些特征的组合可能甚至更好。</p><p>完全连接层的输出概率之和为1。这可以通过使用SoftMax作为完全连接层的输出层中的激活函数来确保。Softmax函数采用任意实值得分的向量，并将其压扁为零和一之间的值之和，这些值 和为一</p></blockquote><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20201221112325.png" alt="image-20201221112324548"></p><p><strong>==卷积+合并层充当输入图像的特征提取器，而完全连接层充当分类器==</strong></p><h3 id="5、使用BP算法进行训练"><a href="#5、使用BP算法进行训练" class="headerlink" title="5、使用BP算法进行训练"></a>5、使用BP算法进行训练</h3><ul><li><p>BackPropagation 反向传播</p></li><li><p>以下图为例</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20201221121332.png" alt="image-20201221121331286"></p></li></ul><ul><li>训练步骤</li></ul><ol><li><p>使用随机值初始化所有过滤器和权重。filter的个数，权值，都是随机的</p></li><li><p>将训练图像作为输入，进行正向传播，并找到每个类别的输出概率。正向传播： 卷积，非线性化，池化，全连接层的正向传播</p><ul><li>假设输出为 [0.2,0.4,0.1,0.3]</li><li>由于权重是为第一个训练示例随机分配的，因此输出概率也是随机的。</li></ul></li><li><p>计算输出层的总误差，所有类别误差和 ∑½（目标概率–输出概率）²</p></li><li><p>利用反向传播计算所有权重误差梯度，使用梯度下降更新所有滤波器权重和参数值，以最大程度地减少输出误差。</p><ul><li>权重根据总误差调整</li><li>再次输入同一图像，输出概率可能是[0.1，0.1，0.7，0.1]，更接近目标矢量[0，0，1，0]。</li><li>这意味着网络已<em>学会</em>通过调整其权重/滤波器来正确分类此特定图像，从而减少输出误差。</li><li>诸如过滤器数量，过滤器大小，网络体系结构等参数在步骤1之前都已固定，在训练过程中不会更改，仅更新过滤器矩阵的值和连接权重。</li></ul></li><li><p>对训练集所有图像重复2-4</p></li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">至此，所有的参数和权重都已经优化，可以对训练集中的图像做正确分类</span><br><span class="line">当新的图像输入时，输出概率是通过之前训练好的权重和参数值来计算的。如果训练集足够大，那么这个卷积神经网络就能很好地将它们分类为正确的类别</span><br></pre></td></tr></table></figure><h2 id="4、通过实例更好的理解卷积神经网络"><a href="#4、通过实例更好的理解卷积神经网络" class="headerlink" title="==4、通过实例更好的理解卷积神经网络=="></a><strong>==4、通过实例更好的理解卷积神经网络==</strong></h2><p><a href="https://www.cs.ryerson.ca/~aharley/vis/conv/flat.html">https://www.cs.ryerson.ca/~aharley/vis/conv/flat.html</a></p><p><a href="https://www.cs.ryerson.ca/~aharley/vis/conv/">https://www.cs.ryerson.ca/~aharley/vis/conv/</a></p><h1 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h1><ol><li><p>是什么</p><blockquote><p>自然语言处理（简称NLP），是研究计算机处理人类语言的一门技术，是理解给定文本的含义与结构的流程, 最重要的是要让计算机理解.</p></blockquote><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20201222111246.png" alt="image-20201222105857511"></p></li></ol><h2 id="x、通过Demo更好的理解"><a href="#x、通过Demo更好的理解" class="headerlink" title="x、通过Demo更好的理解"></a>x、通过Demo更好的理解</h2><p><a href="https://explosion.ai/demos/displacy?text=I%20love%20NLP&amp;model=en_core_web_sm&amp;cpu=1&amp;cph=1">https://explosion.ai/demos/displacy?text=I%20love%20NLP&amp;model=en_core_web_sm&amp;cpu=1&amp;cph=1</a> </p><p><a href="https://explosion.ai/software">https://explosion.ai/software</a> 各种demo</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;1、百度百科、维基百科 - 基本介绍&lt;/p&gt;
&lt;p&gt;2、Magi - 相关内容 【Magi是一个自然语言搜索引擎，通过自然语言算法处理，直接给出问题答案】&lt;/p&gt;
&lt;p&gt;4、google、 百度、技术论坛、博客&lt;/p&gt;
&lt;p&gt;5、 知乎专栏 - 通俗理解&lt;/p&gt;
&lt;p&gt;6、paper&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;CNN&quot;&gt;&lt;a href=&quot;#CNN&quot; class=&quot;headerlink&quot; title=&quot;CNN&quot;&gt;&lt;/a&gt;CNN&lt;/h1&gt;&lt;h2 id=&quot;1、是什么&quot;&gt;&lt;a href=&quot;#1、是什么&quot; class=&quot;headerlink&quot; title=&quot;1、是什么&quot;&gt;&lt;/a&gt;1、是什么&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;一种算法，基于卷积的数学运算&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;2、干什么的&quot;&gt;&lt;a href=&quot;#2、干什么的&quot; class=&quot;headerlink&quot; title=&quot;2、干什么的&quot;&gt;&lt;/a&gt;2、干什么的&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;对输入对象做模式匹配&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;3、有什么用&quot;&gt;&lt;a href=&quot;#3、有什么用&quot; class=&quot;headerlink&quot; title=&quot;3、有什么用&quot;&gt;&lt;/a&gt;3、有什么用&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;分类&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;4、用在哪&quot;&gt;&lt;a href=&quot;#4、用在哪&quot; class=&quot;headerlink&quot; title=&quot;4、用在哪&quot;&gt;&lt;/a&gt;4、用在哪&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;图片分类，语音识别，NLP&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="CNN" scheme="http://iscurry.com/categories/CNN/"/>
    
    <category term="NLP" scheme="http://iscurry.com/categories/CNN/NLP/"/>
    
    
    <category term="CNN" scheme="http://iscurry.com/tags/CNN/"/>
    
    <category term="NLP" scheme="http://iscurry.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Spark(1)</title>
    <link href="http://iscurry.com/2020/06/25/spark/"/>
    <id>http://iscurry.com/2020/06/25/spark/</id>
    <published>2020-06-25T02:11:13.000Z</published>
    <updated>2020-09-25T02:22:31.367Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-分布式计算原理"><a href="#1-分布式计算原理" class="headerlink" title="1. 分布式计算原理"></a>1. 分布式计算原理</h1><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200706105944.png" alt="image-20200706105941183"></p><a id="more"></a><h1 id="2-类加载器"><a href="#2-类加载器" class="headerlink" title="2. 类加载器"></a>2. 类加载器</h1><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200706111730.png" alt="image-20200706111645484"></p><hr><blockquote><ul><li>jdk<ul><li>jre<ul><li>lib</li><li>classes  – 启动类加载器</li><li>.ext<ul><li>lib</li><li>classes  – 扩展类加载器</li></ul></li></ul></li></ul></li><li>用户自定义<ul><li>classes  – 应用类加载器</li><li>lib</li></ul></li></ul><ol><li>加载类时，遵循双亲委派机制</li><li>加载一个类，委派给扩展类加载器加载，扩展类加载器会委派给启动类加载器加载</li><li>若启动类加载器有则加载。无则返回Null， 扩展类加载器加载</li><li>扩展类加载器有则加载，无则抛出异常给应用类加载器，由应用类加载器加载</li><li>应用类加载器有则加载，无则跑出异常给jvm，classNotFoundException</li></ol></blockquote><hr><h1 id="3-加载文件的方式"><a href="#3-加载文件的方式" class="headerlink" title="3. 加载文件的方式"></a>3. 加载文件的方式</h1><ul><li>线程.现在的线程类加载器.获取自资源</li><li>通过classpath路径或项目路径直接加载</li></ul><h1 id="4-隐式转换"><a href="#4-隐式转换" class="headerlink" title="4. 隐式转换"></a>4. 隐式转换</h1><ul><li>rdd.reduceByKey   返回 pairRdd ，是因为rdd中有隐示转换的方法</li></ul><h1 id="5-集群角色"><a href="#5-集群角色" class="headerlink" title="5. 集群角色"></a>5. 集群角色</h1><ul><li><p>资源调度</p><ul><li>master 集群资源管理</li><li>worker 单点资源管理</li></ul></li><li><p>计算</p><ul><li>driver  计算驱动器，驱使计算执行，所有的逻辑在这里，main主线程，是入口, sparkContext</li><li>excutor  driver将计算发过来，真正执行计算，返回结果给driver，一个excuotr是启动一个task</li></ul></li><li><p>解耦</p><ul><li>applicationMaster   将计算和资源调度解耦， driver 启动在applictionMater里，AM与Master建立联系</li><li>container  将excutor封装，与worker解耦</li></ul></li></ul><h1 id="6-端口号"><a href="#6-端口号" class="headerlink" title="6. 端口号"></a>6. 端口号</h1><ul><li>8080  master的web访问端口</li><li>4040  driver的web访问端口，查看执行的任务</li><li>7077 spark集群对外提供服务的端口</li><li>18080  spark历史服务web端口</li></ul><h1 id="7-spark部署模式"><a href="#7-spark部署模式" class="headerlink" title="7. spark部署模式"></a>7. spark部署模式</h1><h2 id="1-本地模式【local】-spark-shell"><a href="#1-本地模式【local】-spark-shell" class="headerlink" title="1. 本地模式【local】 spark shell"></a>1. 本地模式【local】 spark shell</h2><h2 id="2-独立部署【standalone】"><a href="#2-独立部署【standalone】" class="headerlink" title="2. 独立部署【standalone】"></a>2. 独立部署【standalone】</h2><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h3><ol><li>standalone指的是所有的资源调度和计算都是spark集群完成，不借助其他框架，而Hadoop的独立是指所有的角色都在一台机器上</li></ol><h3 id="2-历史服务器"><a href="#2-历史服务器" class="headerlink" title="2. 历史服务器"></a>2. 历史服务器</h3><ul><li>spark-shell模式可以访问4040端口查看任务执行情况，而退出后就不能查看了</li><li>将历史服务器配置在hdfs上，并开启spark历史服务，通过18080端口访问，只要spark集群ok</li></ul><h3 id="3-高可用"><a href="#3-高可用" class="headerlink" title="3. 高可用"></a>3. 高可用</h3><ul><li>需要启动zookeeper</li></ul><h2 id="3-yarn部署"><a href="#3-yarn部署" class="headerlink" title="3. yarn部署"></a>3. yarn部署</h2><ul><li><p>client模式进程</p><ul><li><p>sparkSubmit</p></li><li><p>executorLauncher</p></li><li><p>CoarseGrainedExecutorBackend</p></li></ul></li><li><p>cluster模式进程</p><ul><li>sparkSubmit</li><li>applicationmaster</li><li>CoarseGrainedExecutorBackend</li></ul></li></ul><h2 id="4-messos部署【国内不常用】"><a href="#4-messos部署【国内不常用】" class="headerlink" title="4. messos部署【国内不常用】"></a>4. messos部署【国内不常用】</h2><h1 id="8-协议"><a href="#8-协议" class="headerlink" title="8. 协议"></a>8. 协议</h1><ol><li>本地文件协议  file:///</li><li>hdfs文件协议 hdfs://</li><li>spark协议  spark://</li></ol><h1 id="9-RDD"><a href="#9-RDD" class="headerlink" title="9. RDD"></a>9. RDD</h1><h2 id="1-简介-1"><a href="#1-简介-1" class="headerlink" title="1. 简介"></a>1. 简介</h2><ul><li>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据（计算）抽象。(适合并行计算的最小计算单元)</li><li>特点<ul><li>弹性</li><li>分区</li><li>只读</li><li>依赖（血缘）</li><li>缓存</li><li>checkpoint</li></ul></li></ul><h2 id="2-5大特性"><a href="#2-5大特性" class="headerlink" title="2. 5大特性"></a>2. 5大特性</h2><ol><li><p>A list of partitions  多个分区. 分区可以看成是数据集的基本组成单位</p></li><li><p>A function for computing each split  计算每个切片(分区)的函数.</p></li><li><p>A list of dependencies on other RDDs    与其他 RDD 之间的依赖关系</p></li><li><p>ptionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</p></li></ol><p>​        对存储键值对的 RDD, 还有一个可选的分区器.</p><ol start="5"><li><p>Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)</p><p>存储每个切片优先(preferred location)位置的列表,  移动计算比移动数据更高效</p></li></ol><h2 id="3-3中获取RDD的方式"><a href="#3-3中获取RDD的方式" class="headerlink" title="3. 3中获取RDD的方式"></a>3. 3中获取RDD的方式</h2><ol><li>从内存<ul><li>makeRDD(seq,partitions)</li><li>parallelize(seq,paritions)</li></ul></li><li>从外部文件<ul><li>sc.textFile()</li></ul></li><li>从一个RDD转换生成另一个RDD<ul><li>rdd1.map()</li></ul></li></ol><h2 id="4-分区数"><a href="#4-分区数" class="headerlink" title="4. 分区数"></a>4. 分区数</h2><ol><li>从内存读取的分区数</li><li>从外部文件读取的分区数</li></ol><h1 id="10-算子"><a href="#10-算子" class="headerlink" title="10. 算子"></a>10. 算子</h1><ul><li>转换算子</li><li>行动算子</li></ul><h2 id="1-map"><a href="#1-map" class="headerlink" title="1. map"></a>1. map</h2><ul><li>效率不如mapPartitions,但是如果数据量过大，考虑到内存问题，用这个，可以GC</li></ul><h2 id="2-mapPartitions"><a href="#2-mapPartitions" class="headerlink" title="2. mapPartitions"></a>2. mapPartitions</h2><ul><li>对每个分区进行操作，每个分区为一个整体，如果数据量过大，内存溢出，GC无法回收</li></ul><h2 id="3-mapPartitionsWithIndex"><a href="#3-mapPartitionsWithIndex" class="headerlink" title="3. mapPartitionsWithIndex"></a>3. mapPartitionsWithIndex</h2><ul><li>取到分区号</li></ul><h2 id="4-flatMap"><a href="#4-flatMap" class="headerlink" title="4. flatMap"></a>4. flatMap</h2><ul><li>转换 &amp;  扁平化</li></ul><h2 id="5-glom"><a href="#5-glom" class="headerlink" title="5. glom"></a>5. glom</h2><ul><li>将每个分区的数据放在数组中</li><li>求每个分区中最大值的和  rdd1.glom(datas=&gt;datas.max).sum</li></ul><h2 id="6-groupBy"><a href="#6-groupBy" class="headerlink" title="6. groupBy"></a>6. groupBy</h2><ul><li>rdd1.groupBy(num=&gt;num%2)</li></ul><h2 id="7-filter"><a href="#7-filter" class="headerlink" title="7. filter"></a>7. filter</h2><ul><li>过滤</li><li>rdd1.filter(n=&gt;n%2==0)</li></ul><h2 id="8-sample"><a href="#8-sample" class="headerlink" title="8. sample"></a>8. sample</h2><ul><li>sample(withReplacement, fraction, seed)</li></ul><ol><li>放回抽样</li><li>不放回抽样</li></ol><h2 id="9-distinct"><a href="#9-distinct" class="headerlink" title="9. distinct"></a>9. distinct</h2><ul><li>去重， 打乱重组，shuffle,  分区间数据有联系，分区等待，占内存，内存溢出，要落盘，io,效率低</li><li>例如 …byKey</li><li>前面的算子不需要落盘，不需要等待，每个分区间数据没关系</li></ul><h2 id="10-coalesce"><a href="#10-coalesce" class="headerlink" title="10. coalesce"></a>10. coalesce</h2><ul><li>重分区，无shffle，只是分区</li><li>有shuffle，数据均衡</li><li>无shuffle,快，可能会数据倾斜</li><li>有shuffle,落盘，Io，慢，最起码能跑通</li></ul><h2 id="11-repartition"><a href="#11-repartition" class="headerlink" title="11.  repartition"></a>11.  repartition</h2><ul><li>重分区，有shuffle, 底层是有shuffle的coalesce</li><li>repartition就是coalesce</li></ul><h2 id="12-sortBy"><a href="#12-sortBy" class="headerlink" title="12. sortBy"></a>12. sortBy</h2><h2 id="13-pipe-脚本管道"><a href="#13-pipe-脚本管道" class="headerlink" title="13. pipe 脚本管道"></a>13. pipe 脚本管道</h2><ul><li><p>将rdd的数据作为参数传进来，每个echo为返回结果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/opt/datas/pipe.sh</span><br><span class="line"><span class="comment">#/bin/bash</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;hello&quot;</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">read</span> line;<span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;&gt;&gt;&gt;&quot;</span><span class="variable">$line</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd1.pipe(<span class="string">&quot;/opt/...&quot;</span>).collect</span><br></pre></td></tr></table></figure></li></ul><h2 id="双value类型交互"><a href="#双value类型交互" class="headerlink" title="双value类型交互"></a>双value类型交互</h2><ol><li>union</li><li>subtract</li><li>intersection</li><li>cartesian</li><li>zip</li></ol><h2 id="k-v类型算子"><a href="#k-v类型算子" class="headerlink" title="k-v类型算子"></a>k-v类型算子</h2><h2 id="1-reduceByKey"><a href="#1-reduceByKey" class="headerlink" title="1.  reduceByKey"></a>1.  reduceByKey</h2><ul><li>有与聚合功能，mapsideCombine,</li></ul><h2 id="2-groupByKey"><a href="#2-groupByKey" class="headerlink" title="2. groupByKey"></a>2. groupByKey</h2><ul><li>先group 效率远远低于reduceByKey</li></ul><h2 id="3-aggregateByKey"><a href="#3-aggregateByKey" class="headerlink" title="3. aggregateByKey"></a>3. aggregateByKey</h2><ul><li>初始值，分区内聚合，分区间聚合</li><li>求出每个分区key最大值，并相加，初始值为0</li></ul><h2 id="4-foldByKey"><a href="#4-foldByKey" class="headerlink" title="4. foldByKey"></a>4. foldByKey</h2><ul><li>和aggregateByKey一样，当分区内和分区间操作一致时，用这个</li></ul><h2 id="5-combineByKey"><a href="#5-combineByKey" class="headerlink" title="5. combineByKey"></a>5. combineByKey</h2><ul><li>对每个值的结构操作，分区内聚合，分区间聚合</li><li>求平均值</li></ul><h2 id="6-sortByKey"><a href="#6-sortByKey" class="headerlink" title="6. sortByKey"></a>6. sortByKey</h2><h2 id="7-mapValues"><a href="#7-mapValues" class="headerlink" title="7. mapValues"></a>7. mapValues</h2><ul><li>只操作value</li></ul><h2 id="8-join"><a href="#8-join" class="headerlink" title="8. join"></a>8. join</h2><h2 id="9-cogroup"><a href="#9-cogroup" class="headerlink" title="9. cogroup"></a>9. cogroup</h2><h1 id="记录的话"><a href="#记录的话" class="headerlink" title="记录的话"></a>记录的话</h1><ol><li><p>栈上分配   逃逸分析    GC</p></li><li><p>一个分区启动一个task,每个task之间相对独立，保存为文件时有多少个task就保存多少个文件，每个文件的偏移量这样算</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positions</span></span>(length: <span class="type">Long</span>, numSlices: <span class="type">Int</span>): <span class="type">Iterator</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = &#123;</span><br><span class="line"> (<span class="number">0</span> until numSlices).iterator.map &#123; i =&gt;</span><br><span class="line">   <span class="keyword">val</span> start = ((i * length) / numSlices).toInt</span><br><span class="line">   <span class="keyword">val</span> end = (((i + <span class="number">1</span>) * length) / numSlices).toInt</span><br><span class="line">   (start, end)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="number">0.</span> （当前分区*长度/分区数，（当前分区+<span class="number">1</span>）*长度/分区数）</span><br><span class="line"><span class="number">1.</span> <span class="number">3</span>个分区，seq长度为<span class="number">5</span> =&gt; <span class="number">0</span> util <span class="number">3</span>  (<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="number">2.</span> (<span class="number">0</span>,<span class="number">1</span>)  <span class="number">0</span></span><br><span class="line">(<span class="number">1</span>,<span class="number">3</span>) <span class="number">1</span>,<span class="number">2</span></span><br><span class="line">(<span class="number">3</span>,<span class="number">5</span>) <span class="number">3</span>,<span class="number">4</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>lamda表达式在scala中，参数时，传入和传出一致是不能用下划线，因为无法识别</p></li><li><p>从外部读取生成的rdd，指定分区有两个原则</p><ol><li><p>读取文件是以每行为单位</p></li><li><p>读取文件的切片(分区)是以文件为单位</p><p>比如：2个文件，第一个3个字节，3行，第二个6个字节1行，分2个区、</p><p>p0  123 </p><p>p1  456789</p><p>p2</p></li></ol></li><li><p>jvm默认内存 是 1/64，最大1/4</p></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-分布式计算原理&quot;&gt;&lt;a href=&quot;#1-分布式计算原理&quot; class=&quot;headerlink&quot; title=&quot;1. 分布式计算原理&quot;&gt;&lt;/a&gt;1. 分布式计算原理&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://gitee.com/curryfor369/picgo/raw/master/img/20200706105944.png&quot; alt=&quot;image-20200706105941183&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Spark" scheme="http://iscurry.com/categories/Spark/"/>
    
    
    <category term="Detail" scheme="http://iscurry.com/tags/Detail/"/>
    
    <category term="Spark" scheme="http://iscurry.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop(2)</title>
    <link href="http://iscurry.com/2020/05/20/Sqoop(2)/"/>
    <id>http://iscurry.com/2020/05/20/Sqoop(2)/</id>
    <published>2020-05-20T02:15:42.000Z</published>
    <updated>2020-09-25T02:22:12.729Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第1章-Sqoop简介"><a href="#第1章-Sqoop简介" class="headerlink" title="第1章 Sqoop简介"></a>第1章 Sqoop简介</h1><blockquote><p>Sqoop是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。</p></blockquote><blockquote><p>Sqoop项目开始于2009年，最早是作为Hadoop的一个第三方模块存在，后来为了让使用者能够快速部署，也为了让开发人员能够更快速的迭代开发，Sqoop独立成为一个<a href="https://baike.baidu.com/item/Apache/6265">Apache</a>项目。</p></blockquote><blockquote><p>Sqoop2的最新版本是1.99.7。请注意，2与1不兼容，且特征不完整，它并不打算用于生产部署。</p></blockquote><a id="more"></a><h1 id="第2章-Sqoop原理"><a href="#第2章-Sqoop原理" class="headerlink" title="第2章 Sqoop原理"></a>第2章 Sqoop原理</h1><p>将导入或导出命令翻译成mapreduce程序来实现。</p><p>在翻译出的mapreduce中主要是对inputformat和outputformat进行定制。</p><h1 id="第3章-Sqoop安装"><a href="#第3章-Sqoop安装" class="headerlink" title="第3章 Sqoop安装"></a>第3章 Sqoop安装</h1><p>安装Sqoop的前提是已经具备Java和Hadoop的环境。</p><h2 id="3-1-下载并解压"><a href="#3-1-下载并解压" class="headerlink" title="3.1 下载并解压"></a>3.1 下载并解压</h2><ol><li><p>下载地址：<a href="http://mirrors.hust.edu.cn/apache/sqoop/1.4.6/">http://mirrors.hust.edu.cn/apache/sqoop/1.4.6/</a></p></li><li><p>上传安装包sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz到虚拟机中</p></li><li><p>解压sqoop安装包到指定目录，如：</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tar -zxf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><h2 id="3-2-修改配置文件"><a href="#3-2-修改配置文件" class="headerlink" title="3.2 修改配置文件"></a>3.2 修改配置文件</h2><p>Sqoop的配置文件与大多数大数据框架类似，在sqoop根目录下的conf目录中。</p><p><strong>1)</strong> <strong>重命名配置文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mv sqoop-env-template.sh sqoop-env.sh</span><br></pre></td></tr></table></figure><p><strong>2)</strong> <strong>修改配置文件</strong></p><p>sqoop-env.sh</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">export</span> <span class="string">HADOOP_COMMON_HOME=/opt/module/hadoop-2.7.2</span></span><br><span class="line"></span><br><span class="line"><span class="attr">export</span> <span class="string">HADOOP_MAPRED_HOME=/opt/module/hadoop-2.7.2</span></span><br><span class="line"></span><br><span class="line"><span class="attr">export</span> <span class="string">HIVE_HOME=/opt/module/hive</span></span><br><span class="line"></span><br><span class="line"><span class="attr">export</span> <span class="string">ZOOKEEPER_HOME=/opt/module/zookeeper-3.4.10</span></span><br><span class="line"></span><br><span class="line"><span class="attr">export</span> <span class="string">ZOOCFGDIR=/opt/module/zookeeper-3.4.10</span></span><br><span class="line"></span><br><span class="line"><span class="attr">export</span> <span class="string">HBASE_HOME=/opt/module/hbase</span></span><br></pre></td></tr></table></figure><h2 id="3-3-拷贝JDBC驱动"><a href="#3-3-拷贝JDBC驱动" class="headerlink" title="3.3 拷贝JDBC驱动"></a>3.3 拷贝JDBC驱动</h2><p>拷贝jdbc驱动到sqoop的lib目录下，如：</p><blockquote><p>$ cp mysql-connector-java-5.1.27-bin.jar /opt/module/sqoop-1.4.6.bin__hadoop-2.0.4-alpha/lib/</p></blockquote><h2 id="3-4-验证Sqoop"><a href="#3-4-验证Sqoop" class="headerlink" title="3.4 验证Sqoop"></a>3.4 验证Sqoop</h2><p>我们可以通过某一个command来验证sqoop配置是否正确：</p><blockquote><p>$ bin/sqoop help</p></blockquote><p>出现一些Warning警告（警告信息已省略），并伴随着帮助命令的输出：</p><blockquote><p>Available commands:</p><p>codegen      Generate code to interact with database records</p><p>create-hive-table   Import a table definition into Hive</p><p>eval        Evaluate a SQL statement and display the results</p><p>export       Export an HDFS directory to a database table</p><p>help        List available commands</p><p>import       Import a table from a database to HDFS</p><p>import-all-tables   Import tables from a database to HDFS</p><p>import-mainframe  Import datasets from a mainframe server to HDFS</p><p>job        Work with saved jobs</p><p>list-databases    List available databases on a server</p><p>list-tables      List available tables in a database</p><p>merge       Merge results of incremental imports</p><p>metastore      Run a standalone Sqoop metastore</p><p>version      Display version information</p></blockquote><h2 id="3-5-测试Sqoop是否能够成功连接数据库"><a href="#3-5-测试Sqoop是否能够成功连接数据库" class="headerlink" title="3.5 测试Sqoop是否能够成功连接数据库"></a>3.5 测试Sqoop是否能够成功连接数据库</h2><blockquote><p>$ bin/sqoop list-databases –connect jdbc:mysql://hadoop102:3306/ –username root –password 000000</p></blockquote><p>出现如下输出：</p><blockquote><p>information_schema</p><p>metastore</p><p>mysql</p><p>oozie</p><p>performance_schema</p></blockquote><h1 id="第4章-Sqoop的简单使用案例"><a href="#第4章-Sqoop的简单使用案例" class="headerlink" title="第4章 Sqoop的简单使用案例"></a>第4章 Sqoop的简单使用案例</h1><h2 id="4-1-导入数据"><a href="#4-1-导入数据" class="headerlink" title="4.1 导入数据"></a>4.1 导入数据</h2><p>在Sqoop中，“导入”概念指：从非大数据集群（RDBMS）向大数据集群（HDFS，HIVE，HBASE）中传输数据，叫做：导入，即使用import关键字。</p><h3 id="4-1-1-RDBMS到HDFS"><a href="#4-1-1-RDBMS到HDFS" class="headerlink" title="4.1.1 RDBMS到HDFS"></a>4.1.1 RDBMS到HDFS</h3><ol><li><p>确定Mysql服务开启正常</p></li><li><p>在Mysql中新建一张表并插入一些数据</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mysql -uroot -p000000</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create database company;</span><br><span class="line"></span><br><span class="line">mysql&gt; create table company.staff(id int(4) primary key not null auto_increment, name varchar(255), sex varchar(255));</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into company.staff(name, sex) values(&#39;Thomas&#39;, &#39;Male&#39;);</span><br><span class="line"></span><br><span class="line">mysql&gt; insert into company.staff(name, sex) values(&#39;Catalina&#39;, &#39;FeMale&#39;);</span><br></pre></td></tr></table></figure><ol start="3"><li>导入数据</li></ol><p>​    <strong>（1）全部导入</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000 \</span><br><span class="line">--table staff \</span><br><span class="line">--target-dir /user/company \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;\t&quot;</span></span><br></pre></td></tr></table></figure><p>​    <strong>（2）查询导入</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000 \</span><br><span class="line">--target-dir /user/company \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span><br><span class="line">--query <span class="string">&#x27;select name,sex from staff where id &lt;=1 and $CONDITIONS;&#x27;</span></span><br></pre></td></tr></table></figure><p><font color="#FF0000"> 提示：must contain ‘$CONDITIONS’ in WHERE clause.</font></p><p><font color="#FF0000"> 如果query后使用的是双引号，则$CONDITIONS前必须加转移符，防止shell识别为自己的变量。</font></p><p>​    <strong>（3）导入指定列</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000 \</span><br><span class="line">--target-dir /user/company \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span><br><span class="line">--columns id,sex \</span><br><span class="line">--table staff</span><br></pre></td></tr></table></figure><p><font color="#FF0000">提示：columns中如果涉及到多列，用逗号分隔，分隔时不要添加空格 </font></p><p>​    <strong>（4）使用sqoop关键字筛选查询导入数据</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000 \</span><br><span class="line">--target-dir /user/company \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span><br><span class="line">--table staff \</span><br><span class="line">--<span class="built_in">where</span> <span class="string">&quot;id=1&quot;</span></span><br></pre></td></tr></table></figure><h3 id="4-1-2-RDBMS到Hive"><a href="#4-1-2-RDBMS到Hive" class="headerlink" title="4.1.2 RDBMS到Hive"></a>4.1.2 RDBMS到Hive</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000 \</span><br><span class="line">--table staff \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--hive-import \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--hive-table staff_hive</span><br></pre></td></tr></table></figure><p><font color="#FF0000"> 提示：该过程分为两步，第一步将数据导入到HDFS，第二步将导入到HDFS的数据迁移到Hive仓库，第一步默认的临时目录是/user/atguigu/表名</font></p><h3 id="4-1-3-RDBMS到Hbase"><a href="#4-1-3-RDBMS到Hbase" class="headerlink" title="4.1.3 RDBMS到Hbase"></a>4.1.3 RDBMS到Hbase</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000 \</span><br><span class="line">--table company \</span><br><span class="line">--columns <span class="string">&quot;id,name,sex&quot;</span> \</span><br><span class="line">--column-family <span class="string">&quot;info&quot;</span> \</span><br><span class="line">--hbase-create-table \</span><br><span class="line">--hbase-row-key <span class="string">&quot;id&quot;</span> \</span><br><span class="line">--hbase-table <span class="string">&quot;hbase_company&quot;</span> \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--split-by id</span><br></pre></td></tr></table></figure><p><font color="#FF0000"> 提示：sqoop1.4.6只支持HBase1.0.1之前的版本的自动创建HBase表的功能</font></p><p>解决方案：手动创建HBase表</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase&gt; create &#x27;hbase_company,&#x27;info&#x27;</span><br></pre></td></tr></table></figure><p><strong>(5)</strong> <strong>在HBase中scan这张表得到如下内容</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase&gt; scan ‘hbase_company’</span><br></pre></td></tr></table></figure><h2 id="4-2、导出数据"><a href="#4-2、导出数据" class="headerlink" title="4.2、导出数据"></a>4.2、导出数据</h2><p>在Sqoop中，“导出”概念指：从大数据集群（HDFS，HIVE，HBASE）向非大数据集群（RDBMS）中传输数据，叫做：导出，即使用export关键字。</p><h3 id="4-2-1-HIVE-HDFS到RDBMS"><a href="#4-2-1-HIVE-HDFS到RDBMS" class="headerlink" title="4.2.1 HIVE/HDFS到RDBMS"></a>4.2.1 HIVE/HDFS到RDBMS</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop <span class="built_in">export</span> \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000 \</span><br><span class="line">--table staff \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--<span class="built_in">export</span>-dir /user/hive/warehouse/staff_hive \</span><br><span class="line">--input-fields-terminated-by <span class="string">&quot;\t&quot;</span></span><br></pre></td></tr></table></figure><p><font color="#FF0000"> <strong>提示：Mysql中如果表不存在，不会自动创建</strong></font></p><h2 id="4-3-脚本打包"><a href="#4-3-脚本打包" class="headerlink" title="4.3 脚本打包"></a>4.3 脚本打包</h2><p>使用opt格式的文件打包sqoop命令，然后执行</p><p><strong>1)</strong> <strong>创建一个.opt文件</strong></p><blockquote><p>$ mkdir opt</p><p>$ touch opt/job_HDFS2RDBMS.opt</p></blockquote><p><strong>2)</strong> <strong>编写sqoop脚本</strong></p><blockquote><p>$ vi opt/job_HDFS2RDBMS.opt</p></blockquote> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span></span><br><span class="line">--connect</span><br><span class="line">jdbc:mysql://hadoop102:3306/company</span><br><span class="line">--username</span><br><span class="line">root</span><br><span class="line">--password</span><br><span class="line">000000</span><br><span class="line">--table</span><br><span class="line">staff</span><br><span class="line">--num-mappers</span><br><span class="line">1</span><br><span class="line">--<span class="built_in">export</span>-dir</span><br><span class="line">/user/hive/warehouse/staff_hive</span><br><span class="line">--input-fields-terminated-by</span><br><span class="line"><span class="string">&quot;\t&quot;</span></span><br></pre></td></tr></table></figure><p><strong>3)</strong> <strong>执行该脚本</strong></p><blockquote><p>$ bin/sqoop –options-file opt/job_HDFS2RDBMS.opt</p></blockquote><h1 id="第5章-Sqoop一些常用命令及参数"><a href="#第5章-Sqoop一些常用命令及参数" class="headerlink" title="第5章 Sqoop一些常用命令及参数"></a>第5章 Sqoop一些常用命令及参数</h1><h2 id="5-1-常用命令列举"><a href="#5-1-常用命令列举" class="headerlink" title="5.1 常用命令列举"></a>5.1 常用命令列举</h2><p>这里给大家列出来了一部分Sqoop操作时的常用参数，以供参考，需要深入学习的可以参看对应类的源代码。</p><table><thead><tr><th><strong>序号</strong></th><th><strong>命令</strong></th><th><strong>类</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>1</td><td>import</td><td>ImportTool</td><td>将数据导入到集群</td></tr><tr><td>2</td><td>export</td><td>ExportTool</td><td>将集群数据导出</td></tr><tr><td>3</td><td>codegen</td><td>CodeGenTool</td><td>获取数据库中某张表数据生成Java并打包Jar</td></tr><tr><td>4</td><td>create-hive-table</td><td>CreateHiveTableTool</td><td>创建Hive表</td></tr><tr><td>5</td><td>eval</td><td>EvalSqlTool</td><td>查看SQL执行结果</td></tr><tr><td>6</td><td>import-all-tables</td><td>ImportAllTablesTool</td><td>导入某个数据库下所有表到HDFS中</td></tr><tr><td>7</td><td>job</td><td>JobTool</td><td>用来生成一个sqoop的任务，生成后，该任务并不执行，除非使用命令执行该任务。</td></tr><tr><td>8</td><td>list-databases</td><td>ListDatabasesTool</td><td>列出所有数据库名</td></tr><tr><td>9</td><td>list-tables</td><td>ListTablesTool</td><td>列出某个数据库下所有表</td></tr><tr><td>10</td><td>merge</td><td>MergeTool</td><td>将HDFS中不同目录下面的数据合在一起，并存放在指定的目录中</td></tr><tr><td>11</td><td>metastore</td><td>MetastoreTool</td><td>记录sqoop job的元数据信息，如果不启动metastore实例，则默认的元数据存储目录为：~/.sqoop，如果要更改存储目录，可以在配置文件sqoop-site.xml中进行更改。</td></tr><tr><td>12</td><td>help</td><td>HelpTool</td><td>打印sqoop帮助信息</td></tr><tr><td>13</td><td>version</td><td>VersionTool</td><td>打印sqoop版本信息</td></tr></tbody></table><h2 id="5-2-命令-amp-参数详解"><a href="#5-2-命令-amp-参数详解" class="headerlink" title="5.2 命令&amp;参数详解"></a>5.2 命令&amp;参数详解</h2><p>刚才列举了一些Sqoop的常用命令，对于不同的命令，有不同的参数，让我们来一一列举说明。</p><p>首先来我们来介绍一下公用的参数，所谓公用参数，就是大多数命令都支持的参数。</p><h3 id="5-2-1-公用参数：数据库连接"><a href="#5-2-1-公用参数：数据库连接" class="headerlink" title="5.2.1 公用参数：数据库连接"></a>5.2.1 公用参数：数据库连接</h3><table><thead><tr><th><strong>序号</strong></th><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>1</td><td>–connect</td><td>连接关系型数据库的URL</td></tr><tr><td>2</td><td>–connection-manager</td><td>指定要使用的连接管理类</td></tr><tr><td>3</td><td>–driver</td><td>Hadoop根目录</td></tr><tr><td>4</td><td>–help</td><td>打印帮助信息</td></tr><tr><td>5</td><td>–password</td><td>连接数据库的密码</td></tr><tr><td>6</td><td>–username</td><td>连接数据库的用户名</td></tr><tr><td>7</td><td>–verbose</td><td>在控制台打印出详细信息</td></tr></tbody></table><h3 id="5-2-2-公用参数：import"><a href="#5-2-2-公用参数：import" class="headerlink" title="5.2.2 公用参数：import"></a>5.2.2 公用参数：import</h3><table><thead><tr><th><strong>序号</strong></th><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>1</td><td>–enclosed-by  &lt;char&gt;</td><td>给字段值前加上指定的字符</td></tr><tr><td>2</td><td>–escaped-by  &lt;char&gt;</td><td>对字段中的双引号加转义符</td></tr><tr><td>3</td><td>–fields-terminated-by  &lt;char&gt;</td><td>设定每个字段是以什么符号作为结束，默认为逗号</td></tr><tr><td>4</td><td>–lines-terminated-by  &lt;char&gt;</td><td>设定每行记录之间的分隔符，默认是\n</td></tr><tr><td>5</td><td>–mysql-delimiters</td><td>Mysql默认的分隔符设置，字段之间以逗号分隔，行之间以\n分隔，默认转义符是\，字段值以单引号包裹。</td></tr><tr><td>6</td><td>–optionally-enclosed-by  &lt;char&gt;</td><td>给带有双引号或单引号的字段值前后加上指定字符。</td></tr></tbody></table><h3 id="5-2-3-公用参数：export"><a href="#5-2-3-公用参数：export" class="headerlink" title="5.2.3 公用参数：export"></a>5.2.3 公用参数：export</h3><table><thead><tr><th><strong>序号</strong></th><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>1</td><td>–input-enclosed-by  &lt;char&gt;</td><td>对字段值前后加上指定字符</td></tr><tr><td>2</td><td>–input-escaped-by  &lt;char&gt;</td><td>对含有转移符的字段做转义处理</td></tr><tr><td>3</td><td>–input-fields-terminated-by  &lt;char&gt;</td><td>字段之间的分隔符</td></tr><tr><td>4</td><td>–input-lines-terminated-by  &lt;char&gt;</td><td>行之间的分隔符</td></tr><tr><td>5</td><td>–input-optionally-enclosed-by  &lt;char&gt;</td><td>给带有双引号或单引号的字段前后加上指定字符</td></tr></tbody></table><h3 id="5-2-4-公用参数：hive"><a href="#5-2-4-公用参数：hive" class="headerlink" title="5.2.4 公用参数：hive"></a>5.2.4 公用参数：hive</h3><table><thead><tr><th><strong>序号</strong></th><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>1</td><td>–hive-delims-replacement  &lt;arg&gt;</td><td>用自定义的字符串替换掉数据中的\r\n和\013 \010等字符</td></tr><tr><td>2</td><td>–hive-drop-import-delims</td><td>在导入数据到hive时，去掉数据中的\r\n\013\010这样的字符</td></tr><tr><td>3</td><td>–map-column-hive  &lt;arg&gt;</td><td>生成hive表时，可以更改生成字段的数据类型</td></tr><tr><td>4</td><td>–hive-partition-key</td><td>创建分区，后面直接跟分区名，分区字段的默认类型为string</td></tr><tr><td>5</td><td>–hive-partition-value  &lt;v&gt;</td><td>导入数据时，指定某个分区的值</td></tr><tr><td>6</td><td>–hive-home  &lt;dir&gt;</td><td>hive的安装目录，可以通过该参数覆盖之前默认配置的目录</td></tr><tr><td>7</td><td>–hive-import</td><td>将数据从关系数据库中导入到hive表中</td></tr><tr><td>8</td><td>–hive-overwrite</td><td>覆盖掉在hive表中已经存在的数据</td></tr><tr><td>9</td><td>–create-hive-table</td><td>默认是false，即，如果目标表已经存在了，那么创建任务失败。</td></tr><tr><td>10</td><td>–hive-table</td><td>后面接要创建的hive表,默认使用MySQL的表名</td></tr><tr><td>11</td><td>–table</td><td>指定关系数据库的表名</td></tr></tbody></table><p>公用参数介绍完之后，我们来按照命令介绍命令对应的特有参数。</p><h3 id="5-2-5-命令-amp-参数：import"><a href="#5-2-5-命令-amp-参数：import" class="headerlink" title="5.2.5 命令&amp;参数：import"></a>5.2.5 命令&amp;参数：import</h3><p>将关系型数据库中的数据导入到HDFS（包括Hive，HBase）中，如果导入的是Hive，那么当Hive中没有对应表时，则自动创建。</p><p><strong>1)</strong> <strong>命令：</strong></p><p>如：导入数据到hive中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000 \</span><br><span class="line">--table staff \</span><br><span class="line">--hive-import</span><br></pre></td></tr></table></figure><p>如：增量导入数据到hive中，mode=append</p><p><strong>append导入：</strong></p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000 \</span><br><span class="line">--table staff \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span><br><span class="line">--target-dir /user/hive/warehouse/staff_hive \</span><br><span class="line">--check-column id \</span><br><span class="line">--incremental append \</span><br><span class="line">--last-value 3</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><font color="#FF0000"> 尖叫提示</font>：append不能与–hive-等参数同时使用（Append mode for hive imports is not yet supported. Please remove the parameter –append-mode）</p><p>如：增量导入数据到hdfs中，mode=lastmodified</p><p>先在mysql中建表并插入几条数据：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create table company.staff_timestamp(id int(4), name varchar(255), sex varchar(255), last_modified timestamp DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP);</span><br><span class="line">mysql&gt; insert into company.staff_timestamp (id, name, sex) values(1, &#x27;AAA&#x27;, &#x27;female&#x27;);</span><br><span class="line">mysql&gt; insert into company.staff_timestamp (id, name, sex) values(2, &#x27;BBB&#x27;, &#x27;female&#x27;);</span><br></pre></td></tr></table></figure><p>先导入一部分数据：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000 \</span><br><span class="line">--table staff_timestamp \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--m 1</span><br></pre></td></tr></table></figure><p>再增量导入一部分数据：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; insert into company.staff_timestamp (id, name, sex) values(3, &#x27;CCC&#x27;, &#x27;female&#x27;);</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000 \</span><br><span class="line">--table staff_timestamp \</span><br><span class="line">--check-column last_modified \</span><br><span class="line">--incremental lastmodified \</span><br><span class="line">--last-value <span class="string">&quot;2017-09-28 22:20:38&quot;</span> \</span><br><span class="line">--m 1 \</span><br><span class="line">--append</span><br></pre></td></tr></table></figure><p><font color="#FF0000"> 尖叫提示</font>：使用lastmodified方式导入数据要指定增量数据是要–append（追加）还是要–merge-key（合并）</p><p><font color="#FF0000"> 尖叫提示</font>：last-value指定的值是会包含于增量导入的数据中</p><p><strong>2)</strong> <strong>参数：</strong></p><table><thead><tr><th><strong>序号</strong></th><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>1</td><td>–append</td><td>将数据追加到HDFS中已经存在的DataSet中，如果使用该参数，sqoop会把数据先导入到临时文件目录，再合并。</td></tr><tr><td>2</td><td>–as-avrodatafile</td><td>将数据导入到一个Avro数据文件中</td></tr><tr><td>3</td><td>–as-sequencefile</td><td>将数据导入到一个sequence文件中</td></tr><tr><td>4</td><td>–as-textfile</td><td>将数据导入到一个普通文本文件中</td></tr><tr><td>5</td><td>–boundary-query  &lt;statement&gt;</td><td>边界查询，导入的数据为该参数的值（一条sql语句）所执行的结果区间内的数据。</td></tr><tr><td>6</td><td>–columns &lt;col1, col2,  col3&gt;</td><td>指定要导入的字段</td></tr><tr><td>7</td><td>–direct</td><td>直接导入模式，使用的是关系数据库自带的导入导出工具，以便加快导入导出过程。</td></tr><tr><td>8</td><td>–direct-split-size</td><td>在使用上面direct直接导入的基础上，对导入的流按字节分块，即达到该阈值就产生一个新的文件</td></tr><tr><td>9</td><td>–inline-lob-limit</td><td>设定大对象数据类型的最大值</td></tr><tr><td>10</td><td>–m或–num-mappers</td><td>启动N个map来并行导入数据，默认4个。</td></tr><tr><td>11</td><td>–query或–e &lt;statement&gt;</td><td>将查询结果的数据导入，使用时必须伴随参–target-dir，–hive-table，如果查询中有where条件，则条件后必须加上$CONDITIONS关键字</td></tr><tr><td>12</td><td>–split-by &lt;column-name&gt;</td><td>按照某一列来切分表的工作单元，不能与–autoreset-to-one-mapper连用（请参考官方文档）</td></tr><tr><td>13</td><td>–table &lt;table-name&gt;</td><td>关系数据库的表名</td></tr><tr><td>14</td><td>–target-dir &lt;dir&gt;</td><td>指定HDFS路径</td></tr><tr><td>15</td><td>–warehouse-dir &lt;dir&gt;</td><td>与14参数不能同时使用，导入数据到HDFS时指定的目录</td></tr><tr><td>16</td><td>–where</td><td>从关系数据库导入数据时的查询条件</td></tr><tr><td>17</td><td>–z或–compress</td><td>允许压缩</td></tr><tr><td>18</td><td>–compression-codec</td><td>指定hadoop压缩编码类，默认为gzip(Use Hadoop codec  default gzip)</td></tr><tr><td>19</td><td>–null-string  &lt;null-string&gt;</td><td>string类型的列如果null，替换为指定字符串</td></tr><tr><td>20</td><td>–null-non-string  &lt;null-string&gt;</td><td>非string类型的列如果null，替换为指定字符串</td></tr><tr><td>21</td><td>–check-column &lt;col&gt;</td><td>作为增量导入判断的列名</td></tr><tr><td>22</td><td>–incremental &lt;mode&gt;</td><td>mode：append或lastmodified</td></tr><tr><td>23</td><td>–last-value &lt;value&gt;</td><td>指定某一个值，用于标记增量导入的位置</td></tr></tbody></table><h3 id="5-2-6-命令-amp-参数：export"><a href="#5-2-6-命令-amp-参数：export" class="headerlink" title="5.2.6 命令&amp;参数：export"></a>5.2.6 命令&amp;参数：export</h3><p>从HDFS（包括Hive和HBase）中奖数据导出到关系型数据库中。</p><p><strong>1)</strong> <strong>命令：</strong></p><p><strong>如：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop <span class="built_in">export</span> \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000 \</span><br><span class="line">--table staff \</span><br><span class="line">--<span class="built_in">export</span>-dir /user/company \</span><br><span class="line">--input-fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span><br><span class="line">--num-mappers 1</span><br></pre></td></tr></table></figure><p><strong>2)</strong> <strong>参数：</strong></p><table><thead><tr><th><strong>序号</strong></th><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>1</td><td>–direct</td><td>利用数据库自带的导入导出工具，以便于提高效率</td></tr><tr><td>2</td><td>–export-dir  &lt;dir&gt;</td><td>存放数据的HDFS的源目录</td></tr><tr><td>3</td><td>-m或–num-mappers &lt;n&gt;</td><td>启动N个map来并行导入数据，默认4个</td></tr><tr><td>4</td><td>–table  &lt;table-name&gt;</td><td>指定导出到哪个RDBMS中的表</td></tr><tr><td>5</td><td>–update-key &lt;col-name&gt;</td><td>对某一列的字段进行更新操作</td></tr><tr><td>6</td><td>–update-mode  &lt;mode&gt;</td><td>updateonly  allowinsert(默认)</td></tr><tr><td>7</td><td>–input-null-string &lt;null-string&gt;</td><td>请参考import该类似参数说明</td></tr><tr><td>8</td><td>–input-null-non-string  &lt;null-string&gt;</td><td>请参考import该类似参数说明</td></tr><tr><td>9</td><td>–staging-table  &lt;staging-table-name&gt;</td><td>创建一张临时表，用于存放所有事务的结果，然后将所有事务结果一次性导入到目标表中，防止错误。</td></tr><tr><td>10</td><td>–clear-staging-table</td><td>如果第9个参数非空，则可以在导出操作执行前，清空临时事务结果表</td></tr></tbody></table><h3 id="5-2-7-命令-amp-参数：codegen"><a href="#5-2-7-命令-amp-参数：codegen" class="headerlink" title="5.2.7 命令&amp;参数：codegen"></a>5.2.7 命令&amp;参数：codegen</h3><p>将关系型数据库中的表映射为一个Java类，在该类中有各列对应的各个字段。</p><p>如：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop codegen \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000 \</span><br><span class="line">--table staff \</span><br><span class="line">--bindir /home/admin/Desktop/staff \</span><br><span class="line">--class-name Staff \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;\t&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><table><thead><tr><th><strong>序号</strong></th><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>1</td><td>–bindir  &lt;dir&gt;</td><td>指定生成的Java文件、编译成的class文件及将生成文件打包为jar的文件输出路径</td></tr><tr><td>2</td><td>–class-name  &lt;name&gt;</td><td>设定生成的Java文件指定的名称</td></tr><tr><td>3</td><td>–outdir  &lt;dir&gt;</td><td>生成Java文件存放的路径</td></tr><tr><td>4</td><td>–package-name  &lt;name&gt;</td><td>包名，如com.z，就会生成com和z两级目录</td></tr><tr><td>5</td><td>–input-null-non-string  &lt;null-str&gt;</td><td>在生成的Java文件中，可以将null字符串或者不存在的字符串设置为想要设定的值（例如空字符串）</td></tr><tr><td>6</td><td>–input-null-string  &lt;null-str&gt;</td><td>将null字符串替换成想要替换的值（一般与5同时使用）</td></tr><tr><td>7</td><td>–map-column-java &lt;arg&gt;</td><td>数据库字段在生成的Java文件中会映射成各种属性，且默认的数据类型与数据库类型保持对应关系。该参数可以改变默认类型，例如：–map-column-java  id=long, name=String</td></tr><tr><td>8</td><td>–null-non-string  &lt;null-str&gt;</td><td>在生成Java文件时，可以将不存在或者null的字符串设置为其他值</td></tr><tr><td>9</td><td>–null-string &lt;null-str&gt;</td><td>在生成Java文件时，将null字符串设置为其他值（一般与8同时使用）</td></tr><tr><td>10</td><td>–table &lt;table-name&gt;</td><td>对应关系数据库中的表名，生成的Java文件中的各个属性与该表的各个字段一一对应</td></tr></tbody></table><h3 id="5-2-8-命令-amp-参数：create-hive-table"><a href="#5-2-8-命令-amp-参数：create-hive-table" class="headerlink" title="5.2.8 命令&amp;参数：create-hive-table"></a>5.2.8 命令&amp;参数：create-hive-table</h3><p>生成与关系数据库表结构对应的hive表结构。</p><p><strong>命令：</strong></p><p>如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop create-hive-table \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000 \</span><br><span class="line">--table staff \</span><br><span class="line">--hive-table hive_staff</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>参数：</strong></p><table><thead><tr><th><strong>序号</strong></th><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>1</td><td>–hive-home  &lt;dir&gt;</td><td>Hive的安装目录，可以通过该参数覆盖掉默认的Hive目录</td></tr><tr><td>2</td><td>–hive-overwrite</td><td>覆盖掉在Hive表中已经存在的数据</td></tr><tr><td>3</td><td>–create-hive-table</td><td>默认是false，如果目标表已经存在了，那么创建任务会失败</td></tr><tr><td>4</td><td>–hive-table</td><td>后面接要创建的hive表</td></tr><tr><td>5</td><td>–table</td><td>指定关系数据库的表名</td></tr></tbody></table><h3 id="5-2-9-命令-amp-参数：eval"><a href="#5-2-9-命令-amp-参数：eval" class="headerlink" title="5.2.9 命令&amp;参数：eval"></a>5.2.9 命令&amp;参数：eval</h3><p>可以快速的使用SQL语句对关系型数据库进行操作，经常用于在import数据之前，了解一下SQL语句是否正确，数据是否正常，并可以将结果显示在控制台。</p><p><strong>命令：</strong></p><p>如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop <span class="built_in">eval</span> \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000 \</span><br><span class="line">--query <span class="string">&quot;SELECT * FROM staff&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>参数：</strong></p><table><thead><tr><th><strong>序号</strong></th><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>1</td><td>–query或–e</td><td>后跟查询的SQL语句</td></tr></tbody></table><h3 id="5-2-10-命令-amp-参数：import-all-tables"><a href="#5-2-10-命令-amp-参数：import-all-tables" class="headerlink" title="5.2.10 命令&amp;参数：import-all-tables"></a>5.2.10 命令&amp;参数：import-all-tables</h3><p>可以将RDBMS中的所有表导入到HDFS中，每一个表都对应一个HDFS目录</p><p><strong>命令：</strong></p><p>如：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop import-all-tables \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000 \</span><br><span class="line">--warehouse-dir /all_tables</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>参数：</strong></p><table><thead><tr><th><strong>序号</strong></th><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>1</td><td>–as-avrodatafile</td><td>这些参数的含义均和import对应的含义一致</td></tr><tr><td>2</td><td>–as-sequencefile</td><td></td></tr><tr><td>3</td><td>–as-textfile</td><td></td></tr><tr><td>4</td><td>–direct</td><td></td></tr><tr><td>5</td><td>–direct-split-size  &lt;n&gt;</td><td></td></tr><tr><td>6</td><td>–inline-lob-limit  &lt;n&gt;</td><td></td></tr><tr><td>7</td><td>–m或—num-mappers  &lt;n&gt;</td><td></td></tr><tr><td>8</td><td>–warehouse-dir  &lt;dir&gt;</td><td></td></tr><tr><td>9</td><td>-z或–compress</td><td></td></tr><tr><td>10</td><td>–compression-codec</td><td></td></tr></tbody></table><h3 id="5-2-11-命令-amp-参数：job"><a href="#5-2-11-命令-amp-参数：job" class="headerlink" title="5.2.11 命令&amp;参数：job"></a>5.2.11 命令&amp;参数：job</h3><p>用来生成一个sqoop任务，生成后不会立即执行，需要手动执行。</p><p><strong>命令：</strong></p><p>如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop job \</span><br><span class="line"> --create myjob -- import-all-tables \</span><br><span class="line"> --connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line"> --username root \</span><br><span class="line"> --password 000000</span><br><span class="line">$ bin/sqoop job \</span><br><span class="line">--list</span><br><span class="line">$ bin/sqoop job \</span><br><span class="line">--<span class="built_in">exec</span> myjob</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><font color="#FF0000"> 尖叫提示</font>：注意import-all-tables和它左边的–之间有一个空格</p><p><font color="#FF0000"> 尖叫提示</font>：如果需要连接metastore，则–meta-connect jdbc:hsqldb:hsql://linux01:16000/sqoop</p><p>参数：</p><table><thead><tr><th><strong>序号</strong></th><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>1</td><td>–create  &lt;job-id&gt;</td><td>创建job参数</td></tr><tr><td>2</td><td>–delete  &lt;job-id&gt;</td><td>删除一个job</td></tr><tr><td>3</td><td>–exec  &lt;job-id&gt;</td><td>执行一个job</td></tr><tr><td>4</td><td>–help</td><td>显示job帮助</td></tr><tr><td>5</td><td>–list</td><td>显示job列表</td></tr><tr><td>6</td><td>–meta-connect  &lt;jdbc-uri&gt;</td><td>用来连接metastore服务</td></tr><tr><td>7</td><td>–show  &lt;job-id&gt;</td><td>显示一个job的信息</td></tr><tr><td>8</td><td>–verbose</td><td>打印命令运行时的详细信息</td></tr></tbody></table><p><font color="#FF0000"> 尖叫提示</font>：在执行一个job时，如果需要手动输入数据库密码，可以做如下优化</p><h3 id="5-2-12-命令-amp-参数：list-databases"><a href="#5-2-12-命令-amp-参数：list-databases" class="headerlink" title="5.2.12 命令&amp;参数：list-databases"></a>5.2.12 命令&amp;参数：list-databases</h3><p><strong>命令：</strong></p><p>如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop list-databases \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/ \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>参数：</strong>与公用参数一样</p><h3 id="5-2-13-命令-amp-参数：list-tables"><a href="#5-2-13-命令-amp-参数：list-tables" class="headerlink" title="5.2.13 命令&amp;参数：list-tables"></a>5.2.13 命令&amp;参数：list-tables</h3><p><strong>命令：</strong></p><p>如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop list-tables \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>参数：</strong>与公用参数一样</p><h3 id="5-2-14-命令-amp-参数：merge"><a href="#5-2-14-命令-amp-参数：merge" class="headerlink" title="5.2.14 命令&amp;参数：merge"></a>5.2.14 命令&amp;参数：merge</h3><p>将HDFS中不同目录下面的数据合并在一起并放入指定目录中</p><p>数据环境：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">new_staff</span><br><span class="line">1       AAA     male</span><br><span class="line">2       BBB     male</span><br><span class="line">3       CCC     male</span><br><span class="line">4       DDD     male</span><br><span class="line">old_staff</span><br><span class="line">1       AAA     female</span><br><span class="line">2       CCC     female</span><br><span class="line">3       BBB     female</span><br><span class="line">6       DDD     female</span><br></pre></td></tr></table></figure><p><font color="#FF0000">提示 </font>：上边数据的列之间的分隔符应该为\t，行与行之间的分割符为\n，如果直接复制，请检查之。</p><p><strong>命令：</strong></p><p>如：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">创建JavaBean：</span><br><span class="line">$ bin/sqoop codegen \</span><br><span class="line">--connect jdbc:mysql://hadoop102:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 000000 \</span><br><span class="line">--table staff \</span><br><span class="line">--bindir /home/admin/Desktop/staff \</span><br><span class="line">--class-name Staff \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;\t&quot;</span></span><br><span class="line"></span><br><span class="line">开始合并：</span><br><span class="line">$ bin/sqoop merge \</span><br><span class="line">--new-data /<span class="built_in">test</span>/new/ \</span><br><span class="line">--onto /<span class="built_in">test</span>/old/ \</span><br><span class="line">--target-dir /<span class="built_in">test</span>/merged \</span><br><span class="line">--jar-file /home/admin/Desktop/staff/Staff.jar \</span><br><span class="line">--class-name Staff \</span><br><span class="line">--merge-key id</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">1AAAMALE</span><br><span class="line">2BBBMALE</span><br><span class="line">3CCCMALE</span><br><span class="line">4DDDMALE</span><br><span class="line">6DDDFEMALE</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>参数：</p><table><thead><tr><th><strong>序号</strong></th><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>1</td><td>–new-data  &lt;path&gt;</td><td>HDFS  待合并的数据目录，<strong>合并后在新的数据集中保留</strong></td></tr><tr><td>2</td><td>–onto  &lt;path&gt;</td><td>HDFS<strong>合并后，重复的部分在新的数据集中被覆盖</strong></td></tr><tr><td>3</td><td>–merge-key  &lt;col&gt;</td><td>合并键，一般是主键ID</td></tr><tr><td>4</td><td>–jar-file  &lt;file&gt;</td><td>合并时引入的jar包，该jar包是通过Codegen工具生成的jar包</td></tr><tr><td>5</td><td>–class-name  &lt;class&gt;</td><td>对应的表名或对象名，该class类是包含在jar包中的</td></tr><tr><td>6</td><td>–target-dir  &lt;path&gt;</td><td>合并后的数据在HDFS里存放的目录</td></tr></tbody></table><h3 id="5-2-15-命令-amp-参数：metastore"><a href="#5-2-15-命令-amp-参数：metastore" class="headerlink" title="5.2.15 命令&amp;参数：metastore"></a>5.2.15 命令&amp;参数：metastore</h3><p>记录了Sqoop job的元数据信息，如果不启动该服务，那么默认job元数据的存储目录为~/.sqoop，可在sqoop-site.xml中修改。</p><p><strong>命令：</strong></p><p>如：启动sqoop的metastore服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop metastore  </span><br></pre></td></tr></table></figure><p><strong>参数：</strong></p><table><thead><tr><th><strong>序号</strong></th><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>1</td><td>–shutdown</td><td>关闭metastore</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;第1章-Sqoop简介&quot;&gt;&lt;a href=&quot;#第1章-Sqoop简介&quot; class=&quot;headerlink&quot; title=&quot;第1章 Sqoop简介&quot;&gt;&lt;/a&gt;第1章 Sqoop简介&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Sqoop是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Sqoop项目开始于2009年，最早是作为Hadoop的一个第三方模块存在，后来为了让使用者能够快速部署，也为了让开发人员能够更快速的迭代开发，Sqoop独立成为一个&lt;a href=&quot;https://baike.baidu.com/item/Apache/6265&quot;&gt;Apache&lt;/a&gt;项目。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Sqoop2的最新版本是1.99.7。请注意，2与1不兼容，且特征不完整，它并不打算用于生产部署。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="Sqoop" scheme="http://iscurry.com/categories/Sqoop/"/>
    
    
    <category term="Detail" scheme="http://iscurry.com/tags/Detail/"/>
    
    <category term="Sqoop" scheme="http://iscurry.com/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Flume(1)</title>
    <link href="http://iscurry.com/2020/03/04/Flume(1)/"/>
    <id>http://iscurry.com/2020/03/04/Flume(1)/</id>
    <published>2020-03-04T02:23:21.000Z</published>
    <updated>2020-09-25T02:19:46.879Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-flume简介"><a href="#1-flume简介" class="headerlink" title="1. flume简介"></a>1. flume简介</h1><ul><li>是单节点的数据采集工具，指定一个源，对其进行信息采集</li><li>有两个版本<ul><li>flume og 原始</li><li>flume ng 现在（1.0之后）</li></ul></li></ul><a id="more"></a><ul><li>只有一个内部元素是agent[经纪人]，包含了3个子元素<ul><li>sources</li><li>channels</li><li>sinks</li></ul></li><li>配置文件写到任何地方都可以</li><li>官方文档 <a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html">http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html</a></li><li>任何配置都在官方文档里找</li></ul><h1 id="2-flume安装"><a href="#2-flume安装" class="headerlink" title="2. flume安装"></a>2. flume安装</h1><ol><li><p>下载安装包、上传、解压、改名[flume-1.7.0] apache-flume-1.7.0-bin.tar.gz</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rz ...  # 我装的有lrzsz,使用rz上传文件、sz xxx 下载</span><br><span class="line">tar -zxf apache-flume-1.7.0-bin.tar.gz -C /opt/module/ #解压到指定目录</span><br><span class="line">mv apache-flume-1.7.0-bin flume-1.7.0 # 改名</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>环境变量配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line">export FLUME_HOME=/opt/module/flume-1.7.0</span><br><span class="line">export PATH=$PATH:$&#123;FLUME_HOME&#125;/bin</span><br><span class="line"></span><br><span class="line">export TERM=ansi #输的时候避免小键盘失效，编辑完后删除这一行</span><br><span class="line"></span><br><span class="line">source /etc/profile # 使环境文件生效</span><br></pre></td></tr></table></figure></li><li><p>控制台数据 flume- 可以tab键补全证明安装ok</p></li></ol><h1 id="3-flume配置"><a href="#3-flume配置" class="headerlink" title="3. flume配置"></a>3. flume配置</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置文件可以在任意目录，任意名字，为了见名知意 /opt/module/flume-1.7.0/conf/example.conf</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line"><span class="meta">#</span><span class="bash">a1.sources.r1.type = avro</span></span><br><span class="line">a1.sources.r1.bind = node1</span><br><span class="line">a1.sources.r1.port = 88888</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="4-flume命令"><a href="#4-flume命令" class="headerlink" title="4. flume命令"></a>4. flume命令</h1><p>1、 帮助</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flume-ng</span><br></pre></td></tr></table></figure><p>2、 启动flume</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent --n a1 -c /opt/module/flume-1.7.0/conf -f example.conf -Dflume.root.logger=INFO,console 1&gt;/dev/null 2&amp;1</span><br></pre></td></tr></table></figure><p>3、启动nc</p><ul><li>nc是一个可以在路由键信息传递的工具</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc 192.168.10.101 88888</span><br></pre></td></tr></table></figure><p>4、 windows下启动telnet</p><ul><li>telnet和nc性质一样</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">telnet 192.168.10.101 88888</span><br></pre></td></tr></table></figure><h1 id="5-source类型"><a href="#5-source类型" class="headerlink" title="5. source类型"></a>5. source类型</h1><ul><li>数据源类型</li></ul><h2 id="1、agent"><a href="#1、agent" class="headerlink" title="1、agent"></a>1、agent</h2><ul><li>路由间信息传输</li><li>配置文件的souce</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = node1</span><br><span class="line">a1.sources.r1.port = 88888</span><br></pre></td></tr></table></figure><ul><li>使用</li></ul><blockquote><p>==linux启动方式==：</p><p>flume-ng agent -n a1 -c /opt/module/flume-1.7.0/conf -f example.conf Dflume.root.logger=INFO,console</p></blockquote><blockquote><p>==windows启动方式== 目录到bin/</p><p>flume-ng.cmd  agent -conf ../conf  -conf-file ../conf/example.conf  -name a1  -property flume.root.logger=INFO,console</p></blockquote><blockquote><p>==linux通信: nc==</p><p>nc 192.168.10.101 88888</p></blockquote><blockquote><p>==windows通信：telnet==</p><p>telnet 192.168.10.101 88888</p></blockquote><h2 id="2、avro"><a href="#2、avro" class="headerlink" title="2、avro"></a>2、avro</h2><ul><li><p>对一个文件采集,一次性的</p></li><li><p>配置文件的source</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">a1.sources.r1.type = avro</span></span><br><span class="line">a1.sources.r1.bind = node1</span><br><span class="line">a1.sources.r1.port = 88888</span><br></pre></td></tr></table></figure></li><li><p>使用</p></li></ul><blockquote><p>linux启动方式</p><p>flume-ng agent –n a1 -c /opt/module/flume-1.7.0/conf -f avro.conf -Dflume.root.logger=INFO,console</p><p>win启动方式  都是一样的，和上边一样</p></blockquote><blockquote><p>==linux通信： avro-client==   对指定目录读取</p><p>flume-ng avro-client -H node1 -p 55555 -F /root/a</p></blockquote><h2 id="3、-exec"><a href="#3、-exec" class="headerlink" title="3、 exec"></a>3、 exec</h2><ul><li><p>对执行一个命令的返回值进行采集，配合 tail -F 监听文件变化，实时采集</p></li><li><p>配置文件的source</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /root/a</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动<span class="built_in">exec</span>类型的<span class="built_in">source</span></span></span><br><span class="line">flume-ng agent --n a1 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/conf/exec.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li></ul><h2 id="4、-spooling"><a href="#4、-spooling" class="headerlink" title="4、 spooling"></a>4、 spooling</h2><ul><li>对一个文件下的内容监控</li></ul><h2 id="5、-tailDir"><a href="#5、-tailDir" class="headerlink" title="5、 tailDir"></a>5、 tailDir</h2><ul><li>对一个组下的文件和内容监控</li></ul><h1 id="6、-channel类型"><a href="#6、-channel类型" class="headerlink" title="6、 channel类型"></a>6、 channel类型</h1><ul><li>中间数据管道类型</li><li>memory 默认</li><li>fileChannel</li><li>kafkaChannel<ul><li>推荐使用avro-source类型，如果不使用这种则需设置parseAsFlumeEvent=false</li></ul></li></ul><h1 id="7、sinks"><a href="#7、sinks" class="headerlink" title="7、sinks"></a>7、sinks</h1><ul><li>存储类型</li><li>hdfs</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> round 断开重连，不产生新文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> roll 产生新文件</span></span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H%M/%S</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = events-</span><br><span class="line">a1.sinks.k1.hdfs.round = true</span><br><span class="line">a1.sinks.k1.hdfs.roundValue = 10</span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = minute</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.hdfs.dfs.rollInterval = 30</span><br><span class="line">a1.sinks.k1.hdfs.hdfs.rollSize = 1024</span><br><span class="line">a1.sinks.k1.hdfs.hdfs.rollCount = 10</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true # 如果想用上边的时间做为名字，要么开启这个，但是如果服务器不提供时间服务，可以用过滤器来做</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream </span><br><span class="line">a1.sinks.k1.hdfs.hdfs.writeFormat = Text </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 过滤器使用时间戳服务</span></span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = timestamp</span><br></pre></td></tr></table></figure><ul><li>file</li><li>avro</li><li>kafka</li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-flume简介&quot;&gt;&lt;a href=&quot;#1-flume简介&quot; class=&quot;headerlink&quot; title=&quot;1. flume简介&quot;&gt;&lt;/a&gt;1. flume简介&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;是单节点的数据采集工具，指定一个源，对其进行信息采集&lt;/li&gt;
&lt;li&gt;有两个版本&lt;ul&gt;
&lt;li&gt;flume og 原始&lt;/li&gt;
&lt;li&gt;flume ng 现在（1.0之后）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Flume" scheme="http://iscurry.com/categories/Flume/"/>
    
    
    <category term="Flume" scheme="http://iscurry.com/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume(2)详情</title>
    <link href="http://iscurry.com/2020/03/03/Flume(2)%E8%AF%A6%E7%BB%86/"/>
    <id>http://iscurry.com/2020/03/03/Flume(2)%E8%AF%A6%E7%BB%86/</id>
    <published>2020-03-03T02:23:21.000Z</published>
    <updated>2020-09-25T02:19:29.406Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第1章-概述"><a href="#第1章-概述" class="headerlink" title="第1章 概述"></a>第1章 概述</h1><h2 id="1-1-Flume定义"><a href="#1-1-Flume定义" class="headerlink" title="1.1 Flume定义"></a>1.1 Flume定义</h2><p>Flume是Cloudera提供的一个高可用的，高可靠的，==分布式的海量日志采集、聚合和传输的系统==。Flume基于流式架构，灵活简单。</p><ul><li><p>为什么选Flume</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200919101429.png" alt="image-20200918160104738"></p></li></ul><a id="more"></a><h2 id="1-2-Flume组成架构"><a href="#1-2-Flume组成架构" class="headerlink" title="1.2 Flume组成架构"></a>1.2 Flume组成架构</h2><ul><li>Flume组成架构</li></ul><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200919101430.png" alt="image-20200919101430"></p><ul><li><p>Flume组成架构详解</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200919101431.png" alt="image-20200918160442550"></p></li></ul><h3 id="1-2-1-Agent"><a href="#1-2-1-Agent" class="headerlink" title="1.2.1 Agent"></a>1.2.1 Agent</h3><p>Agent是一个JVM进程，它以事件的形式将数据从源头送至目的，==是Flume数据传输的基本单元==。</p><p>Agent主要有3个部分组成，Source、Channel、Sink。</p><h3 id="1-2-2-Source"><a href="#1-2-2-Source" class="headerlink" title="1.2.2 Source"></a>1.2.2 Source</h3><p>==Source是负责接收数据到Flume Agent的组件==。Source组件可以处理各种类型、各种格式的日志数据，包括==avro==、thrift、==exec==、jms、==spooling<br>directory==、netcat、sequence generator、syslog、http、legacy。</p><h3 id="1-2-3-Channel"><a href="#1-2-3-Channel" class="headerlink" title="1.2.3 Channel"></a>1.2.3 Channel</h3><p>==Channel是位于Source和Sink之间的缓冲区==。因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几Source的写入操作和几个Sink的读取操作。</p><p>Flume自带两种Channel：Memory Channel和File Channel。</p><p>==Memory Channel是内存中的队列。Memory Channel 在不需要关心数据丢失的情景下适用==。如果需要关心数据丢失，那么MemoryChannel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。</p><p>==File Channel将所有事件写到磁盘==。因此在程序关闭或机器宕机的情况下不会丢失数据。</p><h3 id="1-2-4-Sink"><a href="#1-2-4-Sink" class="headerlink" title="1.2.4 Sink"></a>1.2.4 Sink</h3><p>==Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。==</p><p>==Sink是完全事务性的==。在从Channel批量删除数据之前，每个SinkChannel启动一个事务。批量事件一旦成功写出到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。</p><p>Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。</p><h3 id="1-2-5-Event"><a href="#1-2-5-Event" class="headerlink" title="1.2.5 Event"></a>1.2.5 Event</h3><p>传输单元，Flume数据传输的基本单元，以事件的形式将数据从源头送至目的地。</p><h2 id="1-3-Flume拓扑结构"><a href="#1-3-Flume拓扑结构" class="headerlink" title="1.3 Flume拓扑结构"></a>1.3 Flume拓扑结构</h2><p>Flume的拓扑结构如图：</p><ul><li>Flume Agent连接</li></ul><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200919101432.png"></p><ul><li>单source，多channel、sink</li></ul><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200919101433.png"></p><ul><li>Flume负载均衡</li></ul><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200919101434.png"></p><ul><li>Flume Agent聚合</li></ul><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200919101435.png"></p><h2 id="1-4-Flume-Agent内部原理"><a href="#1-4-Flume-Agent内部原理" class="headerlink" title="1.4 Flume Agent内部原理"></a>1.4 Flume Agent内部原理</h2><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200919101436.png" alt="image-20200918161116095"></p><h1 id="第2章-快速入门"><a href="#第2章-快速入门" class="headerlink" title="第2章 快速入门"></a>第2章 快速入门</h1><h2 id="2-1-Flume安装地址"><a href="#2-1-Flume安装地址" class="headerlink" title="2.1 Flume安装地址"></a>2.1 Flume安装地址</h2><p>1） Flume官网地址</p><p><a href="http://flume.apache.org/">http://flume.apache.org/</a></p><p>2）文档查看地址</p><p><a href="http://flume.apache.org/FlumeUserGuide.html">http://flume.apache.org/FlumeUserGuide.html</a></p><p>3）下载地址</p><p><a href="http://archive.apache.org/dist/flume/">http://archive.apache.org/dist/flume/</a></p><h2 id="2-2-安装部署"><a href="#2-2-安装部署" class="headerlink" title="2.2 安装部署"></a>2.2 安装部署</h2><blockquote><p>  1）将apache-flume-1.7.0-bin.tar.gz上传到linux的/opt/software目录下</p></blockquote><blockquote><p>  2）解压apache-flume-1.7.0-bin.tar.gz到/opt/module/目录下</p></blockquote><blockquote><p>  [xing@hadoop102 software]$ tar -zxf apache-flume-1.7.0-bin.tar.gz -C /opt/module/</p></blockquote><blockquote><p>  3）修改apache-flume-1.7.0-bin的名称为flume</p></blockquote><blockquote><p>  [xing@hadoop102 module]$ mv apache-flume-1.7.0-bin flume</p></blockquote><blockquote><ol start="4"><li>将flume/conf下的flume-env.sh.template文件修改为flume-env.sh，并配置flume-env.sh文件</li></ol></blockquote><blockquote><p>[xing@hadoop102 conf]$ mv flume-env.sh.template flume-env.sh</p><p>[xing@hadoop102 conf]$ vi flume-env.sh</p><p>export JAVA_HOME=/opt/module/jdk1.8.0_144</p></blockquote><h1 id="第3章-开发案例"><a href="#第3章-开发案例" class="headerlink" title="第3章 开发案例"></a>第3章 开发案例</h1><h2 id="3-1-监控端口数据官方案例"><a href="#3-1-监控端口数据官方案例" class="headerlink" title="3.1 监控端口数据官方案例"></a>3.1 监控端口数据官方案例</h2><p>1）案例需求：首先，Flume监控本机44444端口，然后通过telnet工具向本机44444端口发送消息，最后Flume将监听的数据实时显示在控制台。</p><p>2）需求分析：</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200919101437.png" alt="image-20200918161445230"></p><p>3）实现步骤：</p><p>1．安装telnet工具</p><p>将rpm软件包(xinetd-2.3.14-40.el6.x86_64.rpm、telnet-0.17-48.el6.x86_64.rpm和telnet-server-0.17-48.el6.x86_64.rpm)拷入/opt/software文件夹下面。执行RPM软件包安装命令：</p><blockquote><p>[xing@hadoop102 software]$ sudo rpm -ivh xinetd-2.3.14-40.el6.x86_64.rpm</p></blockquote><blockquote><p>[xing@hadoop102 software]$ sudo rpm -ivh telnet-0.17-48.el6.x86_64.rpm</p></blockquote><blockquote><p>[xing@hadoop102 software]$ sudo rpm -ivh telnet-server-0.17-48.el6.x86_64.rpm</p></blockquote><p>2．判断44444端口是否被占用</p><blockquote><p>[xing@hadoop102 flume-telnet]$ sudo netstat -tunlp | grep 44444</p></blockquote><blockquote><p>功能描述：netstat命令是一个监控TCP/IP网络的非常有用的工具，它可以显示路由表、实际的网络连接以及每一个网络接口设备的状态信息。 </p><p>基本语法：netstat [选项]</p><p>选项参数：</p><p>​    -t或–tcp：显示TCP传输协议的连线状况； </p><p>​    -u或–udp：显示UDP传输协议的连线状况；</p><p>​    -n或–numeric：直接使用ip地址，而不通过域名服务器； </p><p>​    -l或–listening：显示监控中的服务器的Socket； </p><p>​    -p或–programs：显示正在使用Socket的程序识别码和程序名称；</p></blockquote><p>3．创建Flume Agent配置文件flume-telnet-logger.conf</p><p>在flume目录下创建job文件夹并进入job文件夹。</p><blockquote><p>[xing@hadoop102 flume]$ mkdir job</p></blockquote><blockquote><p>[xing@hadoop102 flume]$ cd job/</p></blockquote><p>在job文件夹下创建Flume Agent配置文件flume-telnet-logger.conf。</p><blockquote><p>[xing@hadoop102 job]$ touch flume-telnet-logger.conf</p></blockquote><p>在flume-telnet-logger.conf文件中添加如下内容。</p><blockquote><p>[xing@hadoop102 job]$ vim flume-telnet-logger.conf</p></blockquote><p>添加内容如下：</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line"><span class="meta">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="meta">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="meta">a1.sources.r1.type</span> = <span class="string">netcat</span></span><br><span class="line"><span class="meta">a1.sources.r1.bind</span> = <span class="string">localhost</span></span><br><span class="line"><span class="meta">a1.sources.r1.port</span> = <span class="string">44444</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="meta">a1.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line"><span class="meta">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="meta">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>注：配置文件来源于官方手册<a href="http://flume.apache.org/FlumeUserGuide.html">http://flume.apache.org/FlumeUserGuide.html</a></p><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200919101438.png" alt="image-20200918163148624"></p><ol start="4"><li>先开启flume监听端口</li></ol><blockquote><p>[xing@hadoop102 flume]$ bin/flume-ng agent –conf conf/ –name a1 –conf-file job/flume-telnet-logger.conf -Dflume.root.logger=INFO,console</p></blockquote><blockquote><p>参数说明：</p><p>   –conf conf/ ：表示配置文件存储在conf/目录</p><p>​    –name a1    ：表示给agent起名为a1</p><p>​    –conf-file job/flume-telnet.conf ：flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。</p><p>​    -Dflume.root.logger==INFO,console ：-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。</p></blockquote><p>5．使用telnet工具向本机的44444端口发送内容</p><blockquote><p>[xing@hadoop102 ~]$ telnet localhost 44444</p><p>连接上之后发送信息</p></blockquote><p>6．在Flume监听页面观察接收数据情况</p><h2 id="3-2-实时读取本地文件到HDFS案例"><a href="#3-2-实时读取本地文件到HDFS案例" class="headerlink" title="3.2 实时读取本地文件到HDFS案例"></a>3.2 实时读取本地文件到HDFS案例</h2><p>1）案例需求：实时监控Hive日志，并上传到HDFS中</p><p>2）需求分析：</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200919101439.png" alt="image-20200918163424487"></p><p>3）实现步骤：</p><p>1．Flume要想将数据输出到HDFS，必须持有Hadoop相关jar包</p><blockquote><p>将commons-configuration-1.6.jar、</p><p>hadoop-auth-2.7.2.jar、</p><p>hadoop-common-2.7.2.jar、</p><p>hadoop-hdfs-2.7.2.jar、</p><p>commons-io-2.4.jar、</p><p>htrace-core-3.1.0-incubating.jar</p><p>拷贝到/opt/module/flume/lib文件夹下。</p></blockquote><p>2．创建flume-file-hdfs.conf文件</p><blockquote><p>创建文件</p><p>[xing@hadoop102 job]$ touch flume-file-hdfs.conf</p></blockquote><p>注：要想读取Linux系统中的文件，就得按照Linux命令的规则执行命令。由于Hive日志在Linux系统中所以读取文件的类型选择：exec即execute执行的意思。表示执行Linux命令来读取文件。</p><blockquote><p>[xing@hadoop102 job]$ vim flume-file-hdfs.conf</p></blockquote><p>添加如下内容</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line"><span class="meta">a2.sources</span> = <span class="string">r2</span></span><br><span class="line"><span class="meta">a2.sinks</span> = <span class="string">k2</span></span><br><span class="line"><span class="meta">a2.channels</span> = <span class="string">c2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="meta">a2.sources.r2.type</span> = <span class="string">exec</span></span><br><span class="line"><span class="meta">a2.sources.r2.command</span> = <span class="string">tail -F /opt/module/hive/logs/hive.log</span></span><br><span class="line"><span class="meta">a2.sources.r2.shell</span> = <span class="string">/bin/bash -c</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="meta">a2.sinks.k2.type</span> = <span class="string">hdfs</span></span><br><span class="line"><span class="meta">a2.sinks.k2.hdfs.path</span> = <span class="string">hdfs://hadoop102:9000/flume/%Y%m%d/%H</span></span><br><span class="line"><span class="comment">#上传文件的前缀</span></span><br><span class="line"><span class="meta">a2.sinks.k2.hdfs.filePrefix</span> = <span class="string">logs-</span></span><br><span class="line"><span class="comment">#是否按照时间滚动文件夹</span></span><br><span class="line"><span class="meta">a2.sinks.k2.hdfs.round</span> = <span class="string">true</span></span><br><span class="line"><span class="comment">#多少时间单位创建一个新的文件夹</span></span><br><span class="line"><span class="meta">a2.sinks.k2.hdfs.roundValue</span> = <span class="string">1</span></span><br><span class="line"><span class="comment">#重新定义时间单位</span></span><br><span class="line"><span class="meta">a2.sinks.k2.hdfs.roundUnit</span> = <span class="string">hour</span></span><br><span class="line"><span class="comment">#是否使用本地时间戳</span></span><br><span class="line"><span class="meta">a2.sinks.k2.hdfs.useLocalTimeStamp</span> = <span class="string">true</span></span><br><span class="line"><span class="comment">#积攒多少个Event才flush到HDFS一次</span></span><br><span class="line"><span class="meta">a2.sinks.k2.hdfs.batchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="comment">#设置文件类型，可支持压缩</span></span><br><span class="line"><span class="meta">a2.sinks.k2.hdfs.fileType</span> = <span class="string">DataStream</span></span><br><span class="line"><span class="comment">#多久生成一个新的文件</span></span><br><span class="line"><span class="meta">a2.sinks.k2.hdfs.rollInterval</span> = <span class="string">600</span></span><br><span class="line"><span class="comment">#设置每个文件的滚动大小</span></span><br><span class="line"><span class="meta">a2.sinks.k2.hdfs.rollSize</span> = <span class="string">134217700</span></span><br><span class="line"><span class="comment">#文件的滚动与Event数量无关</span></span><br><span class="line"><span class="meta">a2.sinks.k2.hdfs.rollCount</span> = <span class="string">0</span></span><br><span class="line"><span class="comment">#最小冗余数</span></span><br><span class="line"><span class="meta">a2.sinks.k2.hdfs.minBlockReplicas</span> = <span class="string">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line"><span class="meta">a2.channels.c2.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a2.channels.c2.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a2.channels.c2.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="meta">a2.sources.r2.channels</span> = <span class="string">c2</span></span><br><span class="line"><span class="meta">a2.sinks.k2.channel</span> = <span class="string">c2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200919101440.png" alt="image-20200918163711789"></p><p>3．执行监控配置</p><blockquote><p>[xing@hadoop102 flume]$ bin/flume-ng agent –conf conf/ –name a2 –conf-file job/flume-file-hdfs.conf</p></blockquote><p>4．开启Hadoop和Hive并操作Hive产生日志</p><blockquote><p>[xing@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</p></blockquote><blockquote><p>[xing@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</p></blockquote><blockquote><p>[xing@hadoop102 hive]$ bin/hive</p><p>hive (default)&gt;</p></blockquote><p>5．在HDFS上查看文件。</p><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200919101441.png" alt="image-20200918163827842"></p><h2 id="3-3-实时读取目录文件到HDFS案例"><a href="#3-3-实时读取目录文件到HDFS案例" class="headerlink" title="3.3 实时读取目录文件到HDFS案例"></a>3.3 实时读取目录文件到HDFS案例</h2><p>1）案例需求：使用Flume监听整个目录的文件</p><p>2）需求分析：</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922155202.png" alt="image-20200922155201336"></p><p>3）实现步骤：</p><p>1．创建配置文件flume-dir-hdfs.conf</p><blockquote><p>[xing@hadoop102 job]$ touch flume-dir-hdfs.conf</p></blockquote><blockquote><p>[xing@hadoop102 job]$ vim flume-dir-hdfs.conf</p></blockquote><p>添加如下内容</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">a3.sources</span> = <span class="string">r3</span></span><br><span class="line"><span class="meta">a3.sinks</span> = <span class="string">k3</span></span><br><span class="line"><span class="meta">a3.channels</span> = <span class="string">c3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="meta">a3.sources.r3.type</span> = <span class="string">spooldir</span></span><br><span class="line"><span class="meta">a3.sources.r3.spoolDir</span> = <span class="string">/opt/module/flume/upload</span></span><br><span class="line"><span class="meta">a3.sources.r3.fileSuffix</span> = <span class="string">.COMPLETED</span></span><br><span class="line"><span class="meta">a3.sources.r3.fileHeader</span> = <span class="string">true</span></span><br><span class="line"><span class="comment">#忽略所有以.tmp结尾的文件，不上传</span></span><br><span class="line"><span class="meta">a3.sources.r3.ignorePattern</span> = <span class="string">([^ ]*\.tmp)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="meta">a3.sinks.k3.type</span> = <span class="string">hdfs</span></span><br><span class="line"><span class="meta">a3.sinks.k3.hdfs.path</span> = <span class="string">hdfs://hadoop102:9000/flume/upload/%Y%m%d/%H</span></span><br><span class="line"><span class="comment">#上传文件的前缀</span></span><br><span class="line"><span class="meta">a3.sinks.k3.hdfs.filePrefix</span> = <span class="string">upload-</span></span><br><span class="line"><span class="comment">#是否按照时间滚动文件夹</span></span><br><span class="line"><span class="meta">a3.sinks.k3.hdfs.round</span> = <span class="string">true</span></span><br><span class="line"><span class="comment">#多少时间单位创建一个新的文件夹</span></span><br><span class="line"><span class="meta">a3.sinks.k3.hdfs.roundValue</span> = <span class="string">1</span></span><br><span class="line"><span class="comment">#重新定义时间单位</span></span><br><span class="line"><span class="meta">a3.sinks.k3.hdfs.roundUnit</span> = <span class="string">hour</span></span><br><span class="line"><span class="comment">#是否使用本地时间戳</span></span><br><span class="line"><span class="meta">a3.sinks.k3.hdfs.useLocalTimeStamp</span> = <span class="string">true</span></span><br><span class="line"><span class="comment">#积攒多少个Event才flush到HDFS一次</span></span><br><span class="line"><span class="meta">a3.sinks.k3.hdfs.batchSize</span> = <span class="string">100</span></span><br><span class="line"><span class="comment">#设置文件类型，可支持压缩</span></span><br><span class="line"><span class="meta">a3.sinks.k3.hdfs.fileType</span> = <span class="string">DataStream</span></span><br><span class="line"><span class="comment">#多久生成一个新的文件</span></span><br><span class="line"><span class="meta">a3.sinks.k3.hdfs.rollInterval</span> = <span class="string">600</span></span><br><span class="line"><span class="comment">#设置每个文件的滚动大小大概是128M</span></span><br><span class="line"><span class="meta">a3.sinks.k3.hdfs.rollSize</span> = <span class="string">134217700</span></span><br><span class="line"><span class="comment">#文件的滚动与Event数量无关</span></span><br><span class="line"><span class="meta">a3.sinks.k3.hdfs.rollCount</span> = <span class="string">0</span></span><br><span class="line"><span class="comment">#最小冗余数</span></span><br><span class="line"><span class="meta">a3.sinks.k3.hdfs.minBlockReplicas</span> = <span class="string">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line"><span class="meta">a3.channels.c3.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a3.channels.c3.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a3.channels.c3.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="meta">a3.sources.r3.channels</span> = <span class="string">c3</span></span><br><span class="line"><span class="meta">a3.sinks.k3.channel</span> = <span class="string">c3</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200919101442.png" alt="image-20200918164130096"></p><ol start="2"><li>启动监控文件夹命令</li></ol><blockquote><p>[xing@hadoop102 flume]$ bin/flume-ng agent –conf conf/ –name a3 –conf-file job/flume-dir-hdfs.conf</p></blockquote><blockquote><p>==说明==： 在使用Spooling Directory Source时</p><ol><li><p>不要在监控目录中创建并持续修改文件</p></li><li><p>上传完成的文件会以.COMPLETED结尾</p></li><li><p>被监控文件夹每500毫秒扫描一次文件变动</p></li></ol></blockquote><ol start="3"><li>向upload文件夹中添加文件</li></ol><p>在/opt/module/flume目录下创建upload文件夹</p><blockquote><p>[xing@hadoop102 flume]$ mkdir upload</p></blockquote><p>向upload文件夹中添加文件</p><blockquote><p>[xing@hadoop102 upload]$ touch xing.txt</p><p>[xing@hadoop102 upload]$ touch xing.tmp</p><p>[xing@hadoop102 upload]$ touch xing.log</p></blockquote><ol start="4"><li><p>查看HDFS上的数据</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200919101443.png" alt="image-20200918164329801"></p></li><li><p>等待1s，再次查询upload文件夹</p></li></ol><blockquote><p>[xing@hadoop102 upload]$ ll</p><p>总用量 0</p><p>-rw-rw-r–. 1 xing xing 0 5月 20 22:31 xing.log.COMPLETED</p><p>-rw-rw-r–. 1 xing xing 0 5月 20 22:31 xing.tmp</p><p>-rw-rw-r–. 1 xing xing 0 5月 20 22:31 xing.txt.COMPLETED</p></blockquote><h2 id="3-4-单数据源多出口案例-选择器"><a href="#3-4-单数据源多出口案例-选择器" class="headerlink" title="3.4 单数据源多出口案例(选择器)"></a>3.4 单数据源多出口案例(选择器)</h2><ul><li>单Source多Channel、Sink</li></ul><p>​     <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200919101444.png" alt="image-20200918164425782"></p><p>1）案例需求：使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3负责输出到Local FileSystem。</p><p>2）需求分析：</p><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922155346.png" alt="image-20200922155345477"></p><p>3）实现步骤：</p><p>0．准备工作</p><blockquote><p> 在/opt/module/flume/job目录下创建group1文件夹</p><p>[xing@hadoop102 job]$ cd group1/</p></blockquote><blockquote><p>在/opt/module/datas/目录下创建flume3文件夹</p><p>[xing@hadoop102 datas]$ mkdir flume3</p></blockquote><p>1．创建flume-file-flume.conf</p><p>配置1个接收日志文件的source和两个channel、两个sink，分别输送给flume-flume-hdfs和flume-flume-dir。</p><p>创建配置文件并打开</p><blockquote><p>[xing@hadoop102 group1]$ touch flume-file-flume.conf</p><p>[xing@hadoop102 group1]$ vim flume-file-flume.conf</p></blockquote><p>添加如下内容</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line"><span class="meta">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a1.sinks</span> = <span class="string">k1 k2</span></span><br><span class="line"><span class="meta">a1.channels</span> = <span class="string">c1 c2</span></span><br><span class="line"><span class="comment"># 将数据流复制给所有channel</span></span><br><span class="line"><span class="meta">a1.sources.r1.selector.type</span> = <span class="string">replicating</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="meta">a1.sources.r1.type</span> = <span class="string">exec</span></span><br><span class="line"><span class="meta">a1.sources.r1.command</span> = <span class="string">tail -F /opt/module/hive/logs/hive.log</span></span><br><span class="line"><span class="meta">a1.sources.r1.shell</span> = <span class="string">/bin/bash -c</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="meta">a1.sinks.k1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a1.sinks.k1.hostname</span> = <span class="string">hadoop102 </span></span><br><span class="line"><span class="meta">a1.sinks.k1.port</span> = <span class="string">4141</span></span><br><span class="line"></span><br><span class="line"><span class="meta">a1.sinks.k2.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a1.sinks.k2.hostname</span> = <span class="string">hadoop102</span></span><br><span class="line"><span class="meta">a1.sinks.k2.port</span> = <span class="string">4142</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the channel</span></span><br><span class="line"><span class="meta">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="meta">a1.channels.c2.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a1.channels.c2.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a1.channels.c2.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="meta">a1.sources.r1.channels</span> = <span class="string">c1 c2</span></span><br><span class="line"><span class="meta">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a1.sinks.k2.channel</span> = <span class="string">c2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>注：Avro是由Hadoop创始人Doug Cutting创建的一种语言无关的数据序列化和RPC框架。</p><p>注：RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。</p><p>2．创建flume-flume-hdfs.conf</p><p>配置上级Flume输出的Source，输出是到HDFS的Sink。</p><p>创建配置文件并打开</p><blockquote><p>[xing@hadoop102 group1]$ touch flume-flume-hdfs.conf</p><p>[xing@hadoop102 group1]$ vim flume-flume-hdfs.conf</p></blockquote><p>添加如下内容</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line"><span class="meta">a2.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a2.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="meta">a2.channels</span> = <span class="string">c1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="meta">a2.sources.r1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a2.sources.r1.bind</span> = <span class="string">hadoop102</span></span><br><span class="line"><span class="meta">a2.sources.r1.port</span> = <span class="string">4141</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="meta">a2.sinks.k1.type</span> = <span class="string">hdfs</span></span><br><span class="line"><span class="meta">a2.sinks.k1.hdfs.path</span> = <span class="string">hdfs://hadoop102:9000/flume2/%Y%m%d/%H</span></span><br><span class="line"><span class="comment">#上传文件的前缀</span></span><br><span class="line"><span class="meta">a2.sinks.k1.hdfs.filePrefix</span> = <span class="string">flume2-</span></span><br><span class="line"><span class="comment">#是否按照时间滚动文件夹</span></span><br><span class="line"><span class="meta">a2.sinks.k1.hdfs.round</span> = <span class="string">true</span></span><br><span class="line"><span class="comment">#多少时间单位创建一个新的文件夹</span></span><br><span class="line"><span class="meta">a2.sinks.k1.hdfs.roundValue</span> = <span class="string">1</span></span><br><span class="line"><span class="comment">#重新定义时间单位</span></span><br><span class="line"><span class="meta">a2.sinks.k1.hdfs.roundUnit</span> = <span class="string">hour</span></span><br><span class="line"><span class="comment">#是否使用本地时间戳</span></span><br><span class="line"><span class="meta">a2.sinks.k1.hdfs.useLocalTimeStamp</span> = <span class="string">true</span></span><br><span class="line"><span class="comment">#积攒多少个Event才flush到HDFS一次</span></span><br><span class="line"><span class="meta">a2.sinks.k1.hdfs.batchSize</span> = <span class="string">100</span></span><br><span class="line"><span class="comment">#设置文件类型，可支持压缩</span></span><br><span class="line"><span class="meta">a2.sinks.k1.hdfs.fileType</span> = <span class="string">DataStream</span></span><br><span class="line"><span class="comment">#多久生成一个新的文件</span></span><br><span class="line"><span class="meta">a2.sinks.k1.hdfs.rollInterval</span> = <span class="string">600</span></span><br><span class="line"><span class="comment">#设置每个文件的滚动大小大概是128M</span></span><br><span class="line"><span class="meta">a2.sinks.k1.hdfs.rollSize</span> = <span class="string">134217700</span></span><br><span class="line"><span class="comment">#文件的滚动与Event数量无关</span></span><br><span class="line"><span class="meta">a2.sinks.k1.hdfs.rollCount</span> = <span class="string">0</span></span><br><span class="line"><span class="comment">#最小冗余数</span></span><br><span class="line"><span class="meta">a2.sinks.k1.hdfs.minBlockReplicas</span> = <span class="string">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the channel</span></span><br><span class="line"><span class="meta">a2.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a2.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a2.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="meta">a2.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a2.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>3．创建flume-flume-dir.conf</p><p>配置上级Flume输出的Source，输出是到本地目录的Sink。</p><p>创建配置文件并打开</p><blockquote><p>[xing@hadoop102 group1]$ touch flume-flume-dir.conf</p><p>[xing@hadoop102 group1]$ vim flume-flume-dir.conf</p></blockquote><p>添加如下内容</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line"><span class="meta">a3.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a3.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="meta">a3.channels</span> = <span class="string">c2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="meta">a3.sources.r1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a3.sources.r1.bind</span> = <span class="string">hadoop102</span></span><br><span class="line"><span class="meta">a3.sources.r1.port</span> = <span class="string">4142</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="meta">a3.sinks.k1.type</span> = <span class="string">file_roll</span></span><br><span class="line"><span class="meta">a3.sinks.k1.sink.directory</span> = <span class="string">/opt/module/datas/flume3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the channel</span></span><br><span class="line"><span class="meta">a3.channels.c2.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a3.channels.c2.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a3.channels.c2.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="meta">a3.sources.r1.channels</span> = <span class="string">c2</span></span><br><span class="line"><span class="meta">a3.sinks.k1.channel</span> = <span class="string">c2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>==提示==：输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录。</p><p>4．执行配置文件</p><p>分别开启对应配置文件：flume-flume-dir，flume-flume-hdfs，flume-file-flume。</p><blockquote><p>[xing@hadoop102 flume]$ bin/flume-ng agent –conf conf/ –name a3 –conf-file job/group1/flume-flume-dir.conf</p></blockquote><blockquote><p>[xing@hadoop102 flume]$ bin/flume-ng agent –conf conf/ –name a2 –conf-file job/group1/flume-flume-hdfs.conf</p></blockquote><blockquote><p>[xing@hadoop102 flume]$ bin/flume-ng agent –conf conf/ –name a1 –conf-file job/group1/flume-file-flume.conf</p></blockquote><p>5．启动Hadoop和Hive</p><blockquote><p>[xing@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</p></blockquote><blockquote><p>[xing@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</p></blockquote><blockquote><p>[xing@hadoop102 hive]$ bin/hive</p><p>hive (default)&gt;</p></blockquote><p>6．检查HDFS上数据</p><p>7检查/opt/module/datas/flume3目录中数据</p><blockquote><p>[xing@hadoop102 flume3]$ ll</p><p>总用量 8</p><p>-rw-rw-r–. 1 xing xing 5942 5月 22 00:09 1526918887550-3</p></blockquote><h2 id="3-5-单数据源多出口案例-Sink组"><a href="#3-5-单数据源多出口案例-Sink组" class="headerlink" title="3.5 单数据源多出口案例(Sink组)"></a>3.5 单数据源多出口案例(Sink组)</h2><ul><li><p>单Source、Channel多Sink(负载均衡)</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922155440.png" alt="image-20200918165035775"></p></li></ul><p>1）案例需求：使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3也负责存储到HDFS </p><p>2）需求分析：</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922155513.png" alt="image-20200918165128468"></p><p>3）实现步骤：</p><p>0．准备工作</p><blockquote><p> 在/opt/module/flume/job目录下创建group2文件夹</p><p>[xing@hadoop102 job]$ cd group2/</p></blockquote><p>1．创建flume-netcat-flume.conf</p><blockquote><p>配置1个接收日志文件的source和1个channel、两个sink，分别输送给flume-flume-console1和flume-flume-console2。</p><p>创建配置文件并打开</p></blockquote><blockquote><p>[xing@hadoop102 group2]$ touch flume-netcat-flume.conf</p><p>[xing@hadoop102 group2]$ vim flume-netcat-flume.conf</p></blockquote><p>添加如下内容</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line"><span class="meta">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a1.sinkgroups</span> = <span class="string">g1</span></span><br><span class="line"><span class="meta">a1.sinks</span> = <span class="string">k1 k2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="meta">a1.sources.r1.type</span> = <span class="string">netcat</span></span><br><span class="line"><span class="meta">a1.sources.r1.bind</span> = <span class="string">localhost</span></span><br><span class="line"><span class="meta">a1.sources.r1.port</span> = <span class="string">44444</span></span><br><span class="line"></span><br><span class="line"><span class="meta">a1.sinkgroups.g1.processor.type</span> = <span class="string">load_balance</span></span><br><span class="line"><span class="meta">a1.sinkgroups.g1.processor.backoff</span> = <span class="string">true</span></span><br><span class="line"><span class="meta">a1.sinkgroups.g1.processor.selector</span> = <span class="string">round_robin</span></span><br><span class="line"><span class="meta">a1.sinkgroups.g1.processor.selector.maxTimeOut</span>=<span class="string">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="meta">a1.sinks.k1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a1.sinks.k1.hostname</span> = <span class="string">hadoop102</span></span><br><span class="line"><span class="meta">a1.sinks.k1.port</span> = <span class="string">4141</span></span><br><span class="line"></span><br><span class="line"><span class="meta">a1.sinks.k2.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a1.sinks.k2.hostname</span> = <span class="string">hadoop102</span></span><br><span class="line"><span class="meta">a1.sinks.k2.port</span> = <span class="string">4142</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the channel</span></span><br><span class="line"><span class="meta">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="meta">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a1.sinkgroups.g1.sinks</span> = <span class="string">k1 k2</span></span><br><span class="line"><span class="meta">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a1.sinks.k2.channel</span> = <span class="string">c1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>==注==：Avro是由Hadoop创始人Doug Cutting创建的一种语言无关的数据序列化和RPC框架。</p><p>注：RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。</p><p>2．创建flume-flume-console1.conf</p><p>配置上级Flume输出的Source，输出是到本地控制台。</p><p>创建配置文件并打开</p><blockquote><p>[xing@hadoop102 group2]$ touch flume-flume-console1.conf</p><p>[xing@hadoop102 group2]$ vim flume-flume-console1.conf</p></blockquote><p>添加如下内容</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line"><span class="meta">a2.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a2.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="meta">a2.channels</span> = <span class="string">c1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="meta">a2.sources.r1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a2.sources.r1.bind</span> = <span class="string">hadoop102</span></span><br><span class="line"><span class="meta">a2.sources.r1.port</span> = <span class="string">4141</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="meta">a2.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the channel</span></span><br><span class="line"><span class="meta">a2.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a2.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a2.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="meta">a2.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a2.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>3．创建flume-flume-console2.conf</p><blockquote><p>配置上级Flume输出的Source，输出是到本地控制台。</p><p>创建配置文件并打开</p></blockquote><blockquote><p>[xing@hadoop102 group2]$ touch flume-flume-console2.conf</p><p>[xing@hadoop102 group2]$ vim flume-flume-console2.conf</p></blockquote><p>添加如下内容</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line"><span class="meta">a3.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a3.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="meta">a3.channels</span> = <span class="string">c2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="meta">a3.sources.r1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a3.sources.r1.bind</span> = <span class="string">hadoop102</span></span><br><span class="line"><span class="meta">a3.sources.r1.port</span> = <span class="string">4142</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="meta">a3.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the channel</span></span><br><span class="line"><span class="meta">a3.channels.c2.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a3.channels.c2.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a3.channels.c2.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="meta">a3.sources.r1.channels</span> = <span class="string">c2</span></span><br><span class="line"><span class="meta">a3.sinks.k1.channel</span> = <span class="string">c2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>4．执行配置文件</p><blockquote><p>分别开启对应配置文件：flume-flume-console2，flume-flume-console1，flume-netcat-flume。</p></blockquote><blockquote><p>[xing@hadoop102 flume]$ bin/flume-ng agent –conf conf/ –name a3 –conf-file job/group2/flume-flume-console2.conf -Dflume.root.logger=INFO,console</p></blockquote><blockquote><p>[xing@hadoop102 flume]$ bin/flume-ng agent –conf conf/ –name a2 –conf-file job/group2/flume-flume-console1.conf -Dflume.root.logger=INFO,console </p></blockquote><blockquote><p>[xing@hadoop102 flume]$ bin/flume-ng agent –conf conf/ –name a1 –conf-file job/group2/flume-netcat-flume.conf</p></blockquote><ol start="5"><li>使用telnet工具向本机的44444端口发送内容</li></ol><blockquote><p>$ telnet localhost 44444</p></blockquote><ol start="6"><li>查看Flume2及Flume3的控制台打印日志</li></ol><h2 id="3-6-多数据源汇总案例"><a href="#3-6-多数据源汇总案例" class="headerlink" title="3.6 多数据源汇总案例"></a>3.6 多数据源汇总案例</h2><ul><li><p>多Source汇总数据到单Flume</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922155737.png" alt="image-20200922155636501"></p></li></ul><p>1）  案例需求：</p><blockquote><p>hadoop103上的Flume-1监控文件/opt/module/group.log，</p></blockquote><blockquote><p>hadoop102上的Flume-2监控某一个端口的数据流，</p></blockquote><blockquote><p>Flume-1与Flume-2将数据发送给hadoop104上的Flume-3，Flume-3将最终数据打印到控制台。 </p></blockquote><p>2）需求分析：</p><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922155745.png" alt="image-20200918165728037"></p><p>3）实现步骤：</p><p>0．准备工作</p><blockquote><p>分发Flume</p></blockquote><blockquote><p>[xing@hadoop102 module]$ xsync flume</p><p>​    在hadoop102、hadoop103以及hadoop104的/opt/module/flume/job目录下创建一个group3文件夹。</p><p>[xing@hadoop102 job]$ mkdir group3</p><p>[xing@hadoop103 job]$ mkdir group3</p><p>[xing@hadoop104 job]$ mkdir group3</p></blockquote><p>1．创建flume1-logger-flume.conf</p><blockquote><p>配置Source用于监控hive.log文件，配置Sink输出数据到下一级Flume。</p><p>在hadoop103上创建配置文件并打开</p></blockquote><blockquote><p>[xing@hadoop103 group3]$ touch flume1-logger-flume.conf</p><p>[xing@hadoop103 group3]$ vim flume1-logger-flume.conf </p></blockquote><p>添加如下内容</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line"><span class="meta">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="meta">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="meta">a1.sources.r1.type</span> = <span class="string">exec</span></span><br><span class="line"><span class="meta">a1.sources.r1.command</span> = <span class="string">tail -F /opt/module/group.log</span></span><br><span class="line"><span class="meta">a1.sources.r1.shell</span> = <span class="string">/bin/bash -c</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="meta">a1.sinks.k1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a1.sinks.k1.hostname</span> = <span class="string">hadoop104</span></span><br><span class="line"><span class="meta">a1.sinks.k1.port</span> = <span class="string">4141</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the channel</span></span><br><span class="line"><span class="meta">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="meta">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>2．创建flume2-netcat-flume.conf</p><blockquote><p>配置Source监控端口44444数据流，配置Sink数据到下一级Flume：</p><p>在hadoop102上创建配置文件并打开</p></blockquote><blockquote><p>[xing@hadoop102 group3]$ touch flume2-netcat-flume.conf</p><p>[xing@hadoop102 group3]$ vim flume2-netcat-flume.conf</p></blockquote><p>添加如下内容</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line"><span class="meta">a2.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a2.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="meta">a2.channels</span> = <span class="string">c1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="meta">a2.sources.r1.type</span> = <span class="string">netcat</span></span><br><span class="line"><span class="meta">a2.sources.r1.bind</span> = <span class="string">hadoop102</span></span><br><span class="line"><span class="meta">a2.sources.r1.port</span> = <span class="string">44444</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="meta">a2.sinks.k1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a2.sinks.k1.hostname</span> = <span class="string">hadoop104</span></span><br><span class="line"><span class="meta">a2.sinks.k1.port</span> = <span class="string">4141</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line"><span class="meta">a2.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a2.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a2.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="meta">a2.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a2.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>3．创建flume3-flume-logger.conf</p><blockquote><p>配置source用于接收flume1与flume2发送过来的数据流，最终合并后sink到控制台。</p><p>在hadoop104上创建配置文件并打开</p></blockquote><blockquote><p>[xing@hadoop104 group3]$ touch flume3-flume-logger.conf</p><p>[xing@hadoop104 group3]$ vim flume3-flume-logger.conf</p></blockquote><p>添加如下内容</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line"><span class="meta">a3.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a3.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="meta">a3.channels</span> = <span class="string">c1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="meta">a3.sources.r1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">a3.sources.r1.bind</span> = <span class="string">hadoop104</span></span><br><span class="line"><span class="meta">a3.sources.r1.port</span> = <span class="string">4141</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="meta">a3.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the channel</span></span><br><span class="line"><span class="meta">a3.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a3.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a3.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="meta">a3.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a3.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>4．执行配置文件</p><blockquote><p>分别开启对应配置文件：flume3-flume-logger.conf，flume2-netcat-flume.conf，flume1-logger-flume.conf。</p></blockquote><blockquote><p>[xing@hadoop104 flume]$ bin/flume-ng agent –conf conf/ –name a3 –conf-file job/group3/flume3-flume-logger.conf -Dflume.root.logger=INFO,console</p></blockquote><blockquote><p>[xing@hadoop102 flume]$ bin/flume-ng agent –conf conf/ –name a2 –conf-file job/group3/flume2-netcat-flume.conf</p></blockquote><blockquote><p>[xing@hadoop103 flume]$ bin/flume-ng agent –conf conf/ –name a1 –conf-file job/group3/flume1-logger-flume.conf</p></blockquote><p>5．在hadoop103上向/opt/module目录下的group.log追加内容</p><blockquote><p>[xing@hadoop103 module]$ echo ‘hello’ &gt; group.log</p></blockquote><p>6．在hadoop102上向44444端口发送数据</p><blockquote><p>[xing@hadoop102 flume]$ telnet hadoop102 44444</p></blockquote><p>7.检查hadoop104上数据</p><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922161048.png" alt="image-20200922161042307"></p><h1 id="第4章-Flume监控之Ganglia"><a href="#第4章-Flume监控之Ganglia" class="headerlink" title="第4章 Flume监控之Ganglia"></a>第4章 Flume监控之Ganglia</h1><h2 id="4-1-Ganglia的安装与部署"><a href="#4-1-Ganglia的安装与部署" class="headerlink" title="4.1 Ganglia的安装与部署"></a>4.1 Ganglia的安装与部署</h2><p><strong>1)</strong> <strong>安装httpd服务与php</strong></p><blockquote><p>[xing@hadoop102 flume]$ sudo yum -y install httpd php</p></blockquote><p><strong>2)</strong> <strong>安装其他依赖</strong></p><blockquote><p>[xing@hadoop102 flume]$ sudo yum -y install rrdtool perl-rrdtool rrdtool-devel</p></blockquote><blockquote><p>[xing@hadoop102 flume]$ sudo yum -y install apr-devel</p></blockquote><p><strong>3)</strong> <strong>安装ganglia</strong></p><blockquote><p>[xing@hadoop102 flume]$ sudo rpm -Uvh <a href="http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm">http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm</a></p></blockquote><blockquote><p>[xing@hadoop102 flume]$ sudo yum -y install ganglia-gmetad </p></blockquote><blockquote><p>[xing@hadoop102 flume]$ sudo yum -y install ganglia-web</p></blockquote><blockquote><p>[xing@hadoop102 flume]$ sudo yum install -y ganglia-gmond</p></blockquote><p><strong>4)</strong> <strong>修改配置文件/etc/httpd/conf.d/ganglia.conf</strong></p><blockquote><p>[xing@hadoop102 flume]$ sudo vim /etc/httpd/conf.d/ganglia.conf</p></blockquote><p><strong>修改为红颜色的配置：</strong></p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922160002.png" alt="image-20200922155938629"></p><p><strong>5)</strong> <strong>修改配置文件/etc/ganglia/gmetad.conf</strong></p><blockquote><p>[xing@hadoop102 flume]$ sudo vim /etc/ganglia/gmetad.conf</p></blockquote><p><strong>修改为：</strong></p><p>data_source “hadoop102” 192.168.1.102</p><p><strong>6)</strong> <strong>修改配置文件/etc/ganglia/gmond.conf</strong></p><blockquote><p>[xing@hadoop102 flume]$ sudo vim /etc/ganglia/gmond.conf </p></blockquote><p><strong>修改为：</strong></p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922155831.png" alt="image-20200918170555260"></p><p><strong>7)</strong> <strong>修改配置文件/etc/selinux/config</strong></p><blockquote><p>[xing@hadoop102 flume]$ sudo vim /etc/selinux/config</p></blockquote><p><strong>==修改为==：</strong></p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922160054.png" alt="image-20200922160053908"></p><p>==尖叫提示==：selinux本次生效关闭必须重启，如果此时不想重启，可以临时生效之：</p><blockquote><p>[xing@hadoop102 flume]$ sudo setenforce 0</p></blockquote><p><strong>5)</strong> <strong>启动ganglia</strong></p><blockquote><p>[xing@hadoop102 flume]$ sudo service httpd start</p><p>[xing@hadoop102 flume]$ sudo service gmetad start</p><p>[xing@hadoop102 flume]$ sudo service gmond start</p></blockquote><p><strong>6)</strong> <strong>打开网页浏览ganglia页面</strong></p><p><a href="http://192.168.1.102/ganglia">http://192.168.1.102/ganglia</a></p><p>==尖叫提示==：如果完成以上操作依然出现权限不足错误，请修改/var/lib/ganglia目录的权限：</p><blockquote><p>[xing@hadoop102 flume]$ sudo chmod -R 777 /var/lib/ganglia</p></blockquote><h2 id="4-2-操作Flume测试监控"><a href="#4-2-操作Flume测试监控" class="headerlink" title="4.2 操作Flume测试监控"></a>4.2 操作Flume测试监控</h2><p><strong>1)</strong> <strong>修改/opt/module/flume/conf目录下的flume-env.sh配置：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JAVA_OPTS=<span class="string">&quot;-Dflume.monitoring.type=ganglia</span></span><br><span class="line"><span class="string">-Dflume.monitoring.hosts=192.168.1.102:8649</span></span><br><span class="line"><span class="string">-Xms100m</span></span><br><span class="line"><span class="string">-Xmx200m&quot;</span></span><br></pre></td></tr></table></figure><p><strong>2)</strong> <strong>启动Flume任务</strong></p><blockquote><p>[xing@hadoop102 flume]$ bin/flume-ng agent \</p><p>–conf conf/ \</p><p>–name a1 \</p><p>–conf-file job/flume-telnet-logger.conf \</p><p>-Dflume.root.logger==INFO,console \</p><p>-Dflume.monitoring.type=ganglia \</p><p>-Dflume.monitoring.hosts=192.168.1.102:8649</p></blockquote><p><strong>3)</strong> <strong>发送数据观察ganglia监测图</strong></p><blockquote><p>[xing@hadoop102 flume]$ telnet localhost 44444</p></blockquote><p><strong>样式如图：</strong></p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922160129.png" alt="image-20200918170854866"></p><p>图例说明：</p><table><thead><tr><th>字段（图表名称）</th><th>字段含义</th></tr></thead><tbody><tr><td>EventPutAttemptCount</td><td>source尝试写入channel的事件总数量</td></tr><tr><td>EventPutSuccessCount</td><td>成功写入channel且提交的事件总数量</td></tr><tr><td>EventTakeAttemptCount</td><td>sink尝试从channel拉取事件的总数量。这不意味着每次事件都被返回，因为sink拉取的时候channel可能没有任何数据。</td></tr><tr><td>EventTakeSuccessCount</td><td>sink成功读取的事件的总数量</td></tr><tr><td>StartTime</td><td>channel启动的时间（毫秒）</td></tr><tr><td>StopTime</td><td>channel停止的时间（毫秒）</td></tr><tr><td>ChannelSize</td><td>目前channel中事件的总数量</td></tr><tr><td>ChannelFillPercentage</td><td>channel占用百分比</td></tr><tr><td>ChannelCapacity</td><td>channel的容量</td></tr></tbody></table><h1 id="第5章-Flume高级之自定义MySQLSource"><a href="#第5章-Flume高级之自定义MySQLSource" class="headerlink" title="第5章 Flume高级之自定义MySQLSource"></a>第5章 Flume高级之自定义MySQLSource</h1><h2 id="5-1-自定义Source说明"><a href="#5-1-自定义Source说明" class="headerlink" title="5.1 自定义Source说明"></a>5.1 自定义Source说明</h2><p>==Source是负责接收数据到Flume Agent的组件==。Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、==exec==、jms、==spooling directory==、netcat、sequence generator、syslog、http、legacy。官方提供的source类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些Source。</p><p>如：实时监控MySQL，从MySQL中获取数据传输到HDFS或者其他存储框架，所以此时需要我们自己实现MySQLSource。</p><p>官方也提供了自定义source的接口：</p><p>官网说明：<a href="https://flume.apache.org/FlumeDeveloperGuide.html#source">https://flume.apache.org/FlumeDeveloperGuide.html#source</a></p><h2 id="5-3-自定义MySQLSource组成"><a href="#5-3-自定义MySQLSource组成" class="headerlink" title="5.3 自定义MySQLSource组成"></a>5.3 自定义MySQLSource组成</h2><ul><li>自定义MySQLSource组成</li></ul><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922160312.png" alt="image-20200922160305015"></p><h2 id="5-2-自定义MySQLSource步骤"><a href="#5-2-自定义MySQLSource步骤" class="headerlink" title="5.2 自定义MySQLSource步骤"></a>5.2 自定义MySQLSource步骤</h2><p>根据官方说明自定义MySqlSource需要继承AbstractSource类并实现Configurable和PollableSource接口。</p><p>实现相应方法：</p><p>getBackOffSleepIncrement()//暂不用</p><p>getMaxBackOffSleepInterval()//暂不用</p><p>configure(Context context)//初始化context</p><p>process()//获取数据（从MySql获取数据，业务处理比较复杂，所以我们定义一个专门的类——SQLSourceHelper来处理跟MySql的交互），封装成Event并写入Channel，==这个方法被循环调用==</p><p>stop()//关闭相关的资源</p><h2 id="5-4-代码实现"><a href="#5-4-代码实现" class="headerlink" title="5.4 代码实现"></a>5.4 代码实现</h2><h3 id="5-4-1-导入Pom依赖"><a href="#5-4-1-导入Pom依赖" class="headerlink" title="5.4.1 导入Pom依赖"></a>5.4.1 导入Pom依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="5-4-2-添加配置信息"><a href="#5-4-2-添加配置信息" class="headerlink" title="5.4.2 添加配置信息"></a>5.4.2 添加配置信息</h3><p>在ClassPath下添加jdbc.properties和log4j. properties</p><p>jdbc.properties:</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">dbDriver</span>=<span class="string">com.mysql.jdbc.Driver</span></span><br><span class="line"><span class="attr">dbUrl</span>=<span class="string">jdbc:mysql://hadoop102:3306/mysqlsource?useUnicode=true&amp;characterEncoding=utf-8</span></span><br><span class="line"><span class="attr">dbUser</span>=<span class="string">root</span></span><br><span class="line"><span class="attr">dbPassword</span>=<span class="string">000000</span></span><br></pre></td></tr></table></figure><p>log4j. properties:</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#--------console-----------</span></span><br><span class="line"><span class="meta">log4j.rootLogger</span>=<span class="string">info,myconsole,myfile</span></span><br><span class="line"><span class="meta">log4j.appender.myconsole</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="meta">log4j.appender.myconsole.layout</span>=<span class="string">org.apache.log4j.SimpleLayout</span></span><br><span class="line"><span class="comment">#log4j.appender.myconsole.layout.ConversionPattern =%d [%t] %-5p [%c] - %m%n</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#log4j.rootLogger=error,myfile</span></span><br><span class="line"><span class="meta">log4j.appender.myfile</span>=<span class="string">org.apache.log4j.DailyRollingFileAppender</span></span><br><span class="line"><span class="meta">log4j.appender.myfile.File</span>=<span class="string">/tmp/flume.log</span></span><br><span class="line"><span class="meta">log4j.appender.myfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.myfile.layout.ConversionPattern</span> =<span class="string">%d [%t] %-5p [%c] - %m%n</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="5-4-3-SQLSourceHelper"><a href="#5-4-3-SQLSourceHelper" class="headerlink" title="5.4.3 SQLSourceHelper"></a>5.4.3 SQLSourceHelper</h3><p>1）属性说明：</p><table><thead><tr><th>属性</th><th>说明（括号中为默认值）</th></tr></thead><tbody><tr><td>runQueryDelay</td><td>查询时间间隔（10000）</td></tr><tr><td>batchSize</td><td>缓存大小（100）</td></tr><tr><td>startFrom</td><td>查询语句开始id（0）</td></tr><tr><td>currentIndex</td><td>查询语句当前id，每次查询之前需要查元数据表</td></tr><tr><td>recordSixe</td><td>查询返回条数</td></tr><tr><td>table</td><td>监控的表名</td></tr><tr><td>columnsToSelect</td><td>查询字段（*）</td></tr><tr><td>customQuery</td><td>用户传入的查询语句</td></tr><tr><td>query</td><td>查询语句</td></tr><tr><td>defaultCharsetResultSet</td><td>编码格式（UTF-8）</td></tr></tbody></table><p>2）方法说明：</p><table><thead><tr><th>方法</th><th>说明</th></tr></thead><tbody><tr><td>SQLSourceHelper(Context  context)</td><td>构造方法，初始化属性及获取JDBC连接</td></tr><tr><td>InitConnection(String  url, String user, String pw)</td><td>获取JDBC连接</td></tr><tr><td>checkMandatoryProperties()</td><td>校验相关属性是否设置（实际开发中可增加内容）</td></tr><tr><td>buildQuery()</td><td>根据实际情况构建sql语句，返回值String</td></tr><tr><td>executeQuery()</td><td>执行sql语句的查询操作，返回值List&lt;List<Object>&gt;</td></tr><tr><td>getAllRows(List&lt;List&lt;Object&gt;&gt;  queryResult)</td><td>将查询结果转换为String，方便后续操作</td></tr><tr><td>updateOffset2DB(int  size)</td><td>根据每次查询结果将offset写入元数据表</td></tr><tr><td>execSql(String sql)</td><td>具体执行sql语句方法</td></tr><tr><td>getStatusDBIndex(int startFrom)</td><td>获取元数据表中的offset</td></tr><tr><td>queryOne(String sql)</td><td>获取元数据表中的offset实际sql语句执行方法</td></tr><tr><td>close()</td><td>关闭资源</td></tr></tbody></table><p><strong>3</strong>）代码分析</p><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922160418.png" alt="image-20200922160403508"></p><p><strong>4</strong>）代码实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing.source;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.ConfigurationException;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.sql.*;</span><br><span class="line"><span class="keyword">import</span> java.text.ParseException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SQLSourceHelper</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(SQLSourceHelper.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> runQueryDelay, <span class="comment">//两次查询的时间间隔</span></span><br><span class="line">            startFrom,            <span class="comment">//开始id</span></span><br><span class="line">            currentIndex,        <span class="comment">//当前id</span></span><br><span class="line">            recordSixe = <span class="number">0</span>,      <span class="comment">//每次查询返回结果的条数</span></span><br><span class="line">            maxRow;                <span class="comment">//每次查询的最大条数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String table,       <span class="comment">//要操作的表</span></span><br><span class="line">            columnsToSelect,     <span class="comment">//用户传入的查询的列</span></span><br><span class="line">            customQuery,          <span class="comment">//用户传入的查询语句</span></span><br><span class="line">            query,                 <span class="comment">//构建的查询语句</span></span><br><span class="line">            defaultCharsetResultSet;<span class="comment">//编码集</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//上下文，用来获取配置文件</span></span><br><span class="line">    <span class="keyword">private</span> Context context;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//为定义的变量赋值（默认值），可在flume任务的配置文件中修改</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_QUERY_DELAY = <span class="number">10000</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_START_VALUE = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_MAX_ROWS = <span class="number">2000</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_COLUMNS_SELECT = <span class="string">&quot;*&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_CHARSET_RESULTSET = <span class="string">&quot;UTF-8&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Connection conn = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> PreparedStatement ps = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String connectionURL, connectionUserName, connectionPassword;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//加载静态资源</span></span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line"></span><br><span class="line">        Properties p = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            p.load(SQLSourceHelper.class.getClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;));</span><br><span class="line">            connectionURL = p.getProperty(<span class="string">&quot;dbUrl&quot;</span>);</span><br><span class="line">            connectionUserName = p.getProperty(<span class="string">&quot;dbUser&quot;</span>);</span><br><span class="line">            connectionPassword = p.getProperty(<span class="string">&quot;dbPassword&quot;</span>);</span><br><span class="line">            Class.forName(p.getProperty(<span class="string">&quot;dbDriver&quot;</span>));</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException | ClassNotFoundException e) &#123;</span><br><span class="line">            LOG.error(e.toString());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取JDBC连接</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Connection <span class="title">InitConnection</span><span class="params">(String url, String user, String pw)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line"></span><br><span class="line">            Connection conn = DriverManager.getConnection(url, user, pw);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (conn == <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> SQLException();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> conn;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">SQLSourceHelper(Context context) <span class="keyword">throws</span> ParseException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//初始化上下文</span></span><br><span class="line">        <span class="keyword">this</span>.context = context;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//有默认值参数：获取flume任务配置文件中的参数，读不到的采用默认值</span></span><br><span class="line">        <span class="keyword">this</span>.columnsToSelect = context.getString(<span class="string">&quot;columns.to.select&quot;</span>, DEFAULT_COLUMNS_SELECT);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.runQueryDelay = context.getInteger(<span class="string">&quot;run.query.delay&quot;</span>, DEFAULT_QUERY_DELAY);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.startFrom = context.getInteger(<span class="string">&quot;start.from&quot;</span>, DEFAULT_START_VALUE);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.defaultCharsetResultSet = context.getString(<span class="string">&quot;default.charset.resultset&quot;</span>, DEFAULT_CHARSET_RESULTSET);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//无默认值参数：获取flume任务配置文件中的参数</span></span><br><span class="line">        <span class="keyword">this</span>.table = context.getString(<span class="string">&quot;table&quot;</span>);</span><br><span class="line">        <span class="keyword">this</span>.customQuery = context.getString(<span class="string">&quot;custom.query&quot;</span>);</span><br><span class="line"></span><br><span class="line">        connectionURL = context.getString(<span class="string">&quot;connection.url&quot;</span>);</span><br><span class="line"></span><br><span class="line">        connectionUserName = context.getString(<span class="string">&quot;connection.user&quot;</span>);</span><br><span class="line"></span><br><span class="line">        connectionPassword = context.getString(<span class="string">&quot;connection.password&quot;</span>);</span><br><span class="line"></span><br><span class="line">        conn = InitConnection(connectionURL, connectionUserName, connectionPassword);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//校验相应的配置信息，如果没有默认值的参数也没赋值，抛出异常</span></span><br><span class="line">        checkMandatoryProperties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取当前的id</span></span><br><span class="line">        currentIndex = getStatusDBIndex(startFrom);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//构建查询语句</span></span><br><span class="line">        query = buildQuery();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//校验相应的配置信息（表，查询语句以及数据库连接的参数）</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">checkMandatoryProperties</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (table == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ConfigurationException(<span class="string">&quot;property table not set&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (connectionURL == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ConfigurationException(<span class="string">&quot;connection.url property not set&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (connectionUserName == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ConfigurationException(<span class="string">&quot;connection.user property not set&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (connectionPassword == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ConfigurationException(<span class="string">&quot;connection.password property not set&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构建sql语句</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">buildQuery</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        String sql = <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取当前id</span></span><br><span class="line">        currentIndex = getStatusDBIndex(startFrom);</span><br><span class="line">        LOG.info(currentIndex + <span class="string">&quot;&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (customQuery == <span class="keyword">null</span>) &#123;</span><br><span class="line">            sql = <span class="string">&quot;SELECT &quot;</span> + columnsToSelect + <span class="string">&quot; FROM &quot;</span> + table;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            sql = customQuery;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        StringBuilder execSql = <span class="keyword">new</span> StringBuilder(sql);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//以id作为offset</span></span><br><span class="line">        <span class="keyword">if</span> (!sql.contains(<span class="string">&quot;where&quot;</span>)) &#123;</span><br><span class="line">            execSql.append(<span class="string">&quot; where &quot;</span>);</span><br><span class="line">            execSql.append(<span class="string">&quot;id&quot;</span>).append(<span class="string">&quot;&gt;&quot;</span>).append(currentIndex);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> execSql.toString();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">int</span> length = execSql.toString().length();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> execSql.toString().substring(<span class="number">0</span>, length - String.valueOf(currentIndex).length()) + currentIndex;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//执行查询</span></span><br><span class="line">List&lt;List&lt;Object&gt;&gt; executeQuery() &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//每次执行查询时都要重新生成sql，因为id不同</span></span><br><span class="line">            customQuery = buildQuery();</span><br><span class="line"></span><br><span class="line">            <span class="comment">//存放结果的集合</span></span><br><span class="line">            List&lt;List&lt;Object&gt;&gt; results = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (ps == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">//</span></span><br><span class="line">                ps = conn.prepareStatement(customQuery);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            ResultSet result = ps.executeQuery(customQuery);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (result.next()) &#123;</span><br><span class="line"></span><br><span class="line">                <span class="comment">//存放一条数据的集合（多个列）</span></span><br><span class="line">                List&lt;Object&gt; row = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">                <span class="comment">//将返回结果放入集合</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= result.getMetaData().getColumnCount(); i++) &#123;</span><br><span class="line">                    row.add(result.getObject(i));</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                results.add(row);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            LOG.info(<span class="string">&quot;execSql:&quot;</span> + customQuery + <span class="string">&quot;\nresultSize:&quot;</span> + results.size());</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> results;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">            LOG.error(e.toString());</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 重新连接</span></span><br><span class="line">            conn = InitConnection(connectionURL, connectionUserName, connectionPassword);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将结果集转化为字符串，每一条数据是一个list集合，将每一个小的list集合转化为字符串</span></span><br><span class="line"><span class="function">List&lt;String&gt; <span class="title">getAllRows</span><span class="params">(List&lt;List&lt;Object&gt;&gt; queryResult)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        List&lt;String&gt; allRows = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (queryResult == <span class="keyword">null</span> || queryResult.isEmpty())</span><br><span class="line">            <span class="keyword">return</span> allRows;</span><br><span class="line"></span><br><span class="line">        StringBuilder row = <span class="keyword">new</span> StringBuilder();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (List&lt;Object&gt; rawRow : queryResult) &#123;</span><br><span class="line"></span><br><span class="line">            Object value = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (Object aRawRow : rawRow) &#123;</span><br><span class="line"></span><br><span class="line">                value = aRawRow;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (value == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    row.append(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    row.append(aRawRow.toString()).append(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            allRows.add(row.toString());</span><br><span class="line">            row = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> allRows;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//更新offset元数据状态，每次返回结果集后调用。必须记录每次查询的offset值，为程序中断续跑数据时使用，以id为offset</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">updateOffset2DB</span><span class="params">(<span class="keyword">int</span> size)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//以source_tab做为KEY，如果不存在则插入，存在则更新（每个源表对应一条记录）</span></span><br><span class="line">        String sql = <span class="string">&quot;insert into flume_meta(source_tab,currentIndex) VALUES(&#x27;&quot;</span></span><br><span class="line">                + <span class="keyword">this</span>.table</span><br><span class="line">                + <span class="string">&quot;&#x27;,&#x27;&quot;</span> + (recordSixe += size)</span><br><span class="line">                + <span class="string">&quot;&#x27;) on DUPLICATE key update source_tab=values(source_tab),currentIndex=values(currentIndex)&quot;</span>;</span><br><span class="line"></span><br><span class="line">        LOG.info(<span class="string">&quot;updateStatus Sql:&quot;</span> + sql);</span><br><span class="line"></span><br><span class="line">        execSql(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//执行sql语句</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">execSql</span><span class="params">(String sql)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ps = conn.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">            LOG.info(<span class="string">&quot;exec::&quot;</span> + sql);</span><br><span class="line"></span><br><span class="line">            ps.execute();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取当前id的offset</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Integer <span class="title">getStatusDBIndex</span><span class="params">(<span class="keyword">int</span> startFrom)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从flume_meta表中查询出当前的id是多少</span></span><br><span class="line">        String dbIndex = queryOne(<span class="string">&quot;select currentIndex from flume_meta where source_tab=&#x27;&quot;</span> + table + <span class="string">&quot;&#x27;&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (dbIndex != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> Integer.parseInt(dbIndex);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//如果没有数据，则说明是第一次查询或者数据表中还没有存入数据，返回最初传入的值</span></span><br><span class="line">        <span class="keyword">return</span> startFrom;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//查询一条数据的执行语句(当前id)</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">queryOne</span><span class="params">(String sql)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        ResultSet result = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ps = conn.prepareStatement(sql);</span><br><span class="line">            result = ps.executeQuery();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (result.next()) &#123;</span><br><span class="line">                <span class="keyword">return</span> result.getString(<span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//关闭相关资源</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ps.close();</span><br><span class="line">            conn.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getCurrentIndex</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> currentIndex;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setCurrentIndex</span><span class="params">(<span class="keyword">int</span> newValue)</span> </span>&#123;</span><br><span class="line">        currentIndex = newValue;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getRunQueryDelay</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> runQueryDelay;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">String <span class="title">getQuery</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> query;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">String <span class="title">getConnectionURL</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> connectionURL;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">isCustomQuerySet</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (customQuery != <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">Context <span class="title">getContext</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> context;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getConnectionUserName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> connectionUserName;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getConnectionPassword</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> connectionPassword;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">String <span class="title">getDefaultCharsetResultSet</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> defaultCharsetResultSet;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="5-4-4-MySQLSource"><a href="#5-4-4-MySQLSource" class="headerlink" title="5.4.4 MySQLSource"></a>5.4.4 MySQLSource</h3><p>代码实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing.source;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.EventDeliveryException;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.PollableSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.event.SimpleEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.source.AbstractSource;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.ParseException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SQLSource</span> <span class="keyword">extends</span> <span class="title">AbstractSource</span> <span class="keyword">implements</span> <span class="title">Configurable</span>, <span class="title">PollableSource</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//打印日志</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(SQLSource.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义sqlHelper</span></span><br><span class="line">    <span class="keyword">private</span> SQLSourceHelper sqlSourceHelper;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getBackOffSleepIncrement</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getMaxBackOffSleepInterval</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//初始化</span></span><br><span class="line">            sqlSourceHelper = <span class="keyword">new</span> SQLSourceHelper(context);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ParseException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//查询数据表</span></span><br><span class="line">            List&lt;List&lt;Object&gt;&gt; result = sqlSourceHelper.executeQuery();</span><br><span class="line"></span><br><span class="line">            <span class="comment">//存放event的集合</span></span><br><span class="line">            List&lt;Event&gt; events = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">            <span class="comment">//存放event头集合</span></span><br><span class="line">            HashMap&lt;String, String&gt; header = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">            <span class="comment">//如果有返回数据，则将数据封装为event</span></span><br><span class="line">            <span class="keyword">if</span> (!result.isEmpty()) &#123;</span><br><span class="line"></span><br><span class="line">                List&lt;String&gt; allRows = sqlSourceHelper.getAllRows(result);</span><br><span class="line"></span><br><span class="line">                Event event = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> (String row : allRows) &#123;</span><br><span class="line">                    event = <span class="keyword">new</span> SimpleEvent();</span><br><span class="line">                    event.setBody(row.getBytes());</span><br><span class="line">                    event.setHeaders(header);</span><br><span class="line">                    events.add(event);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">//将event写入channel</span></span><br><span class="line">                <span class="keyword">this</span>.getChannelProcessor().processEventBatch(events);</span><br><span class="line"></span><br><span class="line">                <span class="comment">//更新数据表中的offset信息</span></span><br><span class="line">                sqlSourceHelper.updateOffset2DB(result.size());</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//等待时长</span></span><br><span class="line">            Thread.sleep(sqlSourceHelper.getRunQueryDelay());</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> Status.READY;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            LOG.error(<span class="string">&quot;Error procesing row&quot;</span>, e);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> Status.BACKOFF;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">stop</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        LOG.info(<span class="string">&quot;Stopping sql source &#123;&#125; ...&quot;</span>, getName());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//关闭资源</span></span><br><span class="line">            sqlSourceHelper.close();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="keyword">super</span>.stop();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="5-5-测试"><a href="#5-5-测试" class="headerlink" title="5.5 测试"></a>5.5 测试</h2><h3 id="5-5-1-Jar包准备"><a href="#5-5-1-Jar包准备" class="headerlink" title="5.5.1 Jar包准备"></a>5.5.1 Jar包准备</h3><p><strong>1)</strong> <strong>将MySql</strong>驱动包放入Flume的lib目录下</p><blockquote><p>[xing@hadoop102 flume]$ cp \</p><p>/opt/sorfware/mysql-libs/mysql-connector-java-5.1.27/mysql-connector-java-5.1.27-bin.jar \</p><p>/opt/module/flume/lib/</p></blockquote><p><strong>2)</strong> <strong>打包项目并将Jar</strong>包放入Flume的lib目录下</p><h3 id="5-5-2-配置文件准备"><a href="#5-5-2-配置文件准备" class="headerlink" title="5.5.2 配置文件准备"></a>5.5.2 配置文件准备</h3><p>1）创建配置文件并打开</p><blockquote><p>[xing@hadoop102 job]$ touch mysql.conf</p></blockquote><blockquote><p>[xing@hadoop102 job]$ vim mysql.conf </p></blockquote><p>2）添加如下内容</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line"><span class="meta">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="meta">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="meta">a1.sources.r1.type</span> = <span class="string">com.xing.source.SQLSource  </span></span><br><span class="line"><span class="meta">a1.sources.r1.connection.url</span> = <span class="string">jdbc:mysql://192.168.9.102:3306/mysqlsource</span></span><br><span class="line"><span class="meta">a1.sources.r1.connection.user</span> = <span class="string">root  </span></span><br><span class="line"><span class="meta">a1.sources.r1.connection.password</span> = <span class="string">000000  </span></span><br><span class="line"><span class="meta">a1.sources.r1.table</span> = <span class="string">student  </span></span><br><span class="line"><span class="meta">a1.sources.r1.columns.to.select</span> = <span class="string">*  </span></span><br><span class="line"><span class="comment">#a1.sources.r1.incremental.column.name = id  </span></span><br><span class="line"><span class="comment">#a1.sources.r1.incremental.value = 0 </span></span><br><span class="line"><span class="meta">a1.sources.r1.run.query.delay</span>=<span class="string">5000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="meta">a1.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the channel</span></span><br><span class="line"><span class="meta">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="meta">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="5-5-3-MySql表准备"><a href="#5-5-3-MySql表准备" class="headerlink" title="5.5.3 MySql表准备"></a>5.5.3 MySql表准备</h3><p><strong>1)</strong> <strong>创建MySqlSource数据库</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> mysqlsource；</span><br></pre></td></tr></table></figure><p><strong>2)</strong> <strong>在MySqlSource</strong>数据库下创建数据表Student<strong>和元数据表Flume_meta</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`student`</span> (</span><br><span class="line"><span class="string">`id`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT,</span><br><span class="line"><span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">);</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`flume_meta`</span> (</span><br><span class="line"><span class="string">`source_tab`</span> <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`currentIndex`</span> <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`source_tab`</span>)</span><br><span class="line">);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>3)</strong>   <strong>向数据表中添加数据</strong></p><blockquote><p>1 zhangsan</p><p>2 lisi</p><p>3 wangwu</p><p>4 zhaoliu</p></blockquote><h3 id="5-5-4-测试并查看结果"><a href="#5-5-4-测试并查看结果" class="headerlink" title="5.5.4   测试并查看结果"></a>5.5.4   测试并查看结果</h3><p><strong>1)</strong>   <strong>任务执行</strong></p><blockquote><p>[xing@hadoop102 flume]$ bin/flume-ng agent –conf conf/ –name a1 \</p><p>–conf-file job/mysql.conf -Dflume.root.logger=INFO,console</p></blockquote><p><strong>2)</strong>   <strong>结果展示</strong></p><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922160522.png" alt="image-20200922160521789"></p><h1 id="第6章-知识扩展"><a href="#第6章-知识扩展" class="headerlink" title="第6章 知识扩展"></a>第6章 知识扩展</h1><h2 id="6-1-常见正则表达式语法"><a href="#6-1-常见正则表达式语法" class="headerlink" title="6.1 常见正则表达式语法"></a>6.1 常见正则表达式语法</h2><table><thead><tr><th>元字符</th><th>描述</th></tr></thead><tbody><tr><td>^</td><td>匹配输入字符串的开始位置。如果设置了RegExp对象的Multiline属性，^也匹配“\n”或“\r”之后的位置。</td></tr><tr><td>$</td><td>匹配输入字符串的结束位置。如果设置了RegExp对象的Multiline属性，$也匹配“\n”或“\r”之前的位置。</td></tr><tr><td>*</td><td>匹配前面的子表达式任意次。例如，zo<em>能匹配“z”，“zo”以及“zoo”。</em>等价于{0,}。</td></tr><tr><td>+</td><td>匹配前面的子表达式一次或多次(大于等于1次）。例如，“zo+”能匹配“zo”以及“zoo”，但不能匹配“z”。+等价于{1,}。</td></tr><tr><td>[a-z]</td><td>字符范围。匹配指定范围内的任意字符。例如，“[a-z]”可以匹配“a”到“z”范围内的任意小写字母字符。  注意:只有连字符在字符组内部时,并且出现在两个字符之间时,才能表示字符的范围; 如果出字符组的开头,则只能表示连字符本身.</td></tr></tbody></table><h2 id="6-2-练习"><a href="#6-2-练习" class="headerlink" title="6.2 练习"></a>6.2 练习</h2><p>案例需求：</p><p>1）flume-1监控hive.log日志，flume-1的数据传送给flume-2，flume-2将数据追加到本地文件，同时将数据传输到flume-3。</p><p>2）flume-4监控本地另一个自己创建的文件any.txt，并将数据传送给flume-3。</p><p>3）flume-3将汇总数据写入到HDFS。</p><p>请先画出结构图，再开始编写任务脚本。</p><h1 id="第7章-Flume面试题"><a href="#第7章-Flume面试题" class="headerlink" title="第7章 Flume面试题"></a>第7章 Flume面试题</h1><h2 id="7-1-你是如何实现Flume数据传输的监控的"><a href="#7-1-你是如何实现Flume数据传输的监控的" class="headerlink" title="7.1 你是如何实现Flume数据传输的监控的"></a>7.1 你是如何实现Flume数据传输的监控的</h2><p>使用第三方框架Ganglia实时监控Flume。</p><h2 id="7-2-Flume的Source，Sink，Channel的作用？你们Source是什么类型？"><a href="#7-2-Flume的Source，Sink，Channel的作用？你们Source是什么类型？" class="headerlink" title="7.2 Flume的Source，Sink，Channel的作用？你们Source是什么类型？"></a>7.2 Flume的Source，Sink，Channel的作用？你们Source是什么类型？</h2><p>​    1、作用</p><p>​        （1）Source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy</p><p>​        （2）Channel组件对采集到的数据进行缓存，可以存放在Memory或File中。</p><p>​        （3）Sink组件是用于把数据发送到目的地的组件，目的地包括Hdfs、Logger、avro、thrift、ipc、file、Hbase、solr、自定义。</p><p>2、我公司采用的Source类型为：</p><p>（1）监控后台日志：exec</p><p>（2）监控后台产生日志的端口：netcat  Exec spooldir</p><h2 id="7-3-Flume的Channel-Selectors"><a href="#7-3-Flume的Channel-Selectors" class="headerlink" title="7.3 Flume的Channel Selectors"></a>7.3 Flume的Channel Selectors</h2><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922160822.png" alt="image-20200922160816157"></p><h2 id="7-4-Flume参数调优"><a href="#7-4-Flume参数调优" class="headerlink" title="7.4 Flume参数调优"></a>7.4 Flume参数调优</h2><ol><li>Source</li></ol><p>增加Source个（使用Tair Dir Source时可增加FileGroups个数）可以增大Source的读取数据的能力。例如：当某一个目录产生的文件过多时需要将这个文件目录拆分成多个文件目录，同时配置好多个Source 以保证Source有足够的能力获取到新产生的数据。</p><p>batchSize参数决定Source一次批量运输到Channel的event条数，适当调大这个参数可以提高Source搬运Event到Channel时的性能。</p><ol start="2"><li>Channel </li></ol><p>type 选择memory时Channel的性能最好，但是如果Flume进程意外挂掉可能会丢失数据。type选择file时Channel的容错性更好，但是性能上会比memory channel差。</p><p>使用file Channel时dataDirs配置多个不同盘下的目录可以提高性能。</p><p>Capacity 参数决定Channel可容纳最大的event条数。transactionCapacity 参数决定每次Source往channel里面写的最大event条数和每次Sink从channel里面读的最大event条数。transactionCapacity需要大于Source和Sink的batchSize参数。</p><ol start="3"><li>Sink </li></ol><p>增加Sink的个数可以增加Sink消费event的能力。Sink也不是越多越好够用就行，过多的Sink会占用系统资源，造成系统资源不必要的浪费。</p><p>batchSize参数决定Sink一次批量从Channel读取的event条数，适当调大这个参数可以提高Sink从Channel搬出event的性能。</p><h2 id="7-5-Flume的事务机制"><a href="#7-5-Flume的事务机制" class="headerlink" title="7.5 Flume的事务机制"></a>7.5 Flume的事务机制</h2><p>Flume的事务机制（类似数据库的事务机制）：Flume使用两个独立的事务分别负责从Soucrce到Channel，以及从Channel到Sink的事件传递。比如spooling directory source 为文件的每一行创建一个事件，一旦事务中所有的事件全部传递到Channel且提交成功，那么Soucrce就将该文件标记为完成。同理，事务以类似的方式处理从Channel到Sink的传递过程，如果因为某种原因使得事件无法记录，那么事务将会回滚。且所有的事件都会保持到Channel中，等待重新传递。</p><h2 id="7-6-Flume采集数据会丢失吗"><a href="#7-6-Flume采集数据会丢失吗" class="headerlink" title="7.6 Flume采集数据会丢失吗?"></a>7.6 Flume采集数据会丢失吗?</h2><p>不会，Channel存储可以存储在File中，数据传输自身有事务。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;第1章-概述&quot;&gt;&lt;a href=&quot;#第1章-概述&quot; class=&quot;headerlink&quot; title=&quot;第1章 概述&quot;&gt;&lt;/a&gt;第1章 概述&lt;/h1&gt;&lt;h2 id=&quot;1-1-Flume定义&quot;&gt;&lt;a href=&quot;#1-1-Flume定义&quot; class=&quot;headerlink&quot; title=&quot;1.1 Flume定义&quot;&gt;&lt;/a&gt;1.1 Flume定义&lt;/h2&gt;&lt;p&gt;Flume是Cloudera提供的一个高可用的，高可靠的，==分布式的海量日志采集、聚合和传输的系统==。Flume基于流式架构，灵活简单。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;为什么选Flume&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://gitee.com/curryfor369/picgo/raw/master/img/20200919101429.png&quot; alt=&quot;image-20200918160104738&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Flume" scheme="http://iscurry.com/categories/Flume/"/>
    
    
    <category term="Flume" scheme="http://iscurry.com/tags/Flume/"/>
    
    <category term="Detail" scheme="http://iscurry.com/tags/Detail/"/>
    
  </entry>
  
  <entry>
    <title>Hbase(2)详细</title>
    <link href="http://iscurry.com/2020/02/22/Hbase(2)/"/>
    <id>http://iscurry.com/2020/02/22/Hbase(2)/</id>
    <published>2020-02-22T10:49:04.000Z</published>
    <updated>2020-09-25T02:18:25.346Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第1章-HBase简介"><a href="#第1章-HBase简介" class="headerlink" title="第1章 HBase简介"></a>第1章 HBase简介</h1><h2 id="1-1-HBase定义"><a href="#1-1-HBase定义" class="headerlink" title="1.1 HBase定义"></a>1.1 HBase定义</h2><p>HBase是一种分布式、可扩展、支持海量数据存储的NoSQL数据库。</p><a id="more"></a><h2 id="1-2-HBase数据模型"><a href="#1-2-HBase数据模型" class="headerlink" title="1.2 HBase数据模型"></a>1.2 HBase数据模型</h2><p>逻辑上，HBase的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。但从HBase的底层物理存储结构（K-V）来看，HBase更像是一个<font color="#FF0000"><strong>multi-dimensional map</strong></font>。</p><h3 id="1-2-1-HBase逻辑结构"><a href="#1-2-1-HBase逻辑结构" class="headerlink" title="1.2.1 HBase逻辑结构"></a>1.2.1 HBase逻辑结构</h3><p>   <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922185304.png" alt="image-20200922185303332">                            </p><h3 id="1-2-2-HBase物理存储结构"><a href="#1-2-2-HBase物理存储结构" class="headerlink" title="1.2.2 HBase物理存储结构"></a>1.2.2 HBase物理存储结构</h3><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922185539.png" alt="image-20200922185538721"></p><h3 id="1-2-3-数据模型"><a href="#1-2-3-数据模型" class="headerlink" title="1.2.3 数据模型"></a>1.2.3 数据模型</h3><p><strong>1）Name Space</strong></p><p>命名空间，类似于关系型数据库的DatabBase概念，每个命名空间下有多个表。HBase有两个自带的命名空间，分别是<strong>hbase</strong>和<strong>default</strong>，<strong>hbase</strong>中存放的是HBase<strong>内置的表</strong>，<strong>default</strong>表是用户<strong>默认使用的命名空间</strong>。</p><p><strong>2）Region</strong></p><p>类似于关系型数据库的表概念。不同的是，HBase定义表时只需要声明<strong>列族</strong>即可，不需要声明具体的列。这意味着，往HBase写入数据时，字段可以<strong>动态、按需</strong>指定。因此，和关系型数据库相比，HBase能够轻松应对字段变更的场景。</p><p><strong>3）Row</strong></p><p>HBase表中的每行数据都由一个<strong>RowKey</strong>和多个<strong>Column</strong>（列）组成，数据是按照RowKey的<strong>字典顺序存储</strong>的，并且查询数据时只能根据RowKey进行检索，所以RowKey的设计十分重要。</p><p><strong>4）Column</strong></p><p>HBase中的每个列都由<strong>Column Family(列族)**和</strong>Column Qualifier（列限定符）**进行限定</p><p>例如 info：name，info：age。</p><p>建表时，只需指明列族，而列限定符无需预先定义。</p><p><strong>5）Time Stamp</strong></p><p><strong>用于标识数据的不同版本（version），每条数据写入时，如果不指定时间戳，系统会自动为其加上该字段，其值为写入HBase的时间</strong>。</p><p><strong>6）Cell</strong></p><p>由{rowkey, column Family：column Qualifier, time Stamp} 唯一确定的单元。cell中的数据是没有类型的，全部是字节码形式存贮。</p><h2 id="1-3-HBase基本架构"><a href="#1-3-HBase基本架构" class="headerlink" title="1.3 HBase基本架构"></a>1.3 HBase基本架构</h2><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922190334.png" alt="image-20200922190333206"></p><p><strong>架构角色：</strong></p><p><strong>1）Region Server</strong></p><p>Region Server为 Region的管理者，其实现类为<font color="#FF0000"><strong>HRegionServer</strong> </font>，主要作用如下:</p><p>对于数据的操作：get, put, delete；</p><p>对于Region的操作：splitRegion、compactRegion。</p><p><strong>2）Master</strong></p><p>Master是所有Region Server的管理者，其实现类为HMaster，主要作用如下：</p><p>  对于表的操作：create, delete, alter</p><p>对于RegionServer的操作：分配regions到每个RegionServer，监控每个RegionServer的状态，负载均衡和故障转移。</p><p><strong>3）Zookeeper</strong></p><p>HBase通过Zookeeper来做Master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作。</p><p><strong>4）HDFS</strong></p><p>HDFS为HBase提供最终的底层数据存储服务，同时为HBase提供高可用的支持。</p><h1 id="第2章-HBase快速入门"><a href="#第2章-HBase快速入门" class="headerlink" title="第2章 HBase快速入门"></a>第2章 HBase快速入门</h1><h2 id="2-1-HBase安装部署"><a href="#2-1-HBase安装部署" class="headerlink" title="2.1 HBase安装部署"></a>2.1 HBase安装部署</h2><h3 id="2-1-1-Zookeeper正常部署"><a href="#2-1-1-Zookeeper正常部署" class="headerlink" title="2.1.1 Zookeeper正常部署"></a>2.1.1 Zookeeper正常部署</h3><p>首先保证Zookeeper集群的正常部署，并启动之：</p><blockquote><p>[xing@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh start</p><p>[xing@hadoop103 zookeeper-3.4.10]$ bin/zkServer.sh start</p><p>[xing@hadoop104 zookeeper-3.4.10]$ bin/zkServer.sh start</p></blockquote><h3 id="2-1-2-Hadoop正常部署"><a href="#2-1-2-Hadoop正常部署" class="headerlink" title="2.1.2 Hadoop正常部署"></a>2.1.2 Hadoop正常部署</h3><p>Hadoop集群的正常部署并启动：</p><blockquote><p>[xing@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</p><p>[xing@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</p></blockquote><h3 id="2-1-3-HBase的解压"><a href="#2-1-3-HBase的解压" class="headerlink" title="2.1.3 HBase的解压"></a>2.1.3 HBase的解压</h3><p>解压Hbase到指定目录：</p><blockquote><p>[xing@hadoop102 software]$ tar -zxvf hbase-1.3.1-bin.tar.gz -C /opt/module</p></blockquote><h3 id="2-1-4-HBase的配置文件"><a href="#2-1-4-HBase的配置文件" class="headerlink" title="2.1.4 HBase的配置文件"></a>2.1.4 HBase的配置文件</h3><p>修改HBase对应的配置文件。</p><p>1）hbase-env.sh修改内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.6.0_144</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HBASE_MANAGES_ZK=<span class="literal">false</span></span><br></pre></td></tr></table></figure><p>2）hbase-site.xml修改内容：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000/HBase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="comment">&lt;!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>16000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102,hadoop103,hadoop104<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/zookeeper-3.4.10/zkData<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>3）regionservers：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop102</span><br><span class="line"></span><br><span class="line">hadoop103</span><br><span class="line"></span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><p>4）软连接hadoop配置文件到HBase：</p><blockquote><p>[xing@hadoop102 module]$ ln -s /opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml /opt/module/hbase/conf/core-site.xml</p><p>[xing@hadoop102 module]$ ln -s /opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml /opt/module/hbase/conf/hdfs-site.xml</p></blockquote><h3 id="2-1-5-HBase远程发送到其他集群"><a href="#2-1-5-HBase远程发送到其他集群" class="headerlink" title="2.1.5 HBase远程发送到其他集群"></a>2.1.5 HBase远程发送到其他集群</h3><blockquote><p>[xing@hadoop102 module]$ xsync hbase/</p></blockquote><h3 id="2-1-6-HBase服务的启动"><a href="#2-1-6-HBase服务的启动" class="headerlink" title="2.1.6 HBase服务的启动"></a>2.1.6 HBase服务的启动</h3><p><strong>1．启动方式</strong></p><blockquote><p>[xing@hadoop102 hbase]$ bin/hbase-daemon.sh start master</p></blockquote><blockquote><p>[xing@hadoop102 hbase]$ bin/hbase-daemon.sh start regionserver</p></blockquote><p><strong>提示：如果集群之间的节点时间不同步，会导致regionserver无法启动</strong>，抛出ClockOutOfSyncException异常。</p><p>修复提示：</p><p>a、同步时间服务</p><p>请参看帮助文档：《Hadoop入门》</p><p>b、属性：hbase.master.maxclockskew设置更大的值</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.maxclockskew<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>180000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Time difference of regionserver from master<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>2．启动方式2</strong></p><blockquote><p>[xing@hadoop102 hbase]$ bin/start-hbase.sh</p></blockquote><p>对应的停止服务：</p><blockquote><p>[xing@hadoop102 hbase]$ bin/stop-hbase.sh</p></blockquote><h3 id="2-1-7-查看HBase页面"><a href="#2-1-7-查看HBase页面" class="headerlink" title="2.1.7 查看HBase页面"></a>2.1.7 查看HBase页面</h3><p>启动成功后，可以通过“host:port”的方式来访问HBase管理页面，例如：</p><p><a href="http://linux01:16010/">http://hadoop102:16010</a> </p><h2 id="2-2-HBase-Shell操作"><a href="#2-2-HBase-Shell操作" class="headerlink" title="2.2 HBase Shell操作"></a>2.2 HBase Shell操作</h2><h3 id="2-2-1-基本操作"><a href="#2-2-1-基本操作" class="headerlink" title="2.2.1 基本操作"></a>2.2.1 基本操作</h3><p><strong>1．进入HBase客户端命令行</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xing@hadoop102 hbase]$ bin/hbase shell</span><br></pre></td></tr></table></figure><p><strong>2．查看帮助命令</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; help</span><br></pre></td></tr></table></figure><p><strong>3．查看当前数据库中有哪些表</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):002:0&gt; list</span><br></pre></td></tr></table></figure><h3 id="2-2-2-表的操作"><a href="#2-2-2-表的操作" class="headerlink" title="2.2.2 表的操作"></a>2.2.2 表的操作</h3><p><strong>1．创建表</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):002:0&gt; create &#x27;student&#x27;,&#x27;info&#x27;</span><br></pre></td></tr></table></figure><p><strong>2．插入数据到表</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):003:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:sex&#x27;,&#x27;male&#x27;</span><br><span class="line"></span><br><span class="line">hbase(main):004:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:age&#x27;,&#x27;18&#x27;</span><br><span class="line"></span><br><span class="line">hbase(main):005:0&gt; put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:name&#x27;,&#x27;Janna&#x27;</span><br><span class="line"></span><br><span class="line">hbase(main):006:0&gt; put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:sex&#x27;,&#x27;female&#x27;</span><br><span class="line"></span><br><span class="line">hbase(main):007:0&gt; put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:age&#x27;,&#x27;20&#x27;</span><br></pre></td></tr></table></figure><p><strong>3．扫描查看表数据</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):008:0&gt; scan &#x27;student&#x27;</span><br><span class="line"></span><br><span class="line">hbase(main):009:0&gt; scan &#x27;student&#x27;,&#123;STARTROW =&gt; &#x27;1001&#x27;, STOPROW =&gt; &#x27;1001&#x27;&#125;</span><br><span class="line"></span><br><span class="line">hbase(main):010:0&gt; scan &#x27;student&#x27;,&#123;STARTROW =&gt; &#x27;1001&#x27;&#125;</span><br></pre></td></tr></table></figure><p><strong>4．查看表结构</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):011:0&gt; describe ‘student’</span><br></pre></td></tr></table></figure><p><strong>5．更新指定字段的数据</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):012:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:name&#x27;,&#x27;Nick&#x27;</span><br><span class="line"></span><br><span class="line">hbase(main):013:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:age&#x27;,&#x27;100&#x27;</span><br></pre></td></tr></table></figure><p><strong>6．查看“指定行”或“指定列族:列”的数据</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):014:0&gt; get &#x27;student&#x27;,&#x27;1001&#x27;</span><br><span class="line"></span><br><span class="line">hbase(main):015:0&gt; get &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:name&#x27;</span><br></pre></td></tr></table></figure><p><strong>7．统计表数据行数</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):021:0&gt; count &#x27;student&#x27;</span><br></pre></td></tr></table></figure><p><strong>8．删除数据</strong></p><p>删除某rowkey的全部数据：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):016:0&gt; deleteall &#x27;student&#x27;,&#x27;1001&#x27;</span><br></pre></td></tr></table></figure><p>删除某rowkey的某一列数据：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):017:0&gt; delete &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:sex&#x27;</span><br></pre></td></tr></table></figure><p><strong>9．清空表数据</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):018:0&gt; truncate &#x27;student&#x27;</span><br></pre></td></tr></table></figure><p><strong>提示：清空表的操作顺序为先disable，然后再truncate。</strong></p><p><strong>10．删除表</strong></p><p><strong>首先需要先让该表为disable状态：</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):019:0&gt; disable &#x27;student&#x27;</span><br></pre></td></tr></table></figure><p>然后才能drop这个表：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):020:0&gt; drop &#x27;student&#x27;</span><br></pre></td></tr></table></figure><p><strong>提示</strong>：如果直接drop表，会报错：ERROR: Table student is enabled. Disable it first.</p><p><strong>11．变更表信息</strong></p><p>将info列族中的数据存放3个版本：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):022:0&gt; alter &#x27;student&#x27;,&#123;NAME=&gt;&#x27;info&#x27;,VERSIONS=&gt;3&#125;</span><br><span class="line"></span><br><span class="line">hbase(main):022:0&gt; get &#x27;student&#x27;,&#x27;1001&#x27;,&#123;COLUMN=&gt;&#x27;info:name&#x27;,VERSIONS=&gt;3&#125;</span><br></pre></td></tr></table></figure><h1 id="第3章-HBase进阶"><a href="#第3章-HBase进阶" class="headerlink" title="第3章 HBase进阶"></a>第3章 HBase进阶</h1><h2 id="3-1-架构原理"><a href="#3-1-架构原理" class="headerlink" title="3.1 架构原理"></a>3.1 架构原理</h2><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922205425.png" alt="image-20200922205423446"></p><p><strong>1）StoreFile</strong></p><p>保存实际数据的物理文件，StoreFile以HFile的形式存储在HDFS上。每个Store会有一个或多个StoreFile（HFile），数据在每个StoreFile中都是有序的。</p><p><strong>2）MemStore</strong></p><p>写缓存，由于HFile中的数据要求是有序的，所以数据是先存储在MemStore中，排好序后，等到达刷写时机才会刷写到HFile，每次刷写都会形成一个新的HFile。</p><p><strong>3）WAL</strong></p><p>由于数据要经MemStore排序后才能刷写到HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入MemStore中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。</p><h2 id="3-2-写流程"><a href="#3-2-写流程" class="headerlink" title="3.2 写流程"></a>3.2 写流程</h2><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922205941.png" alt="image-20200922205940814"></p><p>写流程：</p><p>1）Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。</p><p>2）访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。</p><p>3）与目标Region Server进行通讯；</p><p>4）将数据顺序写入（追加）到WAL；</p><p>5）将数据写入对应的MemStore，数据会在MemStore进行排序；</p><p>6）向客户端发送ack；</p><p>7）等达到MemStore的刷写时机后，将数据刷写到HFile。</p><h2 id="3-3-MemStore-Flush"><a href="#3-3-MemStore-Flush" class="headerlink" title="3.3 MemStore Flush"></a>3.3 MemStore Flush</h2><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922210839.png" alt="image-20200922210838411"></p><p><strong>MemStore刷写时机：</strong></p><p>1.当某个memstroe的大小达到了<font color="#FF0000"> <strong>hbase.hregion.memstore.flush.size（默认值128M）</strong></font>，<strong>其所在region的所有memstore都会刷写</strong>。</p><p>当memstore的大小达到了</p><p><font color="#FF0000"><strong>hbase.hregion.memstore.flush.size（默认值128M）</strong> </font></p><p><font color="#FF0000"> * <strong>hbase.hregion.memstore.block.multiplier（默认值4）</strong></font></p><p>时，会<strong>阻止继续</strong>往该memstore写数据。</p><p>2.当region server中memstore的总大小达到</p><p><font color="#FF0000"><strong>java_heapsize</strong> </font></p><p><strong><font color="#FF0000"> *hbase.regionserver.global.memstore.size（默认值0.4）</font></strong></p><p>*<em><font color="#FF0000"> \</em>hbase.regionserver.global.memstore.size.lower.limit（默认值0.95）</font>**，</p><p>region会按照其所有memstore的大小顺序（由大到小）依次进行刷写。直到region server中所有memstore的总大小减小到上述值以下。</p><p>当region server中memstore的总大小达到</p><p><strong><font color="#FF0000"> java_heapsize*hbase.regionserver.global.memstore.size（默认值0.4）</font></strong></p><p>时，会<strong>阻止继续</strong>往所有的memstore写数据。</p><p>3.到达自动刷写的时间，也会触发memstore flush。自动刷新的时间间隔由该属性进行配置<font color="#FF0000"> <strong>hbase.regionserver.optionalcacheflushinterval（默认1小时）</strong></font>。</p><p>4.当WAL文件的数量超过<font color="#FF0000"> <strong>hbase.regionserver.max.logs</strong></font>，region会按照时间顺序依次进行刷写，直到WAL文件数量减小到<font color="#FF0000">**hbase.regionserver.max.log **</font>以下（<font color="#FF0000"> 该属性名已经废弃，现无需手动设置，最大值为32</font>）。</p><h2 id="3-4-读流程"><a href="#3-4-读流程" class="headerlink" title="3.4 读流程"></a>3.4 读流程</h2><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922211553.png" alt="image-20200922211552180"></p><p><strong>读流程</strong></p><p>1）Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。</p><p>2）访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。</p><p>3）与目标Region Server进行通讯；</p><p>4）分别在Block Cache（读缓存），MemStore和Store File（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put/Delete）。</p><p>5） 将从文件中查询到的数据块（Block，HFile数据存储单元，默认大小为64KB）缓存到Block Cache。</p><p>6）将合并后的最终结果返回给客户端。</p><h2 id="3-5-StoreFile-Compaction"><a href="#3-5-StoreFile-Compaction" class="headerlink" title="3.5 StoreFile Compaction"></a>3.5 StoreFile Compaction</h2><p>由于memstore每次刷写都会生成一个新的HFile，且同一个字段的不同版本（timestamp）和不同类型（Put/Delete）有可能会分布在不同的HFile中，因此查询时需要遍历所有的HFile。为了减少HFile的个数，以及清理掉过期和删除的数据，会进行StoreFile Compaction。</p><p>Compaction分为两种，分别是<font color="#FF0000">Minor Compaction和Major Compaction </font>。Minor Compaction会将临近的若干个较小的HFile合并成一个较大的HFile，<font color="#FF0000">但不会清理过期和删除的数据 </font>。Major Compaction会将一个Store下的所有的HFile合并成一个大HFile，并且<font color="#FF0000">会清理掉过期和删除的数据。 </font></p><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922211851.png" alt="image-20200922211849101"></p><h2 id="3-6-Region-Split"><a href="#3-6-Region-Split" class="headerlink" title="3.6 Region Split"></a>3.6 Region Split</h2><p>默认情况下，每个Table起初只有一个Region，随着数据的不断写入，Region会自动进行拆分。刚拆分时，两个子Region都位于当前的Region Server，但处于负载均衡的考虑，HMaster有可能会将某个Region转移给其他的Region Server。</p><p><strong>Region Split时机：</strong></p><p>1.当1个region中的某个Store下所有StoreFile的总大小超过<strong>hbase.hregion.max.filesize</strong>，该Region就会进行拆分（0.94版本之前）。</p><p>2.当1个region中的某个Store下所有StoreFile的总大小超过<font color="#FF0000"> <strong>Min(R^2* “hbase.hregion.memstore.flush.size”,hbase.hregion.max.filesize”)</strong></font>，该Region就会进行拆分，其中R为当前Region Server中属于该Table的Region的个数（0.94版本之后）。</p><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922212154.png" alt="image-20200922212153296"></p><h1 id="第4章-HBase-API"><a href="#第4章-HBase-API" class="headerlink" title="第4章 HBase API"></a>第4章 HBase API</h1><h2 id="4-1-环境准备"><a href="#4-1-环境准备" class="headerlink" title="4.1 环境准备"></a>4.1 环境准备</h2><p>新建项目后在pom.xml中添加依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="4-2-HBaseAPI"><a href="#4-2-HBaseAPI" class="headerlink" title="4.2 HBaseAPI"></a>4.2 HBaseAPI</h2><h3 id="4-2-1-获取Configuration对象"><a href="#4-2-1-获取Configuration对象" class="headerlink" title="4.2.1 获取Configuration对象"></a>4.2.1 获取Configuration对象</h3>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> Configuration conf;</span><br><span class="line"><span class="keyword">static</span>&#123;</span><br><span class="line"><span class="comment">//使用HBaseConfiguration的单例方法实例化</span></span><br><span class="line">conf = HBaseConfiguration.create();</span><br><span class="line">    conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;192.166.9.102&quot;</span>);</span><br><span class="line">    conf.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, <span class="string">&quot;2181&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-2-2-判断表是否存在"><a href="#4-2-2-判断表是否存在" class="headerlink" title="4.2.2 判断表是否存在"></a>4.2.2 判断表是否存在</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isTableExist</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> MasterNotRunningException,</span></span><br><span class="line"><span class="function"> ZooKeeperConnectionException, IOException</span>&#123;</span><br><span class="line"><span class="comment">//在HBase中管理、访问表需要先创建HBaseAdmin对象</span></span><br><span class="line">    <span class="comment">//Connection connection = ConnectionFactory.createConnection(conf);</span></span><br><span class="line">    <span class="comment">//HBaseAdmin admin = (HBaseAdmin) connection.getAdmin();</span></span><br><span class="line">HBaseAdmin admin = <span class="keyword">new</span> HBaseAdmin(conf);</span><br><span class="line"><span class="keyword">return</span> admin.tableExists(tableName);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-2-3-创建表"><a href="#4-2-3-创建表" class="headerlink" title="4.2.3 创建表"></a>4.2.3 创建表</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createTable</span><span class="params">(String tableName, String... columnFamily)</span> <span class="keyword">throws</span></span></span><br><span class="line"><span class="function"> MasterNotRunningException, ZooKeeperConnectionException, IOException</span>&#123;</span><br><span class="line">HBaseAdmin admin = <span class="keyword">new</span> HBaseAdmin(conf);</span><br><span class="line"><span class="comment">//判断表是否存在</span></span><br><span class="line"><span class="keyword">if</span>(isTableExist(tableName))&#123;</span><br><span class="line">System.out.println(<span class="string">&quot;表&quot;</span> + tableName + <span class="string">&quot;已存在&quot;</span>);</span><br><span class="line"><span class="comment">//System.exit(0);</span></span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line"><span class="comment">//创建表属性对象,表名需要转字节</span></span><br><span class="line">HTableDescriptor descriptor = <span class="keyword">new</span> HTableDescriptor(TableName.valueOf(tableName));</span><br><span class="line"><span class="comment">//创建多个列族</span></span><br><span class="line"><span class="keyword">for</span>(String cf : columnFamily)&#123;</span><br><span class="line">descriptor.addFamily(<span class="keyword">new</span> HColumnDescriptor(cf));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//根据对表的配置，创建表</span></span><br><span class="line">admin.createTable(descriptor);</span><br><span class="line">System.out.println(<span class="string">&quot;表&quot;</span> + tableName + <span class="string">&quot;创建成功！&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="4-2-4-删除表"><a href="#4-2-4-删除表" class="headerlink" title="4.2.4 删除表"></a>4.2.4 删除表</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">dropTable</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> MasterNotRunningException,</span></span><br><span class="line"><span class="function"> ZooKeeperConnectionException, IOException</span>&#123;</span><br><span class="line">HBaseAdmin admin = <span class="keyword">new</span> HBaseAdmin(conf);</span><br><span class="line"><span class="keyword">if</span>(isTableExist(tableName))&#123;</span><br><span class="line">admin.disableTable(tableName);</span><br><span class="line">admin.deleteTable(tableName);</span><br><span class="line">System.out.println(<span class="string">&quot;表&quot;</span> + tableName + <span class="string">&quot;删除成功！&quot;</span>);</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">System.out.println(<span class="string">&quot;表&quot;</span> + tableName + <span class="string">&quot;不存在！&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-2-5-向表中插入数据"><a href="#4-2-5-向表中插入数据" class="headerlink" title="4.2.5 向表中插入数据"></a>4.2.5 向表中插入数据</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">addRowData</span><span class="params">(String tableName, String rowKey,  String columnFamily, String column, String value)</span>  <span class="keyword">throws</span> IOException</span>&#123;    </span><br><span class="line">      <span class="comment">//创建HTable对象    </span></span><br><span class="line">      HTable hTable = <span class="keyword">new</span>  HTable(conf, tableName); </span><br><span class="line">      </span><br><span class="line">      <span class="comment">//向表中插入数据    </span></span><br><span class="line">      Put put = <span class="keyword">new</span>  Put(Bytes.toBytes(rowKey)); </span><br><span class="line">      </span><br><span class="line">      <span class="comment">//向Put对象中组装数据    </span></span><br><span class="line">      put.add(Bytes.toBytes(columnFamily),  Bytes.toBytes(column), Bytes.toBytes(value));   hTable.put(put);    </span><br><span class="line">      hTable.close();    </span><br><span class="line">      System.out.println(<span class="string">&quot;插入数据成功&quot;</span>);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure><h3 id="4-2-6-删除多行数据"><a href="#4-2-6-删除多行数据" class="headerlink" title="4.2.6 删除多行数据"></a>4.2.6 删除多行数据</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">deleteMultiRow</span><span class="params">(String tableName, String... rows)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">HTable hTable = <span class="keyword">new</span> HTable(conf, tableName);</span><br><span class="line">List&lt;Delete&gt; deleteList = <span class="keyword">new</span> ArrayList&lt;Delete&gt;();</span><br><span class="line"><span class="keyword">for</span>(String row : rows)&#123;</span><br><span class="line">Delete delete = <span class="keyword">new</span> Delete(Bytes.toBytes(row));</span><br><span class="line">deleteList.add(delete);</span><br><span class="line">&#125;</span><br><span class="line">hTable.delete(deleteList);</span><br><span class="line">hTable.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="4-2-7-获取所有数据"><a href="#4-2-7-获取所有数据" class="headerlink" title="4.2.7 获取所有数据"></a>4.2.7 获取所有数据</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getAllRows</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">HTable hTable = <span class="keyword">new</span> HTable(conf, tableName);</span><br><span class="line"><span class="comment">//得到用于扫描region的对象</span></span><br><span class="line">Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line"><span class="comment">//使用HTable得到resultcanner实现类的对象</span></span><br><span class="line">ResultScanner resultScanner = hTable.getScanner(scan);</span><br><span class="line"><span class="keyword">for</span>(Result result : resultScanner)&#123;</span><br><span class="line">Cell[] cells = result.rawCells();</span><br><span class="line"><span class="keyword">for</span>(Cell cell : cells)&#123;</span><br><span class="line"><span class="comment">//得到rowkey</span></span><br><span class="line">System.out.println(<span class="string">&quot;行键:&quot;</span> + Bytes.toString(CellUtil.cloneRow(cell)));</span><br><span class="line"><span class="comment">//得到列族</span></span><br><span class="line">System.out.println(<span class="string">&quot;列族&quot;</span> + Bytes.toString(CellUtil.cloneFamily(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;列:&quot;</span> + Bytes.toString(CellUtil.cloneQualifier(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;值:&quot;</span> + Bytes.toString(CellUtil.cloneValue(cell)));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="4-2-8-获取某一行数据"><a href="#4-2-8-获取某一行数据" class="headerlink" title="4.2.8 获取某一行数据"></a>4.2.8 获取某一行数据</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getRow</span><span class="params">(String tableName, String rowKey)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">HTable table = <span class="keyword">new</span> HTable(conf, tableName);</span><br><span class="line">Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span><br><span class="line"><span class="comment">//get.setMaxVersions();显示所有版本</span></span><br><span class="line">    <span class="comment">//get.setTimeStamp();显示指定时间戳的版本</span></span><br><span class="line">Result result = table.get(get);</span><br><span class="line"><span class="keyword">for</span>(Cell cell : result.rawCells())&#123;</span><br><span class="line">System.out.println(<span class="string">&quot;行键:&quot;</span> + Bytes.toString(result.getRow()));</span><br><span class="line">System.out.println(<span class="string">&quot;列族&quot;</span> + Bytes.toString(CellUtil.cloneFamily(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;列:&quot;</span> + Bytes.toString(CellUtil.cloneQualifier(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;值:&quot;</span> + Bytes.toString(CellUtil.cloneValue(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;时间戳:&quot;</span> + cell.getTimestamp());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="4-2-9-获取某一行指定“列族-列”的数据"><a href="#4-2-9-获取某一行指定“列族-列”的数据" class="headerlink" title="4.2.9 获取某一行指定“列族:列”的数据"></a>4.2.9 获取某一行指定“列族:列”的数据</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getRowQualifier</span><span class="params">(String tableName, String rowKey, String family, String</span></span></span><br><span class="line"><span class="function"><span class="params"> qualifier)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">HTable table = <span class="keyword">new</span> HTable(conf, tableName);</span><br><span class="line">Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span><br><span class="line">get.addColumn(Bytes.toBytes(family), Bytes.toBytes(qualifier));</span><br><span class="line">Result result = table.get(get);</span><br><span class="line"><span class="keyword">for</span>(Cell cell : result.rawCells())&#123;</span><br><span class="line">System.out.println(<span class="string">&quot;行键:&quot;</span> + Bytes.toString(result.getRow()));</span><br><span class="line">System.out.println(<span class="string">&quot;列族&quot;</span> + Bytes.toString(CellUtil.cloneFamily(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;列:&quot;</span> + Bytes.toString(CellUtil.cloneQualifier(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;值:&quot;</span> + Bytes.toString(CellUtil.cloneValue(cell)));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="4-3-MapReduce"><a href="#4-3-MapReduce" class="headerlink" title="4.3 MapReduce"></a>4.3 MapReduce</h2><p>通过HBase的相关JavaAPI，我们可以实现伴随HBase操作的MapReduce过程，比如使用MapReduce将数据从本地文件系统导入到HBase的表中，比如我们从HBase中读取一些原始数据后使用MapReduce做数据分析。</p><h3 id="4-3-1-官方HBase-MapReduce"><a href="#4-3-1-官方HBase-MapReduce" class="headerlink" title="4.3.1 官方HBase-MapReduce"></a>4.3.1 官方HBase-MapReduce</h3><p>1．查看HBase的MapReduce任务的执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hbase mapredcp</span><br></pre></td></tr></table></figure><p>2．环境变量的导入</p><p>（1）执行环境变量的导入（临时生效，在命令行执行下述操作）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> HBASE_HOME=/opt/module/hbase</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">export</span> HADOOP_CLASSPATH=`<span class="variable">$&#123;HBASE_HOME&#125;</span>/bin/hbase mapredcp`</span><br></pre></td></tr></table></figure><p>（2）永久生效：在/etc/profile配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HBASE_HOME=/opt/module/hbase</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure><p>并在hadoop-env.sh中配置：（注意：在for循环之后配）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_CLASSPATH=<span class="variable">$HADOOP_CLASSPATH</span>:/opt/module/hbase/lib/*</span><br></pre></td></tr></table></figure><p>3．运行官方的MapReduce任务</p><p>– 案例一：统计Student表中有多少行数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ /opt/module/hadoop-2.7.2/bin/yarn jar lib/hbase-server-1.3.1.jar rowcounter student</span><br></pre></td></tr></table></figure><p>– 案例二：使用MapReduce将本地数据导入到HBase</p><p>1）在本地创建一个tsv格式的文件：fruit.tsv</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1001  Apple Red  1002  Pear  Yellow  1003  Pineapple Yellow  </span><br></pre></td></tr></table></figure><p>2）创建Hbase表</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hbase(main):001:0&gt; create &#x27;fruit&#x27;,&#x27;info&#x27;</span><br></pre></td></tr></table></figure><p>3）在HDFS中创建input_fruit文件夹并上传fruit.tsv文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ /opt/module/hadoop-2.7.2/bin/hdfs dfs -mkdir /input_fruit/</span><br><span class="line"></span><br><span class="line">$ /opt/module/hadoop-2.7.2/bin/hdfs dfs -put fruit.tsv /input_fruit/</span><br></pre></td></tr></table></figure><p>4）执行MapReduce到HBase的fruit表中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ /opt/module/hadoop-2.7.2/bin/yarn jar lib/hbase-server-1.3.1.jar importtsv \</span><br><span class="line">-Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit \</span><br><span class="line">hdfs://hadoop102:9000/input_fruit</span><br></pre></td></tr></table></figure><p>5）使用scan命令查看导入后的结果</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hbase(main):001:0&gt; scan ‘fruit’</span><br></pre></td></tr></table></figure><h3 id="4-3-2-自定义HBase-MapReduce1"><a href="#4-3-2-自定义HBase-MapReduce1" class="headerlink" title="4.3.2 自定义HBase-MapReduce1"></a>4.3.2 自定义HBase-MapReduce1</h3><p>目标：将fruit表中的一部分数据，通过MR迁入到fruit_mr表中。</p><p>分步实现：</p><p>1．构建ReadFruitMapper类，用于读取fruit表中的数据</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.Cell;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.CellUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Result;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableMapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReadFruitMapper</span> <span class="keyword">extends</span> <span class="title">TableMapper</span>&lt;<span class="title">ImmutableBytesWritable</span>, <span class="title">Put</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(ImmutableBytesWritable key, Result value, Context context)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//将fruit的name和color提取出来，相当于将每一行数据读取出来放入到Put对象中。</span></span><br><span class="line">Put put = <span class="keyword">new</span> Put(key.get());</span><br><span class="line"><span class="comment">//遍历添加column行</span></span><br><span class="line"><span class="keyword">for</span>(Cell cell: value.rawCells())&#123;</span><br><span class="line"><span class="comment">//添加/克隆列族:info</span></span><br><span class="line"><span class="keyword">if</span>(<span class="string">&quot;info&quot;</span>.equals(Bytes.toString(CellUtil.cloneFamily(cell))))&#123;</span><br><span class="line"><span class="comment">//添加/克隆列：name</span></span><br><span class="line"><span class="keyword">if</span>(<span class="string">&quot;name&quot;</span>.equals(Bytes.toString(CellUtil.cloneQualifier(cell))))&#123;</span><br><span class="line"><span class="comment">//将该列cell加入到put对象中</span></span><br><span class="line">put.add(cell);</span><br><span class="line"><span class="comment">//添加/克隆列:color</span></span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;color&quot;</span>.equals(Bytes.toString(CellUtil.cloneQualifier(cell))))&#123;</span><br><span class="line"><span class="comment">//向该列cell加入到put对象中</span></span><br><span class="line">put.add(cell);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//将从fruit读取到的每行数据写入到context中作为map的输出</span></span><br><span class="line">context.write(key, put);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>2． 构建WriteFruitMRReducer类，用于将读取到的fruit表中的数据写入到fruit_mr表中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing.Hbase_mr;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.Hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.Hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.Hbase.mapreduce.TableReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WriteFruitMRReducer</span> <span class="keyword">extends</span> <span class="title">TableReducer</span>&lt;<span class="title">ImmutableBytesWritable</span>, <span class="title">Put</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//读出来的每一行数据写入到fruit_mr表中</span></span><br><span class="line"><span class="keyword">for</span>(Put put: values)&#123;</span><br><span class="line">context.write(NullWritable.get(), put);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>3．构建Fruit2FruitMRRunner extends Configured implements Tool用于组装运行Job任务</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//组装Job</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">//得到Configuration</span></span><br><span class="line">    Configuration conf = <span class="keyword">this</span>.getConf();</span><br><span class="line">    <span class="comment">//创建Job任务</span></span><br><span class="line">    Job job = Job.getInstance(conf, <span class="keyword">this</span>.getClass().getSimpleName());</span><br><span class="line">    job.setJarByClass(Fruit2FruitMRRunner.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//配置Job</span></span><br><span class="line">    Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">    scan.setCacheBlocks(<span class="keyword">false</span>);</span><br><span class="line">    scan.setCaching(<span class="number">500</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置Mapper，注意导入的是mapreduce包下的，不是mapred包下的，后者是老版本</span></span><br><span class="line">    TableMapReduceUtil.initTableMapperJob(</span><br><span class="line">        <span class="string">&quot;fruit&quot;</span>, <span class="comment">//数据源的表名</span></span><br><span class="line">        scan, <span class="comment">//scan扫描控制器</span></span><br><span class="line">        ReadFruitMapper.class,//设置Mapper类</span><br><span class="line">        ImmutableBytesWritable.class,//设置Mapper输出key类型</span><br><span class="line">        Put.class,//设置Mapper输出value值类型</span><br><span class="line">        job<span class="comment">//设置给哪个JOB</span></span><br><span class="line">    );</span><br><span class="line">    <span class="comment">//设置Reducer</span></span><br><span class="line">    TableMapReduceUtil.initTableReducerJob(<span class="string">&quot;fruit_mr&quot;</span>, WriteFruitMRReducer.class, job);</span><br><span class="line">    <span class="comment">//设置Reduce数量，最少1个</span></span><br><span class="line">    job.setNumReduceTasks(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> isSuccess = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">if</span>(!isSuccess)&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;Job running with error&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> isSuccess ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>4．主函数中调用运行该Job任务</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String[] args )</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    Configuration conf = HbaseConfiguration.create();</span><br><span class="line">    <span class="keyword">int</span> status = ToolRunner.run(conf, <span class="keyword">new</span> Fruit2FruitMRRunner(), args);</span><br><span class="line">    System.exit(status);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>5．打包运行任务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ /opt/module/hadoop-2.7.2/bin/yarn jar ~/softwares/jars/Hbase-0.0.1-SNAPSHOT.jar com.z.Hbase.mr1.Fruit2FruitMRRunner</span><br></pre></td></tr></table></figure><p><strong>提示</strong>：运行任务前，如果待数据导入的表不存在，则需要提前创建。</p><p><strong>提示</strong>：maven打包命令：-P local clean package或-P dev clean package install（将第三方jar包一同打包，需要插件：maven-shade-plugin）</p><h3 id="4-3-3-自定义Hbase-MapReduce2"><a href="#4-3-3-自定义Hbase-MapReduce2" class="headerlink" title="4.3.3 自定义Hbase-MapReduce2"></a>4.3.3 自定义Hbase-MapReduce2</h3><p>目标：实现将HDFS中的数据写入到Hbase表中。</p><p>分步实现：</p><p>1．构建ReadFruitFromHDFSMapper于读取HDFS中的文件数据</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReadFruitFromHDFSMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">ImmutableBytesWritable</span>, <span class="title">Put</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//从HDFS中读取的数据</span></span><br><span class="line">String lineValue = value.toString();</span><br><span class="line"><span class="comment">//读取出来的每行数据使用\t进行分割，存于String数组</span></span><br><span class="line">String[] values = lineValue.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//根据数据中值的含义取值</span></span><br><span class="line">String rowKey = values[<span class="number">0</span>];</span><br><span class="line">String name = values[<span class="number">1</span>];</span><br><span class="line">String color = values[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化rowKey</span></span><br><span class="line">ImmutableBytesWritable rowKeyWritable = <span class="keyword">new</span> ImmutableBytesWritable(Bytes.toBytes(rowKey));</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化put对象</span></span><br><span class="line">Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowKey));</span><br><span class="line"></span><br><span class="line"><span class="comment">//参数分别:列族、列、值  </span></span><br><span class="line">        put.add(Bytes.toBytes(<span class="string">&quot;info&quot;</span>), Bytes.toBytes(<span class="string">&quot;name&quot;</span>),  Bytes.toBytes(name)); </span><br><span class="line">        put.add(Bytes.toBytes(<span class="string">&quot;info&quot;</span>), Bytes.toBytes(<span class="string">&quot;color&quot;</span>),  Bytes.toBytes(color)); </span><br><span class="line">        </span><br><span class="line">        context.write(rowKeyWritable, put);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>2．构建WriteFruitMRFromTxtReducer类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.z.Hbase.mr2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WriteFruitMRFromTxtReducer</span> <span class="keyword">extends</span> <span class="title">TableReducer</span>&lt;<span class="title">ImmutableBytesWritable</span>, <span class="title">Put</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//读出来的每一行数据写入到fruit_hdfs表中</span></span><br><span class="line"><span class="keyword">for</span>(Put put: values)&#123;</span><br><span class="line">context.write(NullWritable.get(), put);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>3．创建Txt2FruitRunner组装Job</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">//得到Configuration</span></span><br><span class="line">    Configuration conf = <span class="keyword">this</span>.getConf();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建Job任务</span></span><br><span class="line">    Job job = Job.getInstance(conf, <span class="keyword">this</span>.getClass().getSimpleName());</span><br><span class="line">    job.setJarByClass(Txt2FruitRunner.class);</span><br><span class="line">    Path inPath = <span class="keyword">new</span> Path(<span class="string">&quot;hdfs://hadoop102:9000/input_fruit/fruit.tsv&quot;</span>);</span><br><span class="line">    FileInputFormat.addInputPath(job, inPath);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置Mapper</span></span><br><span class="line">    job.setMapperClass(ReadFruitFromHDFSMapper.class);</span><br><span class="line">    job.setMapOutputKeyClass(ImmutableBytesWritable.class);</span><br><span class="line">    job.setMapOutputValueClass(Put.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置Reducer</span></span><br><span class="line">    TableMapReduceUtil.initTableReducerJob(<span class="string">&quot;fruit_mr&quot;</span>, WriteFruitMRFromTxtReducer.class, job);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置Reduce数量，最少1个</span></span><br><span class="line">    job.setNumReduceTasks(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> isSuccess = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">if</span>(!isSuccess)&#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;Job running with error&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> isSuccess ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>4．调用执行Job</p> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Configuration conf = HBaseConfiguration.create();</span><br><span class="line">    <span class="keyword">int</span> status = ToolRunner.run(conf, <span class="keyword">new</span> Txt2FruitRunner(), args);</span><br><span class="line">    System.exit(status);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>5．打包运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ /opt/module/hadoop-2.7.2/bin/yarn jar hbase-0.0.1-SNAPSHOT.jar \</span><br><span class="line">com.xing.hbase.mr2.Txt2FruitRunner</span><br></pre></td></tr></table></figure><p><strong>提示</strong>：运行任务前，如果待数据导入的表不存在，则需要提前创建之。</p><p><strong>提示</strong>：maven打包命令：-P local clean package或-P dev clean package install（将第三方jar包一同打包，需要插件：maven-shade-plugin）</p><h2 id="4-4-与Hive的集成"><a href="#4-4-与Hive的集成" class="headerlink" title="4.4 与Hive的集成"></a>4.4 与Hive的集成</h2><h3 id="4-4-1-HBase与Hive的对比"><a href="#4-4-1-HBase与Hive的对比" class="headerlink" title="4.4.1 HBase与Hive的对比"></a>4.4.1 HBase与Hive的对比</h3><p><strong>1．Hive</strong></p><p>(1) 数据仓库</p><p>Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。</p><p>(2) 用于数据分析、清洗</p><p>Hive适用于离线的数据分析和清洗，延迟较高。</p><p>(3) 基于HDFS、MapReduce</p><p>Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。</p><p><strong>2．HBase</strong></p><p>(1) 数据库</p><p>是一种<font color="#FF0000"> <strong>面向列族存储</strong></font>的非关系型数据库。</p><p>(2) 用于存储结构化和非结构化的数据</p><p>适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。</p><p>(3) 基于HDFS</p><p>数据持久化存储的体现形式是HFile，存放于DataNode中，被ResionServer以region的形式进行管理。</p><p>(4) 延迟较低，接入在线业务使用</p><p>面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。</p><h3 id="4-4-2-HBase与Hive集成使用"><a href="#4-4-2-HBase与Hive集成使用" class="headerlink" title="4.4.2 HBase与Hive集成使用"></a>4.4.2 HBase与Hive集成使用</h3><p><font color="#FF0000"> <strong>尖叫提示</strong></font>：HBase与Hive的集成在最新的两个版本中无法兼容。所以，我们只能含着泪勇敢的重新编译：hive-hbase-handler-1.2.2.jar！！</p><p><strong>环境准备</strong></p><p>因为我们后续可能会在操作Hive的同时对HBase也会产生影响，所以Hive需要持有操作HBase的Jar，那么接下来拷贝Hive所依赖的Jar包（或者使用软连接的形式）。</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HBASE_HOME=/opt/module/hbase  <span class="built_in">export</span> HIVE_HOME=/opt/module/hive     ln -s <span class="variable">$HBASE_HOME</span>/lib/hbase-common-1.3.1.jar <span class="variable">$HIVE_HOME</span>/lib/hbase-common-1.3.1.jar  ln -s <span class="variable">$HBASE_HOME</span>/lib/hbase-server-1.3.1.jar <span class="variable">$HIVE_HOME</span>/lib/hbase-server-1.3.1.jar  ln -s <span class="variable">$HBASE_HOME</span>/lib/hbase-client-1.3.1.jar <span class="variable">$HIVE_HOME</span>/lib/hbase-client-1.3.1.jar  ln -s <span class="variable">$HBASE_HOME</span>/lib/hbase-protocol-1.3.1.jar <span class="variable">$HIVE_HOME</span>/lib/hbase-protocol-1.3.1.jar  ln -s <span class="variable">$HBASE_HOME</span>/lib/hbase-it-1.3.1.jar <span class="variable">$HIVE_HOME</span>/lib/hbase-it-1.3.1.jar  ln -s <span class="variable">$HBASE_HOME</span>/lib/htrace-core-3.1.0-incubating.jar  <span class="variable">$HIVE_HOME</span>/lib/htrace-core-3.1.0-incubating.jar  ln -s <span class="variable">$HBASE_HOME</span>/lib/hbase-hadoop2-compat-1.3.1.jar  <span class="variable">$HIVE_HOME</span>/lib/hbase-hadoop2-compat-1.3.1.jar  ln -s <span class="variable">$HBASE_HOME</span>/lib/hbase-hadoop-compat-1.3.1.jar  <span class="variable">$HIVE_HOME</span>/lib/hbase-hadoop-compat-1.3.1.jar  </span><br></pre></td></tr></table></figure><p>同时在<font color="#FF0000"> hive-site.xml</font>中修改zookeeper的属性，如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102,hadoop103,hadoop104<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The list of ZooKeeper servers to talk to. This is only needed for read/write locks.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.client.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The port of ZooKeeper servers to talk to. This is only needed for read/write locks.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>1．案例一</p><p><strong>目标：</strong>建立Hive表，关联HBase表，插入数据到Hive表的同时能够影响HBase表。</p><p><strong>分步实现：</strong></p><p>(1) 在Hive中创建表同时关联HBase</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hive_hbase_emp_table(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">job <span class="keyword">string</span>,</span><br><span class="line">mgr <span class="built_in">int</span>,</span><br><span class="line">hiredate <span class="keyword">string</span>,</span><br><span class="line">sal <span class="keyword">double</span>,</span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">&#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;</span></span><br><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES (<span class="string">&quot;hbase.columns.mapping&quot;</span> = <span class="string">&quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;</span>)</span><br><span class="line">TBLPROPERTIES (<span class="string">&quot;hbase.table.name&quot;</span> = <span class="string">&quot;hbase_emp_table&quot;</span>);</span><br></pre></td></tr></table></figure><p><strong>提示</strong>：完成之后，可以分别进入Hive和HBase查看，都生成了对应的表</p><p>(2) 在Hive中创建临时中间表，用于load文件中的数据</p><p><strong>提示</strong>：不能将数据直接load进Hive所关联HBase的那张表中</p> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">job <span class="keyword">string</span>,</span><br><span class="line">mgr <span class="built_in">int</span>,</span><br><span class="line">hiredate <span class="keyword">string</span>,</span><br><span class="line">sal <span class="keyword">double</span>,</span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>(3) 向Hive中间表中load数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data local inpath &#x27;/home/admin/softwares/data/emp.txt&#x27;  into table emp;  </span><br></pre></td></tr></table></figure><p>(4) 通过insert命令将中间表中的数据导入到Hive关联Hbase的那张表中</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; insert into table hive_hbase_emp_table select * from  emp; </span><br></pre></td></tr></table></figure><p>(5) 查看Hive以及关联的HBase表中是否已经成功的同步插入了数据</p><p><strong>Hive:</strong> </p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from hive_hbase_emp_table;  </span><br></pre></td></tr></table></figure><p><strong>HBase：</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hbase&gt; scan ‘hbase_emp_table’  </span><br></pre></td></tr></table></figure><p>2．案例二</p><p><strong>目标：</strong>在HBase中已经存储了某一张表hbase_emp_table，然后在Hive中创建一个外部表来关联HBase中的hbase_emp_table这张表，使之可以借助Hive来分析HBase这张表中的数据。</p><p><strong>注：</strong>该案例2紧跟案例1的脚步，所以完成此案例前，请先完成案例1。</p><p><strong>分步实现：</strong></p><p>(1) 在Hive中创建外部表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> relevance_hbase_emp(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">job <span class="keyword">string</span>,</span><br><span class="line">mgr <span class="built_in">int</span>,</span><br><span class="line">hiredate <span class="keyword">string</span>,</span><br><span class="line">sal <span class="keyword">double</span>,</span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">BY</span> </span><br><span class="line"><span class="string">&#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;</span></span><br><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES (<span class="string">&quot;hbase.columns.mapping&quot;</span> = </span><br><span class="line"><span class="string">&quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;</span>) </span><br><span class="line">TBLPROPERTIES (<span class="string">&quot;hbase.table.name&quot;</span> = <span class="string">&quot;hbase_emp_table&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>(2) 关联后就可以使用Hive函数进行一些分析操作了</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from relevance_hbase_emp;  </span><br></pre></td></tr></table></figure><h1 id="第5章-HBase优化"><a href="#第5章-HBase优化" class="headerlink" title="第5章 HBase优化"></a>第5章 HBase优化</h1><h2 id="5-1-高可用"><a href="#5-1-高可用" class="headerlink" title="5.1 高可用"></a>5.1 高可用</h2><p>在HBase中HMaster负责监控HRegionServer的生命周期，均衡RegionServer的负载，如果HMaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对HMaster的高可用配置。</p><p>1．关闭HBase集群（如果没有开启则跳过此步）</p><blockquote><p>[xing@hadoop102 hbase]$ bin/stop-hbase.sh</p></blockquote><p>2．在conf目录下创建backup-masters文件</p><blockquote><p>[xing@hadoop102 hbase]$ touch conf/backup-masters</p></blockquote><p>3．在backup-masters文件中配置高可用HMaster节点</p><blockquote><p>[xing@hadoop102 hbase]$ echo hadoop103 &gt; conf/backup-masters</p></blockquote><p>4．将整个conf目录scp到其他节点</p><blockquote><p>[xing@hadoop102 hbase]$ scp -r conf/ hadoop103:/opt/module/hbase/</p><p>[xing@hadoop102 hbase]$ scp -r conf/ hadoop104:/opt/module/hbase/</p></blockquote><p>5．打开页面测试查看</p><p><a href="http://linux01:16010/">http://hadooo102:16010</a> </p><h2 id="5-2-预分区"><a href="#5-2-预分区" class="headerlink" title="5.2 预分区"></a>5.2 预分区</h2><p>每一个region维护着StartRow与EndRow，如果加入的数据符合某个Region维护的RowKey范围，则该数据交给这个Region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高HBase性能。</p><p><strong>1．手动设定预分区</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hbase&gt; create &#x27;staff1&#x27;,&#x27;info&#x27;,&#x27;partition1&#x27;,SPLITS =&gt; [&#x27;1000&#x27;,&#x27;2000&#x27;,&#x27;3000&#x27;,&#x27;4000&#x27;]</span><br></pre></td></tr></table></figure><p><strong>2．生成16进制序列预分区</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="string">&#x27;staff2&#x27;</span>,<span class="string">&#x27;info&#x27;</span>,<span class="string">&#x27;partition2&#x27;</span>,&#123;NUMREGIONS =&gt; <span class="number">15</span>, SPLITALGO =&gt; <span class="string">&#x27;HexStringSplit&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><p><strong>3．按照文件中设置的规则预分区</strong></p><p>创建splits.txt文件内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aaaa  bbbb  cccc  dddd  </span><br></pre></td></tr></table></figure><p>然后执行：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="string">&#x27;staff3&#x27;</span>,<span class="string">&#x27;partition3&#x27;</span>,SPLITS_FILE =&gt; <span class="string">&#x27;splits.txt&#x27;</span></span><br></pre></td></tr></table></figure><p><strong>4．使用JavaAPI创建预分区</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//自定义算法，产生一系列hash散列值存储在二维数组中</span></span><br><span class="line"><span class="keyword">byte</span>[][] splitKeys = 某个散列值函数</span><br><span class="line"><span class="comment">//创建HbaseAdmin实例</span></span><br><span class="line">HBaseAdmin hAdmin = <span class="keyword">new</span> HBaseAdmin(HbaseConfiguration.create());</span><br><span class="line"><span class="comment">//创建HTableDescriptor实例</span></span><br><span class="line">HTableDescriptor tableDesc = <span class="keyword">new</span> HTableDescriptor(tableName);</span><br><span class="line"><span class="comment">//通过HTableDescriptor实例和散列值二维数组创建带有预分区的Hbase表</span></span><br><span class="line">hAdmin.createTable(tableDesc, splitKeys);</span><br></pre></td></tr></table></figure><h2 id="5-3-RowKey设计"><a href="#5-3-RowKey设计" class="headerlink" title="5.3 RowKey设计"></a>5.3 RowKey设计</h2><p>一条数据的唯一标识就是RowKey，那么这条数据存储于哪个分区，取决于RowKey处于哪个一个预分区的区间内，设计RowKey的主要目的 ，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜。接下来我们就谈一谈RowKey常用的设计方案。</p><p><strong>1．生成随机数、hash、散列值</strong></p><p>  比如：  </p><blockquote><p>原本rowKey为1001的，SHA1后变成：dd01903921ea24941c26a48f2cec24e0bb0e8cc7  </p><p>原本rowKey为3001的，SHA1后变成：49042c54de64a1e9bf0b33e00245660ef92dc7bd  </p><p>原本rowKey为5001的，SHA1后变成：7b61dec07e02c188790670af43e717f0f46e8913 </p></blockquote><p>在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的rowKey来Hash后作为每个分区的临界值。  </p><p><strong>2．字符串反转</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">20170524000001转成10000042507102  </span><br><span class="line">20170524000002转成20000042507102  </span><br></pre></td></tr></table></figure><p>这样也可以在一定程度上散列逐步put进来的数据。</p><p><strong>3．字符串拼接</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">20170524000001_a12e  </span><br><span class="line">20170524000001_93i7 </span><br></pre></td></tr></table></figure><h2 id="5-4-内存优化"><a href="#5-4-内存优化" class="headerlink" title="5.4 内存优化"></a>5.4 内存优化</h2><p>HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是<font color="#FF0000"> <strong>不建议分配非常大的堆内存</strong></font>，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。</p><h2 id="5-5-基础优化"><a href="#5-5-基础优化" class="headerlink" title="5.5 基础优化"></a>5.5 基础优化</h2><p>1．允许在HDFS的文件中追加内容</p><p>hdfs-site.xml、hbase-site.xml</p><blockquote><p>属性：dfs.support.append  解释：开启HDFS追加同步，可以优秀的配合HBase的数据同步和持久化。默认值为true。  </p></blockquote><p>2．优化DataNode允许的最大文件打开数</p><p>hdfs-site.xml</p><blockquote><p>属性：dfs.datanode.max.transfer.threads  解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096  </p></blockquote><p>3．优化延迟高的数据操作的等待时间</p><p>hdfs-site.xml</p><blockquote><p>属性：dfs.image.transfer.timeout  解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值（默认60000毫秒），以确保socket不会被timeout掉。  </p></blockquote><p>4．优化数据的写入效率</p><p>mapred-site.xml</p><blockquote><p>属性：  mapreduce.map.output.compress  mapreduce.map.output.compress.codec  解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec或者其他压缩方式。  </p></blockquote><p>5．设置RPC监听数量</p><p>hbase-site.xml</p><blockquote><p>属性：Hbase.regionserver.handler.count  解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。  </p></blockquote><p>6．优化HStore文件大小</p><p>hbase-site.xml</p><blockquote><p>属性：hbase.hregion.max.filesize  解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile。  </p></blockquote><p>7．优化HBase客户端缓存</p><p>hbase-site.xml</p><blockquote><p>属性：hbase.client.write.buffer  解释：用于指定Hbase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。  </p></blockquote><p>8．指定scan.next扫描HBase所获取的行数</p><p>hbase-site.xml</p><blockquote><p>属性：hbase.client.scanner.caching  解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。  </p></blockquote><p>9．flush、compact、split机制</p><p>当MemStore达到阈值，将Memstore中的数据Flush进Storefile；compact机制则是把flush出来的小文件合并成大的Storefile文件。split则是当Region达到阈值，会把过大的Region一分为二。</p><p><strong>涉及属性：</strong></p><p>即：128M就是Memstore的默认阈值</p><blockquote><p>hbase.hregion.memstore.flush.size：134217728  </p></blockquote><p>即：这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM。</p><blockquote><p>hbase.regionserver.global.memstore.upperLimit：0.4  </p><p>hbase.regionserver.global.memstore.lowerLimit：0.38 </p></blockquote><p>即：当MemStore使用内存总量达到hbase.regionserver.global.memstore.upperLimit指定值时，将会有多个MemStores flush到文件中，MemStore flush 顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于lowerLimit</p><h1 id="第6章-HBase实战之谷粒微博"><a href="#第6章-HBase实战之谷粒微博" class="headerlink" title="第6章 HBase实战之谷粒微博"></a>第6章 HBase实战之谷粒微博</h1><h2 id="6-1-需求分析"><a href="#6-1-需求分析" class="headerlink" title="6.1 需求分析"></a>6.1 需求分析</h2><ol><li><p>微博内容的浏览，数据库表设计</p></li><li><p>用户社交体现：关注用户，取关用户</p></li><li><p>拉取关注的人的微博内容</p></li></ol><h2 id="6-2-代码实现"><a href="#6-2-代码实现" class="headerlink" title="6.2 代码实现"></a>6.2 代码实现</h2><h3 id="6-2-1-代码设计总览："><a href="#6-2-1-代码设计总览：" class="headerlink" title="6.2.1 代码设计总览："></a>6.2.1 代码设计总览：</h3><ol><li><p>创建命名空间以及表名的定义</p></li><li><p>创建微博内容表</p></li><li><p>创建用户关系表</p></li><li><p>创建用户微博内容接收邮件表</p></li><li><p>发布微博内容</p></li><li><p>添加关注用户</p></li><li><p>移除（取关）用户</p></li><li><p>获取关注的人的微博内容</p></li><li><p>测试</p></li></ol><h3 id="6-2-2-创建命名空间以及表名的定义"><a href="#6-2-2-创建命名空间以及表名的定义" class="headerlink" title="6.2.2 创建命名空间以及表名的定义"></a>6.2.2 创建命名空间以及表名的定义</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取配置conf</span></span><br><span class="line"><span class="keyword">private</span> Configuration conf = HbaseConfiguration.create();</span><br><span class="line"></span><br><span class="line"><span class="comment">//微博内容表的表名</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] TABLE_CONTENT = Bytes.toBytes(<span class="string">&quot;weibo:content&quot;</span>);</span><br><span class="line"><span class="comment">//用户关系表的表名</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] TABLE_RELATIONS = Bytes.toBytes(<span class="string">&quot;weibo:relations&quot;</span>);</span><br><span class="line"><span class="comment">//微博收件箱表的表名</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] TABLE_RECEIVE_CONTENT_EMAIL = Bytes.toBytes(<span class="string">&quot;weibo:receive_content_email&quot;</span>);</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initNamespace</span><span class="params">()</span></span>&#123;</span><br><span class="line">HbaseAdmin admin = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">admin = <span class="keyword">new</span> HbaseAdmin(conf);</span><br><span class="line"><span class="comment">//命名空间类似于关系型数据库中的schema，可以想象成文件夹</span></span><br><span class="line">NamespaceDescriptor weibo = NamespaceDescriptor</span><br><span class="line">.create(<span class="string">&quot;weibo&quot;</span>)</span><br><span class="line">.addConfiguration(<span class="string">&quot;creator&quot;</span>, <span class="string">&quot;Jinji&quot;</span>)</span><br><span class="line">.addConfiguration(<span class="string">&quot;create_time&quot;</span>, System.currentTimeMillis() + <span class="string">&quot;&quot;</span>)</span><br><span class="line">.build();</span><br><span class="line">admin.createNamespace(weibo);</span><br><span class="line">&#125; <span class="keyword">catch</span> (MasterNotRunningException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125; <span class="keyword">catch</span> (ZooKeeperConnectionException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;<span class="keyword">finally</span>&#123;</span><br><span class="line"><span class="keyword">if</span>(<span class="keyword">null</span> != admin)&#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">admin.close();</span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="6-2-3-创建微博内容表"><a href="#6-2-3-创建微博内容表" class="headerlink" title="6.2.3 创建微博内容表"></a>6.2.3 创建微博内容表</h3><p><strong>表结构：</strong></p><table><thead><tr><th><strong>方法名</strong></th><th>creatTableeContent</th></tr></thead><tbody><tr><td>Table Name</td><td>weibo:content</td></tr><tr><td>RowKey</td><td>用户ID_时间戳</td></tr><tr><td>ColumnFamily</td><td>info</td></tr><tr><td>ColumnLabel</td><td>标题,内容,图片</td></tr><tr><td>Version</td><td>1个版本</td></tr></tbody></table><p><strong>代码：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建微博内容表</span></span><br><span class="line"><span class="comment"> * Table Name:weibo:content</span></span><br><span class="line"><span class="comment"> * RowKey:用户ID_时间戳</span></span><br><span class="line"><span class="comment"> * ColumnFamily:info</span></span><br><span class="line"><span class="comment"> * ColumnLabel:标题内容图片URL</span></span><br><span class="line"><span class="comment"> * Version:1个版本</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createTableContent</span><span class="params">()</span></span>&#123;</span><br><span class="line">HbaseAdmin admin = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">admin = <span class="keyword">new</span> HbaseAdmin(conf);</span><br><span class="line"><span class="comment">//创建表表述</span></span><br><span class="line">HTableDescriptor content = <span class="keyword">new</span> HTableDescriptor(TableName.valueOf(TABLE_CONTENT));</span><br><span class="line"><span class="comment">//创建列族描述</span></span><br><span class="line">HColumnDescriptor info = <span class="keyword">new</span> HColumnDescriptor(Bytes.toBytes(<span class="string">&quot;info&quot;</span>));</span><br><span class="line"><span class="comment">//设置块缓存</span></span><br><span class="line">info.setBlockCacheEnabled(<span class="keyword">true</span>);</span><br><span class="line"><span class="comment">//设置块缓存大小</span></span><br><span class="line">info.setBlocksize(<span class="number">2097152</span>);</span><br><span class="line"><span class="comment">//设置压缩方式</span></span><br><span class="line"><span class="comment">//info.setCompressionType(Algorithm.SNAPPY);</span></span><br><span class="line"><span class="comment">//设置版本确界</span></span><br><span class="line">info.setMaxVersions(<span class="number">1</span>);</span><br><span class="line">info.setMinVersions(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">content.addFamily(info);</span><br><span class="line">admin.createTable(content);</span><br><span class="line"></span><br><span class="line">&#125; <span class="keyword">catch</span> (MasterNotRunningException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125; <span class="keyword">catch</span> (ZooKeeperConnectionException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;<span class="keyword">finally</span>&#123;</span><br><span class="line"><span class="keyword">if</span>(<span class="keyword">null</span> != admin)&#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">admin.close();</span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="6-2-4-创建用户关系表"><a href="#6-2-4-创建用户关系表" class="headerlink" title="6.2.4 创建用户关系表"></a>6.2.4 创建用户关系表</h3><p><strong>表结构：</strong></p><table><thead><tr><th><strong>方法名</strong></th><th>createTableRelations</th></tr></thead><tbody><tr><td>Table Name</td><td>weibo:relations</td></tr><tr><td>RowKey</td><td>用户ID</td></tr><tr><td>ColumnFamily</td><td>attends、fans</td></tr><tr><td>ColumnLabel</td><td>关注用户ID，粉丝用户ID</td></tr><tr><td>ColumnValue</td><td>用户ID</td></tr><tr><td>Version</td><td>1个版本</td></tr></tbody></table><p><strong>代码：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 用户关系表</span></span><br><span class="line"><span class="comment"> * Table Name:weibo:relations0</span></span><br><span class="line"><span class="comment"> * RowKey:用户ID</span></span><br><span class="line"><span class="comment"> * ColumnFamily:attends,fans</span></span><br><span class="line"><span class="comment"> * ColumnLabel:关注用户ID，粉丝用户ID</span></span><br><span class="line"><span class="comment"> * ColumnValue:用户ID</span></span><br><span class="line"><span class="comment"> * Version：1个版本</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createTableRelations</span><span class="params">()</span></span>&#123;</span><br><span class="line">HbaseAdmin admin = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">admin = <span class="keyword">new</span> HbaseAdmin(conf);</span><br><span class="line">HTableDescriptor relations = <span class="keyword">new</span> HTableDescriptor(TableName.valueOf(TABLE_RELATIONS));</span><br><span class="line"></span><br><span class="line"><span class="comment">//关注的人的列族</span></span><br><span class="line">HColumnDescriptor attends = <span class="keyword">new</span> HColumnDescriptor(Bytes.toBytes(<span class="string">&quot;attends&quot;</span>));</span><br><span class="line"><span class="comment">//设置块缓存</span></span><br><span class="line">attends.setBlockCacheEnabled(<span class="keyword">true</span>);</span><br><span class="line"><span class="comment">//设置块缓存大小</span></span><br><span class="line">attends.setBlocksize(<span class="number">2097152</span>);</span><br><span class="line"><span class="comment">//设置压缩方式</span></span><br><span class="line"><span class="comment">//info.setCompressionType(Algorithm.SNAPPY);</span></span><br><span class="line"><span class="comment">//设置版本确界</span></span><br><span class="line">attends.setMaxVersions(<span class="number">1</span>);</span><br><span class="line">attends.setMinVersions(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//粉丝列族</span></span><br><span class="line">HColumnDescriptor fans = <span class="keyword">new</span> HColumnDescriptor(Bytes.toBytes(<span class="string">&quot;fans&quot;</span>));</span><br><span class="line">fans.setBlockCacheEnabled(<span class="keyword">true</span>);</span><br><span class="line">fans.setBlocksize(<span class="number">2097152</span>);</span><br><span class="line">fans.setMaxVersions(<span class="number">1</span>);</span><br><span class="line">fans.setMinVersions(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">relations.addFamily(attends);</span><br><span class="line">relations.addFamily(fans);</span><br><span class="line">admin.createTable(relations);</span><br><span class="line"></span><br><span class="line">&#125; <span class="keyword">catch</span> (MasterNotRunningException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125; <span class="keyword">catch</span> (ZooKeeperConnectionException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;<span class="keyword">finally</span>&#123;</span><br><span class="line"><span class="keyword">if</span>(<span class="keyword">null</span> != admin)&#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">admin.close();</span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="6-2-5-创建微博收件箱表"><a href="#6-2-5-创建微博收件箱表" class="headerlink" title="6.2.5 创建微博收件箱表"></a>6.2.5 创建微博收件箱表</h3><p><strong>表结构：</strong></p><table><thead><tr><th><strong>方法名</strong></th><th>createTableReceiveContentEmails</th></tr></thead><tbody><tr><td>Table Name</td><td>weibo:receive_content_email</td></tr><tr><td>RowKey</td><td>用户ID</td></tr><tr><td>ColumnFamily</td><td>info</td></tr><tr><td>ColumnLabel</td><td>用户ID</td></tr><tr><td>ColumnValue</td><td>取微博内容的RowKey</td></tr><tr><td>Version</td><td>1000</td></tr></tbody></table><p><strong>代码：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建微博收件箱表</span></span><br><span class="line"><span class="comment"> * Table Name: weibo:receive_content_email</span></span><br><span class="line"><span class="comment"> * RowKey:用户ID</span></span><br><span class="line"><span class="comment"> * ColumnFamily:info</span></span><br><span class="line"><span class="comment"> * ColumnLabel:用户ID-发布微博的人的用户ID</span></span><br><span class="line"><span class="comment"> * ColumnValue:关注的人的微博的RowKey</span></span><br><span class="line"><span class="comment"> * Version:1000</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createTableReceiveContentEmail</span><span class="params">()</span></span>&#123;</span><br><span class="line">HbaseAdmin admin = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">admin = <span class="keyword">new</span> HbaseAdmin(conf);</span><br><span class="line">HTableDescriptor receive_content_email = <span class="keyword">new</span> HTableDescriptor(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL));</span><br><span class="line">HColumnDescriptor info = <span class="keyword">new</span> HColumnDescriptor(Bytes.toBytes(<span class="string">&quot;info&quot;</span>));</span><br><span class="line"></span><br><span class="line">info.setBlockCacheEnabled(<span class="keyword">true</span>);</span><br><span class="line">info.setBlocksize(<span class="number">2097152</span>);</span><br><span class="line">info.setMaxVersions(<span class="number">1000</span>);</span><br><span class="line">info.setMinVersions(<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">receive_content_email.addFamily(info);;</span><br><span class="line">admin.createTable(receive_content_email);</span><br><span class="line">&#125; <span class="keyword">catch</span> (MasterNotRunningException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125; <span class="keyword">catch</span> (ZooKeeperConnectionException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;<span class="keyword">finally</span>&#123;</span><br><span class="line"><span class="keyword">if</span>(<span class="keyword">null</span> != admin)&#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">admin.close();</span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="6-2-6-发布微博内容"><a href="#6-2-6-发布微博内容" class="headerlink" title="6.2.6 发布微博内容"></a>6.2.6 发布微博内容</h3><p>a、微博内容表中添加1条数据</p><p>b、微博收件箱表对所有粉丝用户添加数据</p><p><strong>代码：Message.java</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing.weibo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Message</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> String uid;</span><br><span class="line"><span class="keyword">private</span> String timestamp;</span><br><span class="line"><span class="keyword">private</span> String content;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getUid</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> uid;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUid</span><span class="params">(String uid)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.uid = uid;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getTimestamp</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> timestamp;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setTimestamp</span><span class="params">(String timestamp)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.timestamp = timestamp;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getContent</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> content;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setContent</span><span class="params">(String content)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.content = content;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">&quot;Message [uid=&quot;</span> + uid + <span class="string">&quot;, timestamp=&quot;</span> + timestamp + <span class="string">&quot;, content=&quot;</span> + content + <span class="string">&quot;]&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>代码：public void publishContent(String uid, String content)</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 发布微博</span></span><br><span class="line"><span class="comment"> * a、微博内容表中数据+1</span></span><br><span class="line"><span class="comment"> * b、向微博收件箱表中加入微博的Rowkey</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">publishContent</span><span class="params">(String uid, String content)</span></span>&#123;</span><br><span class="line">HConnection connection = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">connection = HConnectionManager.createConnection(conf);</span><br><span class="line"><span class="comment">//a、微博内容表中添加1条数据，首先获取微博内容表描述</span></span><br><span class="line">HTableInterface contentTBL = connection.getTable(TableName.valueOf(TABLE_CONTENT));</span><br><span class="line"><span class="comment">//组装Rowkey</span></span><br><span class="line"><span class="keyword">long</span> timestamp = System.currentTimeMillis();</span><br><span class="line">String rowKey = uid + <span class="string">&quot;_&quot;</span> + timestamp;</span><br><span class="line"></span><br><span class="line">Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowKey));</span><br><span class="line">put.add(Bytes.toBytes(<span class="string">&quot;info&quot;</span>), Bytes.toBytes(<span class="string">&quot;content&quot;</span>), timestamp, Bytes.toBytes(content));</span><br><span class="line"></span><br><span class="line">contentTBL.put(put);</span><br><span class="line"></span><br><span class="line"><span class="comment">//b、向微博收件箱表中加入发布的Rowkey</span></span><br><span class="line"><span class="comment">//b.1、查询用户关系表，得到当前用户有哪些粉丝</span></span><br><span class="line">HTableInterface relationsTBL = connection.getTable(TableName.valueOf(TABLE_RELATIONS));</span><br><span class="line"><span class="comment">//b.2、取出目标数据</span></span><br><span class="line">Get get = <span class="keyword">new</span> Get(Bytes.toBytes(uid));</span><br><span class="line">get.addFamily(Bytes.toBytes(<span class="string">&quot;fans&quot;</span>));</span><br><span class="line"></span><br><span class="line">Result result = relationsTBL.get(get);</span><br><span class="line">List&lt;<span class="keyword">byte</span>[]&gt; fans = <span class="keyword">new</span> ArrayList&lt;<span class="keyword">byte</span>[]&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">//遍历取出当前发布微博的用户的所有粉丝数据</span></span><br><span class="line"><span class="keyword">for</span>(Cell cell : result.rawCells())&#123;</span><br><span class="line">fans.add(CellUtil.cloneQualifier(cell));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//如果该用户没有粉丝，则直接return</span></span><br><span class="line"><span class="keyword">if</span>(fans.size() &lt;= <span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line"><span class="comment">//开始操作收件箱表</span></span><br><span class="line">HTableInterface recTBL = connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL));</span><br><span class="line">List&lt;Put&gt; puts = <span class="keyword">new</span> ArrayList&lt;Put&gt;();</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">byte</span>[] fan : fans)&#123;</span><br><span class="line">Put fanPut = <span class="keyword">new</span> Put(fan);</span><br><span class="line">fanPut.add(Bytes.toBytes(<span class="string">&quot;info&quot;</span>), Bytes.toBytes(uid), timestamp, Bytes.toBytes(rowKey));</span><br><span class="line">puts.add(fanPut);</span><br><span class="line">&#125;</span><br><span class="line">recTBL.put(puts);</span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;<span class="keyword">finally</span>&#123;</span><br><span class="line"><span class="keyword">if</span>(<span class="keyword">null</span> != connection)&#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">connection.close();</span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="6-2-7-添加关注用户"><a href="#6-2-7-添加关注用户" class="headerlink" title="6.2.7 添加关注用户"></a>6.2.7 添加关注用户</h3><p>a、在微博用户关系表中，对当前主动操作的用户添加新关注的好友</p><p>b、在微博用户关系表中，对被关注的用户添加新的粉丝</p><p>c、微博收件箱表中添加所关注的用户发布的微博</p><p><strong>代码实现：public void addAttends(String uid, String… attends)</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 关注用户逻辑</span></span><br><span class="line"><span class="comment"> * a、在微博用户关系表中，对当前主动操作的用户添加新的关注的好友</span></span><br><span class="line"><span class="comment"> * b、在微博用户关系表中，对被关注的用户添加粉丝（当前操作的用户）</span></span><br><span class="line"><span class="comment"> * c、当前操作用户的微博收件箱添加所关注的用户发布的微博rowkey</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addAttends</span><span class="params">(String uid, String... attends)</span></span>&#123;</span><br><span class="line"><span class="comment">//参数过滤</span></span><br><span class="line"><span class="keyword">if</span>(attends == <span class="keyword">null</span> || attends.length &lt;= <span class="number">0</span> || uid == <span class="keyword">null</span> || uid.length() &lt;= <span class="number">0</span>)&#123;</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line">HConnection connection = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">connection = HConnectionManager.createConnection(conf);</span><br><span class="line"><span class="comment">//用户关系表操作对象（连接到用户关系表）</span></span><br><span class="line">HTableInterface relationsTBL = connection.getTable(TableName.valueOf(TABLE_RELATIONS));</span><br><span class="line">List&lt;Put&gt; puts = <span class="keyword">new</span> ArrayList&lt;Put&gt;();</span><br><span class="line"><span class="comment">//a、在微博用户关系表中，添加新关注的好友</span></span><br><span class="line">Put attendPut = <span class="keyword">new</span> Put(Bytes.toBytes(uid));</span><br><span class="line"><span class="keyword">for</span>(String attend : attends)&#123;</span><br><span class="line"><span class="comment">//为当前用户添加关注的人</span></span><br><span class="line">attendPut.add(Bytes.toBytes(<span class="string">&quot;attends&quot;</span>), Bytes.toBytes(attend), Bytes.toBytes(attend));</span><br><span class="line"><span class="comment">//b、为被关注的人，添加粉丝</span></span><br><span class="line">Put fansPut = <span class="keyword">new</span> Put(Bytes.toBytes(attend));</span><br><span class="line">fansPut.add(Bytes.toBytes(<span class="string">&quot;fans&quot;</span>), Bytes.toBytes(uid), Bytes.toBytes(uid));</span><br><span class="line"><span class="comment">//将所有关注的人一个一个的添加到puts（List）集合中</span></span><br><span class="line">puts.add(fansPut);</span><br><span class="line">&#125;</span><br><span class="line">puts.add(attendPut);</span><br><span class="line">relationsTBL.put(puts);</span><br><span class="line"></span><br><span class="line"><span class="comment">//c.1、微博收件箱添加关注的用户发布的微博内容（content）的rowkey</span></span><br><span class="line">HTableInterface contentTBL = connection.getTable(TableName.valueOf(TABLE_CONTENT));</span><br><span class="line">Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line"><span class="comment">//用于存放取出来的关注的人所发布的微博的rowkey</span></span><br><span class="line">List&lt;<span class="keyword">byte</span>[]&gt; rowkeys = <span class="keyword">new</span> ArrayList&lt;<span class="keyword">byte</span>[]&gt;();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(String attend : attends)&#123;</span><br><span class="line"><span class="comment">//过滤扫描rowkey，即：前置位匹配被关注的人的uid_</span></span><br><span class="line">RowFilter filter = <span class="keyword">new</span> RowFilter(CompareFilter.CompareOp.EQUAL, <span class="keyword">new</span> SubstringComparator(attend + <span class="string">&quot;_&quot;</span>));</span><br><span class="line"><span class="comment">//为扫描对象指定过滤规则</span></span><br><span class="line">scan.setFilter(filter);</span><br><span class="line"><span class="comment">//通过扫描对象得到scanner</span></span><br><span class="line">ResultScanner result = contentTBL.getScanner(scan);</span><br><span class="line"><span class="comment">//迭代器遍历扫描出来的结果集</span></span><br><span class="line">Iterator&lt;Result&gt; iterator = result.iterator();</span><br><span class="line"><span class="keyword">while</span>(iterator.hasNext())&#123;</span><br><span class="line"><span class="comment">//取出每一个符合扫描结果的那一行数据</span></span><br><span class="line">Result r = iterator.next();</span><br><span class="line"><span class="keyword">for</span>(Cell cell : r.rawCells())&#123;</span><br><span class="line"><span class="comment">//将得到的rowkey放置于集合容器中</span></span><br><span class="line">rowkeys.add(CellUtil.cloneRow(cell));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//c.2、将取出的微博rowkey放置于当前操作用户的收件箱中</span></span><br><span class="line"><span class="keyword">if</span>(rowkeys.size() &lt;= <span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line"><span class="comment">//得到微博收件箱表的操作对象</span></span><br><span class="line">HTableInterface recTBL = connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL));</span><br><span class="line"><span class="comment">//用于存放多个关注的用户的发布的多条微博rowkey信息</span></span><br><span class="line">List&lt;Put&gt; recPuts = <span class="keyword">new</span> ArrayList&lt;Put&gt;();</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">byte</span>[] rk : rowkeys)&#123;</span><br><span class="line">Put put = <span class="keyword">new</span> Put(Bytes.toBytes(uid));</span><br><span class="line"><span class="comment">//uid_timestamp</span></span><br><span class="line">String rowKey = Bytes.toString(rk);</span><br><span class="line"><span class="comment">//借取uid</span></span><br><span class="line">String attendUID = rowKey.substring(<span class="number">0</span>, rowKey.indexOf(<span class="string">&quot;_&quot;</span>));</span><br><span class="line"><span class="keyword">long</span> timestamp = Long.parseLong(rowKey.substring(rowKey.indexOf(<span class="string">&quot;_&quot;</span>) + <span class="number">1</span>));</span><br><span class="line"><span class="comment">//将微博rowkey添加到指定单元格中</span></span><br><span class="line">put.add(Bytes.toBytes(<span class="string">&quot;info&quot;</span>), Bytes.toBytes(attendUID), timestamp, rk);</span><br><span class="line">recPuts.add(put);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">recTBL.put(recPuts);</span><br><span class="line"></span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;<span class="keyword">finally</span>&#123;</span><br><span class="line"><span class="keyword">if</span>(<span class="keyword">null</span> != connection)&#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">connection.close();</span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line"><span class="comment">// TODO Auto-generated catch block</span></span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="6-2-8-移除（取关）用户"><a href="#6-2-8-移除（取关）用户" class="headerlink" title="6.2.8 移除（取关）用户"></a>6.2.8 移除（取关）用户</h3><p>a、在微博用户关系表中，对当前主动操作的用户移除取关的好友(attends)</p><p>b、在微博用户关系表中，对被取关的用户移除粉丝</p><p>c、微博收件箱中删除取关的用户发布的微博</p><p><strong>代码：public void removeAttends(String uid, String… attends)</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 取消关注（remove)</span></span><br><span class="line"><span class="comment"> * a、在微博用户关系表中，对当前主动操作的用户删除对应取关的好友</span></span><br><span class="line"><span class="comment"> * b、在微博用户关系表中，对被取消关注的人删除粉丝（当前操作人）</span></span><br><span class="line"><span class="comment"> * c、从收件箱中，删除取关的人的微博的rowkey</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">removeAttends</span><span class="params">(String uid, String... attends)</span></span>&#123;</span><br><span class="line"><span class="comment">//过滤数据</span></span><br><span class="line"><span class="keyword">if</span>(uid == <span class="keyword">null</span> || uid.length() &lt;= <span class="number">0</span> || attends == <span class="keyword">null</span> || attends.length &lt;= <span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line">HConnection connection = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">connection = HConnectionManager.createConnection(conf);</span><br><span class="line"><span class="comment">//a、在微博用户关系表中，删除已关注的好友</span></span><br><span class="line">HTableInterface relationsTBL = connection.getTable(TableName.valueOf(TABLE_RELATIONS));</span><br><span class="line"></span><br><span class="line"><span class="comment">//待删除的用户关系表中的所有数据</span></span><br><span class="line">List&lt;Delete&gt; deletes = <span class="keyword">new</span> ArrayList&lt;Delete&gt;();</span><br><span class="line"><span class="comment">//当前取关操作者的uid对应的Delete对象</span></span><br><span class="line">Delete attendDelete = <span class="keyword">new</span> Delete(Bytes.toBytes(uid));</span><br><span class="line"><span class="comment">//遍历取关，同时每次取关都要将被取关的人的粉丝-1</span></span><br><span class="line"><span class="keyword">for</span>(String attend : attends)&#123;</span><br><span class="line">attendDelete.deleteColumn(Bytes.toBytes(<span class="string">&quot;attends&quot;</span>), Bytes.toBytes(attend));</span><br><span class="line"><span class="comment">//b</span></span><br><span class="line">Delete fansDelete = <span class="keyword">new</span> Delete(Bytes.toBytes(attend));</span><br><span class="line">fansDelete.deleteColumn(Bytes.toBytes(<span class="string">&quot;fans&quot;</span>), Bytes.toBytes(uid));</span><br><span class="line">deletes.add(fansDelete);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">deletes.add(attendDelete);</span><br><span class="line">relationsTBL.delete(deletes);</span><br><span class="line"></span><br><span class="line"><span class="comment">//c、删除取关的人的微博rowkey 从 收件箱表中</span></span><br><span class="line">HTableInterface recTBL = connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL));</span><br><span class="line"></span><br><span class="line">Delete recDelete = <span class="keyword">new</span> Delete(Bytes.toBytes(uid));</span><br><span class="line"><span class="keyword">for</span>(String attend : attends)&#123;</span><br><span class="line">recDelete.deleteColumn(Bytes.toBytes(<span class="string">&quot;info&quot;</span>), Bytes.toBytes(attend));</span><br><span class="line">&#125;</span><br><span class="line">recTBL.delete(recDelete);</span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="6-2-9-获取关注的人的微博内容"><a href="#6-2-9-获取关注的人的微博内容" class="headerlink" title="6.2.9 获取关注的人的微博内容"></a>6.2.9 获取关注的人的微博内容</h3><p>a、从微博收件箱中获取所关注的用户的微博RowKey </p><p>b、根据获取的RowKey，得到微博内容</p><p><strong>代码实现：public List getAttendsContent(String uid)</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 获取微博实际内容</span></span><br><span class="line"><span class="comment"> * a、从微博收件箱中获取所有关注的人的发布的微博的rowkey</span></span><br><span class="line"><span class="comment"> * b、根据得到的rowkey去微博内容表中得到数据</span></span><br><span class="line"><span class="comment"> * c、将得到的数据封装到Message对象中</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;Message&gt; <span class="title">getAttendsContent</span><span class="params">(String uid)</span></span>&#123;</span><br><span class="line">HConnection connection = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">connection = HConnectionManager.createConnection(conf);</span><br><span class="line">HTableInterface recTBL = connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL));</span><br><span class="line"><span class="comment">//a、从收件箱中取得微博rowKey</span></span><br><span class="line">Get get = <span class="keyword">new</span> Get(Bytes.toBytes(uid));</span><br><span class="line"><span class="comment">//设置最大版本号</span></span><br><span class="line">get.setMaxVersions(<span class="number">5</span>);</span><br><span class="line">List&lt;<span class="keyword">byte</span>[]&gt; rowkeys = <span class="keyword">new</span> ArrayList&lt;<span class="keyword">byte</span>[]&gt;();</span><br><span class="line">Result result = recTBL.get(get);</span><br><span class="line"><span class="keyword">for</span>(Cell cell : result.rawCells())&#123;</span><br><span class="line">rowkeys.add(CellUtil.cloneValue(cell));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//b、根据取出的所有rowkey去微博内容表中检索数据</span></span><br><span class="line">HTableInterface contentTBL = connection.getTable(TableName.valueOf(TABLE_CONTENT));</span><br><span class="line">List&lt;Get&gt; gets = <span class="keyword">new</span> ArrayList&lt;Get&gt;();</span><br><span class="line"><span class="comment">//根据rowkey取出对应微博的具体内容</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">byte</span>[] rk : rowkeys)&#123;</span><br><span class="line">Get g = <span class="keyword">new</span> Get(rk);</span><br><span class="line">gets.add(g);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//得到所有的微博内容的result对象</span></span><br><span class="line">Result[] results = contentTBL.get(gets);</span><br><span class="line"></span><br><span class="line">List&lt;Message&gt; messages = <span class="keyword">new</span> ArrayList&lt;Message&gt;();</span><br><span class="line"><span class="keyword">for</span>(Result res : results)&#123;</span><br><span class="line"><span class="keyword">for</span>(Cell cell : res.rawCells())&#123;</span><br><span class="line">Message message = <span class="keyword">new</span> Message();</span><br><span class="line"></span><br><span class="line">String rowKey = Bytes.toString(CellUtil.cloneRow(cell));</span><br><span class="line">String userid = rowKey.substring(<span class="number">0</span>, rowKey.indexOf(<span class="string">&quot;_&quot;</span>));</span><br><span class="line">String timestamp = rowKey.substring(rowKey.indexOf(<span class="string">&quot;_&quot;</span>) + <span class="number">1</span>);</span><br><span class="line">String content = Bytes.toString(CellUtil.cloneValue(cell));</span><br><span class="line"></span><br><span class="line">message.setContent(content);</span><br><span class="line">message.setTimestamp(timestamp);</span><br><span class="line">message.setUid(userid);</span><br><span class="line"></span><br><span class="line">messages.add(message);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> messages;</span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;<span class="keyword">finally</span>&#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">connection.close();</span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="6-2-10-测试"><a href="#6-2-10-测试" class="headerlink" title="6.2.10 测试"></a>6.2.10 测试</h3><p><strong>–</strong> <strong>测试发布微博内容</strong></p><p>public void testPublishContent(WeiBo wb)</p><p><strong>–</strong> <strong>测试添加关注</strong></p><p>public void testAddAttend(WeiBo wb)</p><p><strong>–</strong> <strong>测试取消关注</strong></p><p>public void testRemoveAttend(WeiBo wb)</p><p><strong>–</strong> <strong>测试展示内容</strong></p><p>public void testShowMessage(WeiBo wb)        </p><p><strong>代码：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 发布微博内容</span></span><br><span class="line"><span class="comment"> * 添加关注</span></span><br><span class="line"><span class="comment"> * 取消关注</span></span><br><span class="line"><span class="comment"> * 展示内容</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testPublishContent</span><span class="params">(WeiBo wb)</span></span>&#123;</span><br><span class="line">wb.publishContent(<span class="string">&quot;0001&quot;</span>, <span class="string">&quot;今天买了一包空气，送了点薯片，非常开心！！&quot;</span>);</span><br><span class="line">wb.publishContent(<span class="string">&quot;0001&quot;</span>, <span class="string">&quot;今天天气不错。&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testAddAttend</span><span class="params">(WeiBo wb)</span></span>&#123;</span><br><span class="line">wb.publishContent(<span class="string">&quot;0008&quot;</span>, <span class="string">&quot;准备下课！&quot;</span>);</span><br><span class="line">wb.publishContent(<span class="string">&quot;0009&quot;</span>, <span class="string">&quot;准备关机！&quot;</span>);</span><br><span class="line">wb.addAttends(<span class="string">&quot;0001&quot;</span>, <span class="string">&quot;0008&quot;</span>, <span class="string">&quot;0009&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testRemoveAttend</span><span class="params">(WeiBo wb)</span></span>&#123;</span><br><span class="line">wb.removeAttends(<span class="string">&quot;0001&quot;</span>, <span class="string">&quot;0008&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testShowMessage</span><span class="params">(WeiBo wb)</span></span>&#123;</span><br><span class="line">List&lt;Message&gt; messages = wb.getAttendsContent(<span class="string">&quot;0001&quot;</span>);</span><br><span class="line"><span class="keyword">for</span>(Message message : messages)&#123;</span><br><span class="line">System.out.println(message);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">WeiBo weibo = <span class="keyword">new</span> WeiBo();</span><br><span class="line">weibo.initTable();</span><br><span class="line"></span><br><span class="line">weibo.testPublishContent(weibo);</span><br><span class="line">weibo.testAddAttend(weibo);</span><br><span class="line">weibo.testShowMessage(weibo);</span><br><span class="line">weibo.testRemoveAttend(weibo);</span><br><span class="line">weibo.testShowMessage(weibo);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;第1章-HBase简介&quot;&gt;&lt;a href=&quot;#第1章-HBase简介&quot; class=&quot;headerlink&quot; title=&quot;第1章 HBase简介&quot;&gt;&lt;/a&gt;第1章 HBase简介&lt;/h1&gt;&lt;h2 id=&quot;1-1-HBase定义&quot;&gt;&lt;a href=&quot;#1-1-HBase定义&quot; class=&quot;headerlink&quot; title=&quot;1.1 HBase定义&quot;&gt;&lt;/a&gt;1.1 HBase定义&lt;/h2&gt;&lt;p&gt;HBase是一种分布式、可扩展、支持海量数据存储的NoSQL数据库。&lt;/p&gt;</summary>
    
    
    
    <category term="hbase" scheme="http://iscurry.com/categories/hbase/"/>
    
    
    <category term="Hbase" scheme="http://iscurry.com/tags/Hbase/"/>
    
    <category term="Detail" scheme="http://iscurry.com/tags/Detail/"/>
    
  </entry>
  
  <entry>
    <title>Linux</title>
    <link href="http://iscurry.com/2020/01/04/Linux/"/>
    <id>http://iscurry.com/2020/01/04/Linux/</id>
    <published>2020-01-03T16:49:05.000Z</published>
    <updated>2020-09-25T02:16:49.756Z</updated>
    
    <content type="html"><![CDATA[<p>​    </p><h1 id="1、Centos6-9找回root密码"><a href="#1、Centos6-9找回root密码" class="headerlink" title="1、Centos6.9找回root密码"></a>1、Centos6.9找回root密码</h1><ol><li>开机按enter</li><li>按e进入</li><li>选第二个，按e</li><li>在后面添加 single, 按回车，注意single前有空格</li><li>按b键重启</li><li>输入 passwd 设置密码 </li></ol><a id="more"></a><h1 id="2、vi-vim编辑器"><a href="#2、vi-vim编辑器" class="headerlink" title="2、vi/vim编辑器"></a>2、vi/vim编辑器</h1><h2 id="1、一般模式"><a href="#1、一般模式" class="headerlink" title="1、一般模式"></a>1、一般模式</h2><ul><li>dd  ==删==除一行</li><li>d5d 删除5行</li><li>yy  ==复制==一行</li><li>y5y 复制5行</li><li>p   ==粘贴==</li><li>u ==撤销==</li><li>ctrl r 撤销撤销</li><li>^   当前行==开头==</li><li>$   当前行==结束==</li><li>gg  文件==开头==</li><li>G   文件==结尾==</li></ul><h2 id="2、编辑模式"><a href="#2、编辑模式" class="headerlink" title="2、编辑模式"></a>2、编辑模式</h2><ul><li>i   当前位置插入(进入编辑模式)</li><li>a   当前位置后插入（进入编辑模式）</li><li>o   向下一行插入（进入编辑模式）</li><li>ESC键 退出编辑模式</li></ul><h2 id="3、指令模式"><a href="#3、指令模式" class="headerlink" title="3、指令模式"></a>3、指令模式</h2><ul><li><p>:set nu 显示行号</p></li><li><p>?xxx 查找指定关键字   n/N 上下一个</p></li><li><p>:wq  保存件并退出</p></li><li><p>:!wq 强制保存并退出</p></li><li><p>ZZ    保存并退出</p></li></ul><h1 id="3、文件目录类-命令"><a href="#3、文件目录类-命令" class="headerlink" title="3、文件目录类==(命令)=="></a>3、文件目录类==(命令)==</h1><ol><li><p>创建文件夹 mkdir dirName    </p></li><li><p>创建多层文件夹 mkdir -p dirName</p></li><li><p>查看文件列表 ls -a    (包隐藏文件)    </p></li><li><p>查看文件列表详情 ll     </p></li><li><p>创建文件   touch fileName    </p></li><li><p>删除文件 rm -rf source    </p></li><li><p>切换路径到bin下 cd bin/            </p></li><li><p>回到上一级  cd ..    </p></li><li><p>切换到绝对路径  cd /d/soft/        </p></li><li><p>查看当前路径真实目录 pwd                </p></li><li><p>查看文件    cat fileName    </p></li><li><p>倒叙输出 tac fileName</p></li><li><p>查看文件后10行    tail -n 10    a.txt    </p><ul><li>-f    实时监控变化</li></ul></li><li><p>查看文件前10行    head -n 10        </p></li><li><p>more</p></li><li><p>less</p></li><li><p>追加  echo aaa &gt;&gt; a.txt</p></li><li><p>重定向 echo a &gt; a.txt</p></li><li><p>echo</p><ul><li>-e 转义“”号里面的字符，例如 echo -e “abc\tdef”</li></ul></li><li><p>软链接 ln -s source dest</p></li><li><p>历史 history</p></li><li><p>复制    cp source dest </p></li><li><p>强制复制 \cp source dest </p></li><li><p>移动或改名 mv source dest</p></li></ol><h1 id="4、时间日期类-命令"><a href="#4、时间日期类-命令" class="headerlink" title="4、时间日期类==(命令)=="></a>4、时间日期类==(命令)==</h1><ol><li>查看当前时间 date</li><li>查看当前日历 cal</li><li>指定格式时间 date “+%Y-%m-%d %H:%M:%S”     </li><li>设置时间 date -s “20121111 20:20:20”</li></ol><h1 id="5、用户、组管理类-命令"><a href="#5、用户、组管理类-命令" class="headerlink" title="5、用户、组管理类==(命令)=="></a>5、用户、组管理类==(命令)==</h1><ol><li><p>添加用户 useradd username</p></li><li><p>设置用户密码 passwd username</p></li><li><p>判断是否存在 id username</p></li><li><p>切换 su username</p></li><li><p>删除用户 userdel username</p></li><li><p>查看登录用户的信息 who/who am i /whoami</p></li><li><p>设置普通用户有root权限 </p><ul><li><p>修改 /etc/sudoers ,在root后添加一行</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root    ALL=(ALL)     ALL</span><br><span class="line">user   ALL=(ALL)     ALL</span><br></pre></td></tr></table></figure></li></ul></li><li><p>查看所有用户 cat  /etc/passwd</p></li><li><p>修改用户  usermod -g 用户组 用户名</p></li><li><p>新增组 groupadd 组名</p></li><li><p>删除组 groupdel 组名</p></li><li><p>修改组 groupmod -n newName oldName</p></li><li><p>查看所有组 cat /etc/group</p></li></ol><h1 id="6、权限"><a href="#6、权限" class="headerlink" title="6、权限"></a>6、权限</h1><ol><li>改变权限 <ul><li>chmod  u+rwx g+rwx   o+rwx a.txt</li><li>chmod a+rwx a.txt</li><li>chmod 777 a.txt</li></ul></li><li>改变所有者 chown userName  fileName</li><li>改变所属组 chgrp groupName userName</li></ol><h1 id="7、fdisk"><a href="#7、fdisk" class="headerlink" title="7、fdisk"></a>7、fdisk</h1><ol start="0"><li><p>有两种磁盘结构</p><ul><li>IDE: 标识为： hdxn</li><li>SCSI: 标识为 sdxn   x代表第几块硬盘[a开始]，n代表第几个分区[1开始]</li></ul></li><li><p>查看分区详情  fdisk -l</p></li><li><p>查看所有挂载情况  lsblk</p></li><li><p>查看硬盘使用情况 df -TH</p></li><li><p>目录磁盘使用情况  du -h /目录   例如  du -ach –max-depth= 1 /usr</p><ul><li><p>-s 指定目录占用大小汇总</p></li><li><p>-h 带计量单位</p></li><li><p>-a 含文件</p></li><li><p>-max-depth=1 子目录深度</p></li><li><p>-c 列出明细的同时，增加汇总值</p></li></ul></li><li><p>增加一块新硬盘</p><ol><li><p>分区 fdisk /dev/sdb</p><ul><li>m 显示命令列表</li><li>n 新增分区</li><li>d 删除分区</li><li>w 写入并退出</li><li>p 显示磁盘分区</li></ul></li><li><p>格式化 sdb1  mkfs -t ext4 /dev/sdb1</p></li><li><p>挂载 mount /dev/sdb1   /home</p></li><li><p>卸载  umont 挂载路径或者挂载硬盘</p></li><li><p>永久挂载 vim /etc/fstab 添加一行</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/dev/sdb1/homeext4 defaults 0 0</span><br></pre></td></tr></table></figure></li></ol></li></ol><h1 id="8、crontab"><a href="#8、crontab" class="headerlink" title="8、crontab"></a>8、crontab</h1><ol><li><p>开启服务 service crontab start</p></li><li><p>操作 crontab</p><ul><li><p>-e 编辑</p></li><li><p>-l 查看</p></li><li><p>举例</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">crontab -e</span><br><span class="line">* * * * * date &gt;&gt; /home/date.log  </span><br><span class="line">分 时 天 月 周几</span><br><span class="line">第几分钟 那个小时 每月中的那一天 每年的那一个月 没有的星期几 </span><br><span class="line"></span><br><span class="line">*/3 10-15 1,15 * * date &gt;&gt; /home/date</span><br><span class="line"><span class="comment"># 每3分钟</span></span><br><span class="line"><span class="comment"># 每天的10到15点</span></span><br><span class="line"><span class="comment"># 每月1和15号</span></span><br></pre></td></tr></table></figure></li></ul></li></ol><h1 id="9、搜索查询"><a href="#9、搜索查询" class="headerlink" title="9、搜索查询"></a>9、搜索查询</h1><ol><li><p>find /home -name “java.jar” -user user -size +2048</p></li><li><p>which ls</p></li><li><p>whereis start-dfs.sh</p></li><li><p>==locate==: 快速查询，不扫描整个文件系统，需要定期更新locate数据库  updatedb</p></li><li><p>grep </p><p>－c：只输出匹配行的计数。</p><p>－I：不区分大小写(只适用于单字符)。</p><p>－h：查询多文件时不显示文件名。</p></li></ol><h1 id="10、进程、网络状态"><a href="#10、进程、网络状态" class="headerlink" title="10、进程、网络状态"></a>10、进程、网络状态</h1><ol start="2"><li>查看进程 ps -ef | grep ‘’”</li><li>查看进程 ps -aux </li><li>动态进程【查看内存】 top </li><li>杀死进程【强制】 kill -9 pid</li><li>进程树 pstree</li><li>io进程 iotop</li><li>io进程树 iotree</li><li>网络状态【端口占用】 netstat -antp </li><li>那些端口在监听 netstat -tlnp </li></ol><h1 id="11、ssh免密"><a href="#11、ssh免密" class="headerlink" title="11、ssh免密"></a>11、ssh免密</h1><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">步骤:</span><br><span class="line"></span><br><span class="line">1. 进入目录 cd /root/.ssh</span><br><span class="line">2. 生成私钥和公钥 ssh-keygen -t rsa</span><br><span class="line">3. 创建授权文件 touch authorized_keys</span><br><span class="line">4. 将公钥写入授权文件 并发送到其他虚拟机</span><br><span class="line">5. 保证在其他虚拟机的/root/.ssh/authorized_keys中有本虚拟机的公钥即可,</span><br><span class="line">这样就能在本虚拟机对另一台虚拟机进行免密ssh访问</span><br><span class="line">6. /root/.ssh/下的文件有</span><br><span class="line">1. authorized_keys</span><br><span class="line">2. id_rsa</span><br><span class="line">3. id_rsa.pub</span><br><span class="line">4. known_hosts</span><br><span class="line"></span><br><span class="line">普通用户ssh访问</span><br><span class="line">1. 切换到普通用户家目录 /home/xing/.ssh/</span><br><span class="line">2. 生成公钥，私钥，写入授权文件并发送</span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line">cat id_rsa.pub &gt; authorized_keys</span><br><span class="line">ssh-copy-id node002</span><br><span class="line">3. 注意.ssh 和 里面文件的权限</span><br><span class="line">chmod 700 .ssh </span><br><span class="line">chmod 600 .ssh/*</span><br><span class="line">4. 如果还是因为权限问题登录不上</span><br><span class="line">sudo ssh node002</span><br><span class="line"></span><br><span class="line">另一种：</span><br><span class="line">1. 服务器分别 ssh-keygen -t rsa</span><br><span class="line">2. 复制id到其他机器  ssh-copy-id node1 </span><br><span class="line">3. ok</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="12、虚拟机网络配置"><a href="#12、虚拟机网络配置" class="headerlink" title="12、虚拟机网络配置"></a>12、虚拟机网络配置</h1><hr><p>配置虚拟机网络 三大步</p><ol><li>windows 网络适配器 – VMnet8 – IPv4<br>ip 地址  192.168.10.1<br>子网掩码  255.255.255.0<br>默认网关  192.168.10.2<br>DNS 8.8.8.8</li><li>虚拟机虚拟网络设置<br>VMnet8 – NAT模式–更改设置<br>子网IP 192.168.10.0<br>子网掩码 255.255.255.0<br> NAT设置<br>网关IP: 192.168.10.2</li><li>ifcfg-eth0<br>vi /etc/sysconfig/network-scripts/ifcfg-eth0<br>ONBOOT=yes<br>BOOTPROTO=static<br>IPADDR=192.168.10.201<br>GATEWAY=192.168.10.2<br>PREFIX=24<br>IPV4_FAILURE_FATAL=yes<br>IPV6INIT=no<br>DNS1=192.168.10.2  # 这个和网关一致</li></ol><hr><h1 id="13、系统配置"><a href="#13、系统配置" class="headerlink" title="13、系统配置"></a>13、系统配置</h1><ol><li><p>修改主机ip vi /etc/sysconfig/network-scripts/ifcfg-eth0</p></li><li><p>修改主机名 vi /etc/sysconfig/network</p></li><li><p>修改mac地址 vi /etc/udev/rules.d/70-persistent-net.rules</p></li><li><p>重启网络服务 service network restart</p></li><li><p>关闭防火墙 service iptables stop(start）</p></li><li><p>查看防火墙状态 service iptables status</p></li><li><p>关闭防火墙开机自启 chkconfig iptables off(on)</p></li><li><p>查看防火墙开机自启状态 chkconfig iptables –list</p></li><li><p>开机自启级别设置 chkconfig iptables –level 3,6 on</p></li><li><p>查看ip  ifconfig         </p></li><li><p>查看主机名     hostname </p></li><li><p>修改主机名 vi /etc/sysconfig/network </p></li><li><p>配java环境变量 vi /etc/profile</p><pre><code>|export JAVA_HOME=/home/soft/jdk/jdk-1.8|export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;JAVA_HOME&#125;/jre/bin:$PATH|export CLASSPATH=$&#123;JAVA_HOME&#125;/lib:$&#123;JAVA_HOME&#125;/jre/bin:$CLASSPATH</code></pre></li><li><p>刷新 source /etc/profile  source是关键字，不是命令</p></li><li><p>修改hosts    vim /etc/hosts</p></li><li><p>别名 alias </p></li><li><p>查看所有java进程 jps</p></li><li><p>执行上次以find开头的命令 !find  </p></li><li><p>防止小键盘中文乱码 export TERM=ansi</p></li></ol><h1 id="14、yum-rpm"><a href="#14、yum-rpm" class="headerlink" title="14、yum/rpm"></a>14、yum/rpm</h1><ol><li>yum安装程序    yum -y install git  </li><li>yum搜索程序  yum  list | grep  ‘firefox’</li><li>rpm查看程序    rpm -qa | grep “jdk” </li><li>rpm卸载程序    rpm -e –nodeps <code>rpm -qa | grep &quot;jdk&quot;</code></li><li>rpm程程序安装 rpm -ivh …..  <ul><li>-i 安装 install</li><li>-v 日志 verbose</li><li>-h 刷新 hash</li></ul></li></ol><h1 id="15、压缩"><a href="#15、压缩" class="headerlink" title="15、压缩"></a>15、压缩</h1><ol start="24"><li>zip压缩    gzip source <pre><code>1. 只压缩文件，不压缩目录，不保留原文件，压缩为.gz文件</code></pre></li></ol><ol start="25"><li>zip解压 gunzip source.gz <pre><code>1. 解压.gz文件，不保留原文件</code></pre></li></ol><ol start="26"><li>zip压缩 zip -r test.zip file1 file2 file3  <pre><code>1. 将这三个文件压缩到test.zip,压缩目录，保留原文件</code></pre></li></ol><ol start="27"><li>zip解压 unzip -d dest source.zip <pre><code>1. 解压.zip文件到指定目录</code></pre></li></ol><ol start="28"><li>tar压缩 tar -zcvf source 打包成 *.tar.gz</li></ol><ol start="29"><li>tar解压 tar -zxvf source 解压 *.tar.gz<pre><code>&lt;-C dest&gt; 解压到指定目录 </code></pre></li></ol><h1 id="16、mysql安装卸载"><a href="#16、mysql安装卸载" class="headerlink" title="16、mysql安装卸载"></a>16、mysql安装卸载</h1><ol><li>Q/A 重装mysql<ol><li>查mysql <code>rpm -qa | grep -i mysql </code></li><li>卸载mysql  <code>rpm -e --nodeps mysql名字</code></li><li>手动卸载  <code>whereis mysql</code>     查出mysql所有目录，并删除</li><li>查找mysql <code>find / -name mysql</code></li><li>安装  <code>rpm -ivh Mysql-server</code></li></ol></li></ol><h1 id="17、-上传下载-rzsz"><a href="#17、-上传下载-rzsz" class="headerlink" title="17、 上传下载 rzsz"></a>17、 上传下载 rzsz</h1><ul><li><p>安装、上传、下载 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum install -y lrzsz </span><br><span class="line">rz</span><br><span class="line">sz a.txt </span><br></pre></td></tr></table></figure></li></ul><h1 id="18、-grep"><a href="#18、-grep" class="headerlink" title="18、 grep"></a>18、 grep</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">alisa grep=&#x27;grep --color=auto&#x27; 添加颜色 关键字 颜色</span><br><span class="line">grep &quot;a\|b&quot; 多个条件  同时匹配ab</span><br><span class="line">grep -v &quot;#&quot; 反向过滤  grep -v &#x27;123&#x27; 把123排除</span><br><span class="line">grep -n 显示行号</span><br><span class="line">grep -l 只显示文件名</span><br></pre></td></tr></table></figure><h1 id="19、sed"><a href="#19、sed" class="headerlink" title="19、sed"></a>19、sed</h1><ul><li>流编辑器，一次处理一行</li><li>替换，删除</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1. 修改所有的79 变为 80</span></span><br><span class="line">sed -i &quot;s/79/80/&quot; a.txt </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2. 删除</span></span><br><span class="line">sed -i &quot;/^$/d&quot; a.txt </span><br></pre></td></tr></table></figure><p>sed –help</p><ol><li><p>标准输出 sed ‘p’ a.txt 默认缓存,打印两边</p></li><li><p>取消标准输出 sed -n ‘p’ a.txt  打印一边</p></li><li><p>向下添加 sed ‘1a hello’ a.txt 再所有包含c的行的下方添加hello</p></li><li><p>向上添加 sed ‘/c/i hello’ a.txt 在包含c的行的上方添加一行 hello</p></li><li><p>删除 sed ‘/c/d’ a.txt 删除所有包含c的行</p></li><li><p>替换 sed ‘s/c/C/‘ a.txt  将所有行的首单词的c–&gt;C</p></li><li><p>直接写入文件 sed -i ‘s/[tT]/S/‘ a.txt  替换所有行首单词的t或者T –&gt;S 并保存文件</p></li><li><p>执行多条命令 sed -e ‘expr1’ -e ‘expr2’ a.txt</p></li><li><p>匹配全部 加g  sed ‘s/c/C/g’ a.txt  将a.txt里的所有c–&gt;C</p></li><li><p>保留字替换(换中间) sed ‘s/(lo)cal(host)/\1ve\2/g’ hosts  将所有 localhost –&gt; lovehost</p><pre><code>sed &#39;s/localhost/lovehost/&#39; hostssed &#39;s/ \(lo\) cal \(host\) / \1 ve \2 /&#39; hosts</code></pre></li><li><p>替换(换两边) sed ‘s/local/–&amp;–/g’ hosts  将所有 local–&gt; –local–</p></li><li><p>几次到几次 x{m,n} m, ,n </p></li><li><p>取反 : 删除不包含local的行 sed ‘/local/!d’ a.txt </p></li></ol><h1 id="20、cut"><a href="#20、cut" class="headerlink" title="20、cut"></a>20、cut</h1><ul><li><p>每次读一行</p></li><li><p>cut -d : -f 2     分隔符 ， 取第几列，  默认制表符分割</p></li></ul><h1 id="21、awk"><a href="#21、awk" class="headerlink" title="21、awk"></a>21、awk</h1><ul><li>文本分析工具，把文件逐行读入，默认分隔符是空格或制表符</li><li>awk -F : </li></ul><ol start="0"><li><p>格式: awk <option> [action] file</p></li><li><p>awk ‘{print}’ 默认分隔符是空格或tab,默认打印全部,等同于$0</p></li><li><p>awk -F”:” ‘{print $1}’ /etc/passwd </p></li><li><p>内置变量 NR NF FS<br>awk -F”:” ‘/^root/{print “分隔符:”FS “\t行号: “NR “\t\t用户名: “$1 “\t\t家目录”$(NF-1)}’ /etc/passwd</p></li><li><p>awk ‘BEGIN[count=0]{print count}END{print “结束”}’ /etc/passwd</p></li><li><p>正则+ BEGIN END<br>awk ‘/root/{count++}END{print count}’ /etc/passwd   默认count是0,但是最好要给它初始化</p></li><li><p>if<br>awk ‘{if($1&gt;$2){print}}’ c.txt</p></li><li><p>if else if else<br>awk ‘{if($1&gt;$2){print $1” &gt; “$2}else if($1&lt;$2){print $1” &lt; “$2}else{print $1” = “$2}}’ c.txt</p></li><li><p>while<br>awk ‘{n=1;while(n&lt;=NF){print $n}}’ b.txt  </p></li><li><p>for<br>awk ‘{for(i=1;i&lt;NF;i++){print $i}}’ b.txt</p></li><li><p>for+if<br>awk ‘{for(i=1;i&lt;=NF;i++){print $i;if(i==1){continue}else{break}}}’ b.txt</p></li></ol><p>10  for<br>    awk ‘{name[x++]=$2}END{for(i in name){print name[i]}}’ b.txt<br>    awk ‘{name[x++]=$2}END{for(i=0;i&lt;NR;i++){print name[i]}}’ b.txt<br>    awk ‘{name[$1]=$2}’ ……………………………………..</p><pre><code>awk &#39;&#123;name[$2]++&#125;END&#123;for(i in name)&#123;print i,name[i]&#125;&#125;&#39; b.txt  === workcount [这就是以字符作为下标]</code></pre><ol start="11"><li><p>awk内建函数 sub : 只会找第一个<br>awk ‘{sub(/hello/,”hi”);print}’ y.txt</p></li><li><p>内建函数 gsub : 全文替换</p></li><li><p>index(String,”testString”) 第一次出现的下标的偏移量<br>awk ‘{print index($1,”e”)}’ z.txt</p></li><li><p>字符串的长度 length(Str)<br>字符串的长度   awk ‘{print length($2),$2}’ y.txt<br>每条记录的长度 awk ‘{print length}’ y.txt</p></li><li><p>截取 substr(Str,index,len) 偏移量从0开始,截取几个,不够的就空着<br>awk ‘{print $2,substr($2,2,6)}’ z.txt  从2开始截取,截取6个</p></li></ol><h1 id="22、正则表达式"><a href="#22、正则表达式" class="headerlink" title="22、正则表达式"></a>22、正则表达式</h1><ol><li><p>[^a-f]  ==取反== 除了a-f之外的字符</p></li><li><p>[a-z0-9]  </p></li><li><p>[0-9]{,10}  0-9,不能超过10位</p></li><li><p>&lt;the&gt; 匹配==单词==the &lt;&gt;代表单词边界  the  thee  three  reethe  只能匹配到the</p></li><li><p>^word : ==开头==    ，脱字符加work: 搜索以work开头的内容<br>work$  ==结尾==<br>^$  ==空行==<br>“” ==空格==<br>.  ==任意一个==字符,可代替空格,不能代替空行<br>\  转义符</p><p>* 0次或多次<br>.* 包含==所有==,除了空行<br>^.*  0个或多个开头</p><p>{} ==次数==  {2} {2,} {,5} {3,4}</p></li></ol><blockquote><p>bre 基本正则 ere 扩展正则<br>区别: 元字符的不同<br>   基本正则:$^.[]*<br>   扩展正则:(){}?+|</p></blockquote><h1 id="23、-ntp时间同步"><a href="#23、-ntp时间同步" class="headerlink" title="23、 ntp时间同步"></a>23、 ntp时间同步</h1><ul><li>==<strong>必须是root用户</strong>==</li><li>原理： 把时间服务器配好就行了，其他的服务器只需要执行同步命令就ok</li><li>注意： 只有服务器节点开ntpd服务，其他不开</li></ul><ol><li><p>时间服务器</p><ul><li><p>rpm -qa|grep ntp</p></li><li><p>yun -y install ntp</p></li><li><p>vi /etc/ntp.conf</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一处</span></span><br><span class="line"><span class="comment">#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap为</span></span><br><span class="line">restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line"><span class="comment"># 第二处</span></span><br><span class="line">server 0.centos.pool.ntp.org iburst</span><br><span class="line">server 1.centos.pool.ntp.org iburst</span><br><span class="line">server 2.centos.pool.ntp.org iburst</span><br><span class="line">server 3.centos.pool.ntp.org iburst为</span><br><span class="line"><span class="comment">#server 0.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="comment">#server 1.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="comment">#server 2.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="comment">#server 3.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="comment"># 第三处</span></span><br><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>vim /etc/sysconfig/ntpd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 让硬件与系统时间一起同步</span></span><br><span class="line">SYNC_HWCLOCK=yes</span><br></pre></td></tr></table></figure></li><li><p>重新启动ntpd服务 service ntpd restart</p></li></ul></li></ol><ol start="2"><li><p>其他节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">crontab -e </span><br><span class="line">*/1 * * * * /usr/sbin/ntpdate node1</span><br></pre></td></tr></table></figure></li></ol><h1 id="24、rsync-xsync-集群同步"><a href="#24、rsync-xsync-集群同步" class="headerlink" title="24、rsync/xsync 集群同步"></a>24、rsync/xsync 集群同步</h1><ul><li>创建脚本 xsync</li><li>修改权限 chmod 777 xsync</li><li>发送到 /bin/</li><li>rsync  参数 user@ndode:dir<ul><li>-r 递归  recursion</li><li>-v 显示过程  view</li><li>-l 拷贝链接符号  ln</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="keyword">if</span>(( <span class="variable">$#</span> == 0 ));</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> no args;</span><br><span class="line"><span class="built_in">exit</span>;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">fdir=`<span class="built_in">cd</span> -P $(dirname <span class="variable">$1</span>);<span class="built_in">pwd</span>`;</span><br><span class="line">fname=`basename <span class="variable">$1</span>`;</span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$user</span>;</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$fdir</span>;</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$fname</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>((i=1;i&lt;4;i++));</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"><span class="built_in">echo</span> ---------------node00<span class="variable">$i</span>----------------</span><br><span class="line">rsync -rvl <span class="variable">$fdir</span>/<span class="variable">$fname</span> <span class="variable">$user</span>@node00<span class="variable">$i</span>:<span class="variable">$fdir</span>;</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h1 id="25、yum-ss代理"><a href="#25、yum-ss代理" class="headerlink" title="25、yum ss代理"></a>25、yum ss代理</h1><ul><li><p>前提 </p><ul><li>宿主机ip 192.168.10.1</li><li>虚拟机ip 192.168.10.103</li></ul></li><li><p>宿主机启动ss,并配置</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200610120030.png" alt="image-20200610114823573"></p></li><li><p>虚拟机配置 /etc/yum.conf</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200610120031.png" alt="image-20200610115012323"></p></li></ul><h1 id="26、shell编程"><a href="#26、shell编程" class="headerlink" title="26、shell编程"></a>26、shell编程</h1><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200609004427.png" alt="image-20200608220138131"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">arr=()</span><br><span class="line">arr[0]=1</span><br><span class="line">arr[1]=2</span><br><span class="line">arr[2]=3</span><br><span class="line">name=curryhelloy</span><br><span class="line"><span class="comment"># 数组长度</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;#arr[*]&#125;</span>  <span class="comment"># 3</span></span><br><span class="line"><span class="comment"># 字符串长度</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;#name&#125;</span>   <span class="comment"># 11</span></span><br><span class="line"><span class="comment"># 截取str</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;name:0:3&#125;</span>  <span class="comment"># cur</span></span><br><span class="line"><span class="comment"># 查找str</span></span><br><span class="line"><span class="built_in">echo</span> `expr index <span class="variable">$name</span> y`  <span class="comment"># 5</span></span><br><span class="line"><span class="comment"># 输出arr</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;arr[*]&#125;</span> <span class="comment"># 1 2 3</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;arr[@]&#125;</span> <span class="comment"># 1 2 3</span></span><br><span class="line"><span class="comment"># echo</span></span><br><span class="line"><span class="built_in">echo</span>  -e <span class="string">&quot;hello\c&quot;</span></span><br><span class="line"><span class="built_in">echo</span>  <span class="string">&quot;hi\t\n&quot;</span></span><br></pre></td></tr></table></figure><p>一、变量</p><ol><li><p>定义 变量名=值 变量名用字母数字下划线，数字不能开头，建议大写变量名</p></li><li><p>赋值<br>a. 普通赋值 NAME=’张三’<br>b. 使用$()或反引号``  NAME=$(ls) NAME=<code>ls</code><br>c. 伪赋值 let NAME=’李四’  只是这一行是李四，其它还是张三<br>d. 单引号和双引号的区别：</p><pre><code>单引号： 对所有特殊字符脱义，所有特殊字符只代表字符，没有了含义，so,&#39;$NAME&#39; 只是代表字符串双引号： 只对空格脱义</code></pre></li><li><p>修饰<br>a. unset 变量名 – 撤销变量<br>b. readonly 变量名 – 只读变量，无法unset<br>c. export 变量名 –  提升为全局变量，在文件里定义后，要source将该文件立即生效</p></li><li><p>位置参数变量<br>a. $n </p><pre><code>$0: 代表命令本身$1~$n 代表第1~n个参数 大于10时，用&#123;&#125;包括如：$&#123;10&#125;</code></pre><p>b. $#/$*  代表所有参数，*作为一个整体 ，#所有参数是独立的  </p><p>c. $#  代表所有参数的个数</p></li><li><p>预定义变量<br>a. $? 上一次运行是否正确 0 is true，1 is false ,其他为语法错误 数字有系统决定</p><p>b. $$ 当前进程(脚本)的PID</p><p>c. $! 后台最后一个进程的PID</p></li><li><p>运算符<br>由于 `` 和 $() 是执行命令的<br>a. expr</p><pre><code>a. 1+1 expr 1 + 1 -- 2b. 3-(3*4)   `expr 3 - $(expr 3 \* 4 )`</code></pre><p>b. $(())</p><pre><code>a. 3-(3*4)  $((3-(3*4)))</code></pre><p>c. $[]    </p><pre><code>a.3-(3*4)  $[3-(3*4)]</code></pre></li><li><p>条件判断  数字比较 用 -lt -gt -eq -ne 字符用 = !=<br>a. [ condition ]  echo $?  </p><p>b. 三元运算符 :  [ condition ] &amp;&amp; true || false</p><p>c. 判断运算： -lt小于 -le小于等于 -gt大于 -ge大于等于 -eq等于 -ne不等于  [1 -ne 2]</p><pre><code>    -rwx 读 写 执行  [ -rwx a.txt ]    -fde 文件 文件夹 存在 [ -ef a.txt ]</code></pre><p>d. 或与非 -o -a !</p><pre><code>[ ! expr ] [ expr1 -a expr2 ] [ expr1 -o expr2 ][[ ! expr  ]]  [[ expr1 &amp;&amp; expr2   ]] [[ expr1 || expr2 ]][[ 1 -eq 2 || &quot;xing&quot; = xin? ]] echo $? --&gt; 0if (( 1 == 1 )) #[ 1 -eq 1 ]</code></pre></li><li><p>if<br>if [ condition ]</p><pre><code>then ...</code></pre><p>elif [ conditon ]</p><pre><code>then ...</code></pre><p>else …<br>fi</p></li><li><p>case<br>case $1 in</p><pre><code>1) ... ;;  值可以不用双引号,如果是简单值的话a) ... ;;*) ... ;;</code></pre><p>esac</p></li><li><p>for<br>for var in list do …done</p><p>for(( i=1;i&lt;10;i++ )) do … done<br>for i in $* do … done </p><p>for n in 1 2 3 4 5 6 7<br>do</p><pre><code>echo $n</code></pre><p>done<br>类似于java<br>for(Integer n : n[1,2,3,4,5,6] )<br>String[] str = new String[];</p></li><li><p>while [ condition ] do … done</p></li><li><p>read -p ‘placeholder’ -t 5 var</p></li><li><p>函数 必须先定义后使用,因为shell是解释语言,从上往下,逐行运行<br>a. 系统函数</p><pre><code>1. basename [String | PathName][suffix] 截取文件名2. dirname [String | PathName] 截取最后一层目录</code></pre><p>b. 自定义函数 function funName(){}</p></li><li><p>变量的区域 像输出1906A<br>A=1906<br>echo “${A}A”</p></li><li><p>上传下载<br>yum -y install lrzsz<br>rz / sz name</p></li><li><p>多行注释<br>:&gt;&gt;E<br>…<br>E</p></li></ol><h1 id="待整理"><a href="#待整理" class="headerlink" title="==待整理=="></a>==待整理==</h1>]]></content>
    
    
    <summary type="html">&lt;p&gt;​    &lt;/p&gt;
&lt;h1 id=&quot;1、Centos6-9找回root密码&quot;&gt;&lt;a href=&quot;#1、Centos6-9找回root密码&quot; class=&quot;headerlink&quot; title=&quot;1、Centos6.9找回root密码&quot;&gt;&lt;/a&gt;1、Centos6.9找回root密码&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;开机按enter&lt;/li&gt;
&lt;li&gt;按e进入&lt;/li&gt;
&lt;li&gt;选第二个，按e&lt;/li&gt;
&lt;li&gt;在后面添加 single, 按回车，注意single前有空格&lt;/li&gt;
&lt;li&gt;按b键重启&lt;/li&gt;
&lt;li&gt;输入 passwd 设置密码 &lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Linux" scheme="http://iscurry.com/categories/Linux/"/>
    
    
    <category term="Linux" scheme="http://iscurry.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Hive(1)</title>
    <link href="http://iscurry.com/2020/01/01/Hive(1)/"/>
    <id>http://iscurry.com/2020/01/01/Hive(1)/</id>
    <published>2020-01-01T11:20:20.000Z</published>
    <updated>2020-09-25T02:17:56.477Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-hive简介"><a href="#1-hive简介" class="headerlink" title="1. hive简介"></a>1. hive简介</h1><ol><li>官网 <a href="http://hive.apache.org/">http://hive.apache.org/</a></li><li>文档 <a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></li><li>下载 <a href="http://archive.apache.org/dist/hive/">http://archive.apache.org/dist/hive/</a></li><li>github <a href="https://github.com/apache/hive">https://github.com/apache/hive</a></li></ol><a id="more"></a><h1 id="2-hive安装"><a href="#2-hive安装" class="headerlink" title="2. hive安装"></a>2. hive安装</h1><h2 id="1、解压hive安装包，改名为hive"><a href="#1、解压hive安装包，改名为hive" class="headerlink" title="1、解压hive安装包，改名为hive"></a>1、解压hive安装包，改名为<font color="#FF0000">hive</font></h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /home/soft/</span><br><span class="line">cd /home/soft</span><br><span class="line">mv apache-hive-1.2.1-bin hive</span><br></pre></td></tr></table></figure><h2 id="2、创建hive-env-sh"><a href="#2、创建hive-env-sh" class="headerlink" title="2、创建hive-env.sh"></a>2、创建<font color="#FF0000">hive-env.sh</font></h2><ul><li>修改 hive/conf/hive-env.sh.template 为 hive-env.sh</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure><h2 id="3、配置hive-conf-hive-env-sh"><a href="#3、配置hive-conf-hive-env-sh" class="headerlink" title="3、配置hive/conf/hive-env.sh"></a>3、配置hive/conf/hive-env.sh</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim </span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/home/soft/hadoop</span><br><span class="line"><span class="built_in">export</span> HIVE_CONF_DIR=/home/soft/hive/conf</span><br></pre></td></tr></table></figure><h2 id="4、启动hadoop集群"><a href="#4、启动hadoop集群" class="headerlink" title="4、启动hadoop集群"></a>4、启动hadoop集群</h2><ul><li>因为hive是基于hadoop的，所以要启动hadoop集群</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><h2 id="5、创建hdfs文件-warehouse-和-tmp"><a href="#5、创建hdfs文件-warehouse-和-tmp" class="headerlink" title="5、创建hdfs文件 warehouse 和 tmp"></a>5、创建hdfs文件 warehouse 和 tmp</h2><ul><li>在hdfs上创建 /user/hive/warehouse  和 /tmp 两个目录，并修改group权限w</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /user/hive/warehouse</span><br><span class="line">hdfs dfs -mkdir /tmp</span><br><span class="line">hdfs dfs -chmod g+w /user/hive/warehouse /tmp</span><br></pre></td></tr></table></figure><h2 id="6、浏览器查看hdfs文件"><a href="#6、浏览器查看hdfs文件" class="headerlink" title="6、浏览器查看hdfs文件"></a>6、浏览器查看hdfs文件</h2><figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">192.168.10.101:50070</span></span><br></pre></td></tr></table></figure><h2 id="7、配置环境变量"><a href="#7、配置环境变量" class="headerlink" title="7、配置环境变量"></a>7、配置环境变量</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HIVE_HOME=/home/soft/hive</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$&#123;HIVE_HOME&#125;</span>/bin</span><br></pre></td></tr></table></figure><h2 id="8、启动Hive"><a href="#8、启动Hive" class="headerlink" title="8、启动Hive,"></a>8、启动Hive,</h2><ul><li>在一个目录下启动 =&gt;  hive</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 启动Hive</span><br><span class="line">hive</span><br><span class="line"># 用库，建表，插数据，查看表信息，查看数据</span><br><span class="line">use default;</span><br><span class="line">show tables;</span><br><span class="line">create table stu(id int,name string);</span><br><span class="line">desc stu;</span><br><span class="line">insert into stu values(1,&#39;zs&#39;);</span><br><span class="line">select * from stu;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="9、若启动报错"><a href="#9、若启动报错" class="headerlink" title="9、若启动报错"></a>9、若启动报错</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">无法访问/home/soft/spark/spark-2.1.1/lib/spark-assembly-*.jar: 没有那个文件或目录</span><br><span class="line"></span><br><span class="line">原因：新版本的spark,hive没有及时支持更新.sprak2.0之前jar包在spark/lib/, 2.0之后在spark/jar/</span><br><span class="line"></span><br><span class="line">解决： 修改hive的启动脚本</span><br><span class="line">vim hive/bin/hive</span><br><span class="line"><span class="meta">#</span><span class="bash">sparkAssemblyPath=`ls <span class="variable">$&#123;SPARK_HOME&#125;</span>/lib/spark-assembly-*.jar`</span></span><br><span class="line">sparkAssemblyPath=`ls $&#123;SPARK_HOME&#125;/jars/*.jar`</span><br></pre></td></tr></table></figure><h2 id="10、安装配置mysql"><a href="#10、安装配置mysql" class="headerlink" title="10、安装配置mysql"></a>10、安装配置mysql</h2><ul><li>将hive的元数据信息配置到msyql, 要安装Mysql<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看是否安装mysql</span></span><br><span class="line">rpm -qa | grep mysql</span><br><span class="line"><span class="meta">#</span><span class="bash"> 若安装，则卸载</span></span><br><span class="line">rpm -e --nodeps `msyql....`</span><br><span class="line"><span class="meta">#</span><span class="bash"> 解压mysql.zip包</span></span><br><span class="line">unzip mysql...zip</span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装服务端和客户端</span></span><br><span class="line">rpm -ivh Mysql-server...</span><br><span class="line">rpm -ivh Myql-cli..</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动Mysql服务</span></span><br><span class="line">service mysql start</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看默认密码</span></span><br><span class="line">cat /root/.mysql_secret</span><br><span class="line"><span class="meta">#</span><span class="bash"> 登陆并修改密码</span></span><br><span class="line">mysql -uroot -pa3s54df3as4df</span><br><span class="line">set password=password(&#x27;123123&#x27;);</span><br><span class="line">use mysql;</span><br><span class="line">update user set password=password(&#x27;123123&#x27;);</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置root远程连接权限</span></span><br><span class="line">delete from user where user=&#x27;root&#x27; and host &lt;&gt; &#x27;localhost&#x27;;</span><br><span class="line">updata user set host=&#x27;%&#x27; where user=&#x27;root&#x27;;</span><br><span class="line">flush privilege;</span><br></pre></td></tr></table></figure></li></ul><h2 id="11、hive元数据配置到mysql"><a href="#11、hive元数据配置到mysql" class="headerlink" title="11、hive元数据配置到mysql"></a>11、hive元数据配置到mysql</h2><ul><li>配置完后，==记得重启虚拟机==，==重启后开启hadoop集群==就能用了</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 将ysql连接驱动拷贝到 hive/lib/</span></span><br><span class="line">cp mysql-connector-java-5.1.27-bin.jar /home/soft/hive/lib/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建conf/hive-site.xml,配置jdbc连接</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node1:3306/metastore?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>123123<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="12、多窗口测试"><a href="#12、多窗口测试" class="headerlink" title="12、多窗口测试"></a>12、多窗口测试</h2><ul><li>在不同目录下启动hive,查看内容是一致的</li></ul><h2 id="13、配置数据仓库位置"><a href="#13、配置数据仓库位置" class="headerlink" title="13、配置数据仓库位置"></a>13、配置数据仓库位置</h2><ul><li><p>==默认hive数据仓库位置==在hdfs的 /hive/user/warehouse/</p></li><li><p>==每创建一个数据库==就在warehouse下创建一个文件夹 *.db</p></li><li><p>==每创建一个表==就在.db文件夹下创建一个表名文件夹</p></li><li><p>==每插入一条数据==就在表名文件夹下写入序列化文件 000000_0</p></li><li><p>==默认数据库default==是没有default.db文件夹的，直接在warehouse下进行表文件夹的创建</p></li><li><p>==每load进来一个文件==就把文件复制到当前表下，==若是重名==，则load的文件改名为name_copy_1</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200528095354.png" alt="image-20200527114522997"></p></li><li><p>conf/hive-default.xml.template =&gt;  hive-default.xml,并添加一下信息</p></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="14、配置hive运行-日志目录"><a href="#14、配置hive运行-日志目录" class="headerlink" title="14、配置hive运行 日志目录"></a>14、配置hive运行<font color="#FF0000"> 日志目录</font></h2><ul><li>hive-exec-log4j.properties.template =&gt;  hive-exec-log4j.properties</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.log.dir=/opt/module/hive/logs</span><br></pre></td></tr></table></figure><h2 id="15、-配置显示当前数据库以及查询表的头信息"><a href="#15、-配置显示当前数据库以及查询表的头信息" class="headerlink" title="15、 配置显示当前数据库以及查询表的头信息"></a>15、 配置显示当前数据库以及查询表的头信息</h2><ul><li>hive-site.xml</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h1 id="3-hive命令"><a href="#3-hive命令" class="headerlink" title="3. hive命令"></a>3. hive命令</h1><h2 id="0、帮助"><a href="#0、帮助" class="headerlink" title="0、帮助"></a>0、帮助</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive -help</span><br></pre></td></tr></table></figure><h2 id="1、启动时加载初始化文件"><a href="#1、启动时加载初始化文件" class="headerlink" title="1、启动时加载初始化文件"></a>1、启动时加载初始化文件</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim a.txt </span><br><span class="line">add jar &#x27;xxx.jar&#x27;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive -i &#x27;a.txt&#x27;</span><br></pre></td></tr></table></figure><h2 id="2、不进入交互窗口执行sql"><a href="#2、不进入交互窗口执行sql" class="headerlink" title="2、不进入交互窗口执行sql"></a>2、不进入交互窗口执行sql</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive -e &quot;select * from default.student;&quot;</span><br></pre></td></tr></table></figure><h2 id="3、执行sql脚本"><a href="#3、执行sql脚本" class="headerlink" title="3、执行sql脚本"></a>3、执行sql脚本</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive -f /home/datas/stu.sql </span><br></pre></td></tr></table></figure><h2 id="4、执行脚本并将结果保存到文件"><a href="#4、执行脚本并将结果保存到文件" class="headerlink" title="4、执行脚本并将结果保存到文件"></a>4、执行脚本并将结果保存到文件</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive -f /home/datas/stu.sql &gt; /home/stu.txt</span><br></pre></td></tr></table></figure><h2 id="5、-hive客户端下查看hdfs"><a href="#5、-hive客户端下查看hdfs" class="headerlink" title="5、 hive客户端下查看hdfs"></a>5、 hive客户端下查看hdfs</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfs -ls /</span><br></pre></td></tr></table></figure><h2 id="6、-hive客户端下查看本地系统"><a href="#6、-hive客户端下查看本地系统" class="headerlink" title="6、 hive客户端下查看本地系统"></a>6、 hive客户端下查看本地系统</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">! ls /home/</span><br></pre></td></tr></table></figure><h2 id="7、查看hive所有历史命令"><a href="#7、查看hive所有历史命令" class="headerlink" title="7、查看hive所有历史命令"></a>7、查看hive所有历史命令</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /root/.hivehistory </span><br></pre></td></tr></table></figure><h2 id="8、查看并运行最后一次的hive命令"><a href="#8、查看并运行最后一次的hive命令" class="headerlink" title="8、查看并运行最后一次的hive命令"></a>8、查看并运行最后一次的hive命令</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!hive</span><br></pre></td></tr></table></figure><h2 id="9、静默启动-详细启动"><a href="#9、静默启动-详细启动" class="headerlink" title="9、静默启动/ 详细启动"></a>9、静默启动/ 详细启动</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive [-S] [-v]</span><br></pre></td></tr></table></figure><h2 id="10、启动时直接指定数据库"><a href="#10、启动时直接指定数据库" class="headerlink" title="10、启动时直接指定数据库"></a>10、启动时直接指定数据库</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --database mydb1</span><br></pre></td></tr></table></figure><h2 id="12、beeline客户端"><a href="#12、beeline客户端" class="headerlink" title="12、beeline客户端"></a>12、beeline客户端</h2><ul><li>hive可以通过beeline借助于jdbc连接hive服务端</li><li>启动后加==&amp;==是指==后台运行==，不显示服务界面</li><li>beeline语法： ==以  ! 开头==</li><li>beeline是hive1.0之后才有的，所以jdbc连接时是 jdbc:==hive2==, 而mysql连接jdbc时 jdbc:mysql,对比记忆</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1. 启动hive服务端</span></span><br><span class="line">hiveserver2 start &amp;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2. 查看是否开启成功</span></span><br><span class="line">netstat -antp  # 有一个0:0:0:0:10000端口的程序  </span><br><span class="line">jps # 有RunJar程序</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3. 启动beeline,连接服务端 </span></span><br><span class="line">beeline</span><br><span class="line">!connect jdbc:hive2://node3:10000 # (hive1.0之后才有的,所以是hive2) </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4. 测试</span></span><br><span class="line">show databases;</span><br></pre></td></tr></table></figure><ul><li><p>连接时==错误==</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> 0: jdbc:hive2://node3:10000 (closed)&gt; !close</span><br><span class="line">Permission denied: user=anonymous,access=EXECUTE,inode=&quot;/tmp/hive&quot;:root:supergroup:drwx------at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)</span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash"> 原因  hive没有/tmp的权限</span></span><br><span class="line"><span class="meta"> #</span><span class="bash"> 解决  hdfs dfs -chmod 777 /tmp</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="13、-三种-方式访问hive"><a href="#13、-三种-方式访问hive" class="headerlink" title="13、==三种==方式访问hive"></a>13、==三种==方式访问hive</h2><ol><li>JDBC/UDBC</li><li>CUI</li><li>WebUI</li></ol><h2 id="14、-hive服务端-开启、关闭"><a href="#14、-hive服务端-开启、关闭" class="headerlink" title="14、 hive服务端==开启、关闭=="></a>14、 hive服务端==开启、关闭==</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开启</span></span><br><span class="line">hiveserver2 start &amp;</span><br><span class="line">netstat -antp # 查看是否有:10000端口的进程</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 关闭</span></span><br><span class="line">ps -ef | grep hiveserver2</span><br><span class="line">netstat -antp # 查看10000端口Pid</span><br><span class="line">kill -9 pid</span><br></pre></td></tr></table></figure><h2 id="15、-hive-web-UI-2-0之后有的hiveserver2-webui"><a href="#15、-hive-web-UI-2-0之后有的hiveserver2-webui" class="headerlink" title="15、 hive-==web==UI(2.0之后有的hiveserver2-webui)"></a>15、 hive-==web==UI(2.0之后有的hiveserver2-webui)</h2><h3 id="1-下载hive-src源码包"><a href="#1-下载hive-src源码包" class="headerlink" title="1. 下载hive-src源码包"></a>1. 下载hive-src源码包</h3><ul><li>下载 apache-hive-1.2.1-src-master.zip</li><li>下载完上传到/opt/software/</li></ul><h3 id="2-解压"><a href="#2-解压" class="headerlink" title="2.  解压"></a>2.  解压</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 解压 --&gt;</span></span><br><span class="line">unzip apache-hive-1.2.1-src-master.zip</span><br></pre></td></tr></table></figure><h3 id="3-打war包并复制到hive-lib"><a href="#3-打war包并复制到hive-lib" class="headerlink" title="3. 打war包并复制到hive/lib/"></a>3. 打war包并复制到hive/lib/</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">jar cfM hive-hwi-1.2.1.war -C  web/ .</span><br><span class="line">    cp hive-hwi-1.2.1.war /opt/module/hive/lib/</span><br></pre></td></tr></table></figure><h3 id="4-复制相关jar到lib"><a href="#4-复制相关jar到lib" class="headerlink" title="4. 复制相关jar到lib/"></a>4. 复制相关jar到lib/</h3><ul><li>tools.jar    java/lib</li><li>jasper-compiler-5.5.23.jar</li><li>jasper-runtime-5.5.23.jar</li><li>commons-el-1.0.jar</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find / -name &#x27;tools*&#x27; # 使用find命令查找后复制到hive/lib/</span><br></pre></td></tr></table></figure><h3 id="5-配置conf-hive-site-xml"><a href="#5-配置conf-hive-site-xml" class="headerlink" title="5.  配置conf/hive-site.xml"></a>5.  配置conf/hive-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.hwi.listen.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.0.0.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">        This is the host address the Hive Web Interface will listen on</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.hwi.listen.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>9999<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>This is the port the Hive Web Interface will listen on<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.hwi.war.file<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lib/hive-hwi-1.2.1.war<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">        This sets the path to the HWI war file, relative to $&#123;HIVE_HOME&#125;. </span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="5-启动hive-UI服务"><a href="#5-启动hive-UI服务" class="headerlink" title="5. 启动hive-UI服务"></a>5. 启动hive-UI服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --service hwi</span><br></pre></td></tr></table></figure><h3 id="6-测试"><a href="#6-测试" class="headerlink" title="6. 测试"></a>6. 测试</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">192.168.10.101:9999/hwi</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200528095355.png" alt="image-20200528095348223"></p><h2 id="16、-查看表详细信息"><a href="#16、-查看表详细信息" class="headerlink" title="16、 查看表详细信息"></a>16、 查看表详细信息</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc formatted tableName;</span><br></pre></td></tr></table></figure><h2 id="17、创建-拓展-表-外部表"><a href="#17、创建-拓展-表-外部表" class="headerlink" title="17、创建==拓展==表[外部表]"></a>17、创建==拓展==表[外部表]</h2><ul><li>拓展表删除只删元数据</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create external table xing1(id int); # 默认是管理表【内部表】</span><br></pre></td></tr></table></figure><h2 id="18、-分区-模式和-运行-模式"><a href="#18、-分区-模式和-运行-模式" class="headerlink" title="18、==分区==模式和==运行==模式"></a>18、==分区==模式和==运行==模式</h2><ul><li>运行模式<ul><li>设置 set hive.mapred.mode = strict   </li><li>严格模式： 查询必须加条件，排序必须加limit</li><li>非严格模式[默认]</li></ul></li><li>分区模式<ul><li>设置 set hive.dynamit.partition.mode = nonstrict</li><li>严格模式[默认] ： 不能纯动态分区，第一个必须是静态分区</li><li>非严格模式： 可以纯动态分区</li></ul></li></ul><h2 id="19、-分区"><a href="#19、-分区" class="headerlink" title="19、 分区"></a>19、 分区</h2><ul><li><p>创建分区表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create table p1(   </span><br><span class="line">id int,                                                                               </span><br><span class="line">name String   </span><br><span class="line">)                                                                                     partitioned by (clazz String,gender String); #这一行的意思是:这是一个分区表clazz嵌套着gender</span><br></pre></td></tr></table></figure></li><li><p>添加分区字段</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table p1 add partition(clazz&#x3D;&#39;101&#39;,gender&#x3D;&#39;man&#39;);</span><br></pre></td></tr></table></figure></li><li><p>load数据【静态分区】</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &#39;&#x2F;root&#x2F;p&#39; into table p1 partition (clazz&#x3D;&#39;101&#39;,gender&#x3D;&#39;man&#39;);</span><br></pre></td></tr></table></figure></li><li><p>insert 数据【静态分区】</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into p1 partition(clazz&#x3D;&#39;101&#39;,gender&#x3D;&#39;man&#39;) select id,name from p where clazz&#x3D;&#39;101&#39; and gender&#x3D;&#39;man&#39;;</span><br></pre></td></tr></table></figure></li><li><p>load数据动态分区【动态】，要保证加载的文件是多了两列的，若没有则NULL</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 设置分区模式为非静态</span><br><span class="line">set hive.dynamit.partition.mode &#x3D; nonstrict</span><br><span class="line">load data local inpath &#39;&#x2F;root&#x2F;p&#39; into table p1 partition (clazz,gender);</span><br></pre></td></tr></table></figure></li></ul><h2 id="20、-分桶"><a href="#20、-分桶" class="headerlink" title="20、 分桶"></a>20、 分桶</h2><ul><li>分桶字段要出现在表的定义字段里,mysql元数据里没这个东西</li><li>通过哈希值取余分桶数来分桶</li><li>不能加载数据,可以从另一张表里导数据 若覆盖: insert overwrite table b1 select * from test1;</li><li>默认不开启分桶,需设置强制分桶属性 set hive.enforce.bucketing=true;</li><li>分区是提取目录,分目录,而要把文件更细分,要用到分桶规则,划分为多个文件,提高查询效率</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 案例</span><br><span class="line">create table b1(</span><br><span class="line">id int,</span><br><span class="line">name String,</span><br><span class="line">clazz String</span><br><span class="line">)</span><br><span class="line">clustered by(id) into 3 buckets</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;;</span><br><span class="line"></span><br><span class="line">create table test1(</span><br><span class="line">id int,</span><br><span class="line">name String,</span><br><span class="line">clazz String</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;;</span><br><span class="line"></span><br><span class="line">load data local inpath &#39;&#x2F;root&#x2F;b1&#39; into table test1;</span><br><span class="line">insert overwrite table b1 select * from test1; </span><br></pre></td></tr></table></figure><ul><li>分区分桶结合</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">create table partitionbucket1(</span><br><span class="line">id int,</span><br><span class="line">name String</span><br><span class="line">)</span><br><span class="line">partitioned by (gender String)</span><br><span class="line">clustered by (id) into 3 buckets</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">stored as textfile;</span><br><span class="line"></span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">set hive.enforce.bucketing=true;</span><br><span class="line"></span><br><span class="line">insert into partitionbucket1 partition(gender) select id,name,&#x27;man&#x27; from test1;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="21、-数据类型"><a href="#21、-数据类型" class="headerlink" title="21、 数据类型"></a>21、 数据类型</h2><ul><li><p>创建表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create external table if not exists  xing2(</span><br><span class="line">id int comment&#39;this is id&#39;,</span><br><span class="line">name String comment&#39;this is name&#39;,</span><br><span class="line">hobby array&lt;String&gt; comment&#39;this is hobby&#39;,</span><br><span class="line">parents map&lt;String,String&gt; comment&#39;ex:dad:curry,mom:stephen&#39;,</span><br><span class="line">info struct&lt;age:int,address:String&gt;</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;</span><br><span class="line">collection items terminated by &#39;,&#39;</span><br><span class="line">map keys terminated by &#39;:&#39;</span><br><span class="line">location &#39;&#x2F;db-xing2&#39;;;</span><br></pre></td></tr></table></figure></li><li><p>创建数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;1curryeat,drink,lookdad:30,mom:3133,冉口&quot; &gt; /db-xing2</span><br></pre></td></tr></table></figure></li><li><p>加载数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &#39;&#x2F;root&#x2F;db-xing2&#39; overwrite into table xing2;</span><br></pre></td></tr></table></figure></li><li><p>查询数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from xing2 select name,hobby[1],parents[&quot;dad&quot;],info.age;</span><br></pre></td></tr></table></figure></li><li><p>结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curry   drink   30      33</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 案例二</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 建表</span></span><br><span class="line">create external table t1(</span><br><span class="line">id int,</span><br><span class="line">hobby array&lt;String&gt;,</span><br><span class="line">parents map&lt;String,String&gt;,</span><br><span class="line">tag array&lt;struct&lt;id:int,price:String&gt;&gt;</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">collection items terminated by &#x27;,&#x27;</span><br><span class="line">map keys terminated by &#x27;:&#x27;</span><br><span class="line">location &#x27;/myhive&#x27;</span><br><span class="line">;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建文件 a.txt</span></span><br><span class="line">1eat,drink,fuckdad:ke,mom:hua1:11111,2:22222</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 加载数据</span></span><br><span class="line">load data local inpath &#x27;/root/a.txt&#x27; overwrite into table t1;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看</span></span><br><span class="line">t3.idt3.hobbyt3.parentst3.tag</span><br><span class="line">1[&quot;eat&quot;,&quot;drink&quot;,&quot;fuck&quot;]&#123;&quot;dad&quot;:&quot;ke&quot;,&quot;mom&quot;:&quot;hua&quot;&#125;[&#123;&quot;id&quot;:1,&quot;price&quot;:&quot;11111&quot;&#125;,&#123;&quot;id&quot;:2,&quot;price&quot;:&quot;22222&quot;&#125;]</span><br></pre></td></tr></table></figure><h2 id="22、综合案例"><a href="#22、综合案例" class="headerlink" title="22、综合案例"></a>22、综合案例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">create table a(</span><br><span class="line">id int,</span><br><span class="line">name String</span><br><span class="line">)</span><br><span class="line">comment &#39;THIS TABLE IS PERSON_INFO&#39;</span><br><span class="line">partitioned by (sex String) # 分区, 分区字段不能出现在定义字段里</span><br><span class="line">clustered by (id) into 3 buckets # 分桶，分桶字段要定义在定义表字段里</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39; # 字段分隔符 默认^A</span><br><span class="line">lines terminated by &#39;\n&#39; # 行分割符 </span><br><span class="line">collection items terminated by &#39;,&#39;  # 集合项分隔符 默认 ^B</span><br><span class="line">map keys terminated by &#39;:&#39; # 键值对分隔符 默认 ^C</span><br><span class="line">stored as textfile # 保存文件的类型</span><br><span class="line">location &#39;&#x2F;myhive&#39; # 执行数据保存在hdfs上的位置，默认 &#x2F;user&#x2F;hive&#x2F;ware&#x2F;hourse</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">create table b(</span><br><span class="line">id int ,</span><br><span class="line">name String,</span><br><span class="line">sex String</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">insert into b values(1,&#39;a&#39;,&#39;man&#39;),(2,&#39;b&#39;,&#39;women&#39;),(3,&#39;c&#39;,&#39;man&#39;);</span><br><span class="line"># 分桶要启动多个Reduce任务</span><br><span class="line">set hive.exec.mode.local.auto&#x3D;false; </span><br><span class="line"># 纯动态分区，默认分区模式是非严格模式，支持动态分区，但要保证第一个是静态分区，也就是第一个要指定值，若都不指定值，自动判断的话，需要将分区模式改为非严格模式</span><br><span class="line">set hive.exec.dynamic.partition.mode&#x3D;nonstrict</span><br><span class="line"></span><br><span class="line"># 开启分桶，默认关闭</span><br><span class="line">set hive.enforce.bucketing&#x3D;true;</span><br><span class="line">insert overwrite table a partiton (sex) select * from b;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200528095356.gif"></p><h2 id="23、抽样"><a href="#23、抽样" class="headerlink" title="23、抽样"></a>23、抽样</h2><h3 id="1、随机抽样"><a href="#1、随机抽样" class="headerlink" title="1、随机抽样"></a>1、随机抽样</h3><ul><li>通过随机函数分发到不同的reduce上，再通过随机数排序，limit条数</li><li>下面的语句只有表名和Limit改下就能用</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from test1 distribute by random() sort by random() limit 5;</span><br></pre></td></tr></table></figure><h3 id="2、桶表抽样"><a href="#2、桶表抽样" class="headerlink" title="2、桶表抽样"></a>2、桶表抽样</h3><ul><li>若表本身是分桶表，可以不指定分桶字段，若要按照其他字段分桶，需指定</li><li>若表本身不是分桶表，需指定分桶规则</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 分桶表抽样(定义时按照Id分桶) ,取出第二桶</span><br><span class="line">select * from b1 tablesample(bucket 2 out of 3)</span><br><span class="line"># 分桶表抽样,按照name字段取模分桶，分3桶，取第一桶，</span><br><span class="line">select * from test1 tablesample(bucket 1 out of bucket 3 on name);</span><br><span class="line"># 非分桶表抽样, 通过id分桶，分三桶，取第一桶</span><br><span class="line">select * from b1 tablesample(bucket 1 out of 3 on id)</span><br></pre></td></tr></table></figure><h3 id="3、块抽样"><a href="#3、块抽样" class="headerlink" title="3、块抽样"></a>3、块抽样</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 抽前几行</span><br><span class="line">select * from test1 tablesample(10 rows);</span><br><span class="line"># 抽百分之几</span><br><span class="line">select * from test1 tablesample(10 percent);</span><br><span class="line"># 抽多大内存</span><br><span class="line">select * from test1 tablesample(1k);</span><br></pre></td></tr></table></figure><h2 id="24、-存储类型"><a href="#24、-存储类型" class="headerlink" title="24、 存储类型"></a>24、 存储类型</h2><h3 id="1、分类"><a href="#1、分类" class="headerlink" title="1、分类"></a>1、分类</h3><ul><li><p>sequencefile </p><ul><li>这种存储方式不能以加载数据的形式导入数据,只能从一张元数据表中insert数据</li><li>这是hadoop提供的一种存储类型，对每行加一个key</li></ul></li><li><p>rcfile</p><ul><li>按列存储的数据格式: 将原来的行字段变为列字段，每行存所有相同字段数据 </li><li>例如： 第一行：id  1 2 3 4   第二行 name a b c d  ,有利于数据的读取</li></ul></li><li><p>orc</p><ul><li>是rcfile的优化版本，面向列存储</li></ul></li><li><p>==parquet==</p><ul><li>==hive主推的存储格式==</li><li>面向列的分析性的存储格式，对每列有描述信息</li></ul></li></ul><h3 id="2、注意"><a href="#2、注意" class="headerlink" title="2、注意"></a>2、注意</h3><ul><li>==parquet==,orc,orfile 都是hadoop中==面向列==的储存格式,都以==写入慢==为代价,==提高读取==性能</li></ul><h3 id="3、描述"><a href="#3、描述" class="headerlink" title="3、描述"></a>3、描述</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># textFile</span><br><span class="line">id name sex</span><br><span class="line">1awomen</span><br><span class="line">2bman</span><br><span class="line"># qequencefile 加key</span><br><span class="line">id name sex</span><br><span class="line">11awomen</span><br><span class="line">22bman</span><br><span class="line"># rcfile 面向列</span><br><span class="line">id12</span><br><span class="line">nameab</span><br><span class="line">sexwomenman</span><br></pre></td></tr></table></figure><h3 id="4、-使用"><a href="#4、-使用" class="headerlink" title="4、 使用"></a>4、 使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">create table test2(</span><br><span class="line">id int,</span><br><span class="line">name String,</span><br><span class="line">class String</span><br><span class="line">)</span><br><span class="line">stored as sequencefile;   # 这种存储方式不能以加载数据的形式导入数据,只能从一张元数据表中insert数据</span><br><span class="line"></span><br><span class="line">insert into test2 select * from test1;</span><br></pre></td></tr></table></figure><h2 id="25、索引"><a href="#25、索引" class="headerlink" title="25、索引"></a>25、索引</h2><h3 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h3><ul><li>和mysql索引一样</li><li>索引是一种快速查询和排序的数据类型</li></ul><h3 id="2、注意-1"><a href="#2、注意-1" class="headerlink" title="2、注意"></a>2、注意</h3><ul><li>都是普通索引，没有主外键一说，尽可能插入数据时就保证索引列唯一</li></ul><h3 id="3、使用"><a href="#3、使用" class="headerlink" title="3、使用"></a>3、使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 创建规则</span><br><span class="line">create index index_name on table table_name(col_name)</span><br><span class="line">as &#39;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&#39;</span><br><span class="line">with deferred rebuild</span><br><span class="line">[IN TABLE index_table_name];</span><br><span class="line"></span><br><span class="line"># 创建</span><br><span class="line">create index index_name on table table_name(id)</span><br><span class="line">as &#39;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&#39;</span><br><span class="line">with deferred rebuild in table index_table_name; # 这里创建索引维护表</span><br><span class="line"></span><br><span class="line"># 索引维护表: 默认是没东西的,需要重构</span><br><span class="line"># 重构</span><br><span class="line">alter index index_name on table_name rebuild;</span><br><span class="line">select * from index_table_name; </span><br><span class="line"></span><br><span class="line"># 查看索引</span><br><span class="line">show index on table_name;</span><br><span class="line"># 删除索引</span><br><span class="line">drop index index_name on table_name;</span><br></pre></td></tr></table></figure><h2 id="26、排序"><a href="#26、排序" class="headerlink" title="26、排序"></a>26、排序</h2><ul><li><p>order by </p><ul><li>全局排序，只有一个reudce任务</li></ul></li><li><p>sort by </p><ul><li>对每个reduce内的数据进行排序，只保证每个reduce内部有序</li><li>set mapreduce.job.reduces=3;数据分成3个reducejob运行,保证每个内部有序</li><li>select * from test1 sort by id; 每个reduce内通过id排序</li></ul></li><li><p>distribute by</p><ul><li>先指定分发规则在排序</li><li>set mapreduce.job.reduces=3;  设置分发规则</li><li>select * from test1 distribute by id sort by id;  分发后指定字段排序</li></ul></li><li><p>clusterby = cluster by + sort by</p><ul><li>默认升序,不能指定排序规则，通过谁分发就通过谁排序，默认正序</li><li>select * from test1 clusterby by id  desc;   相当于 distribute by id sort by id;  </li></ul></li></ul><h1 id="4-函数"><a href="#4-函数" class="headerlink" title="4. 函数"></a>4. <a href="./Hive%E5%87%BD%E6%95%B0%E5%A4%A7%E5%85%A8.pdf">函数</a></h1><ol><li>查看所有函数 show functions;</li><li>查看某个函数 desc function concat;</li><li>查看某个函数详细信息  desc function extended concat; (带有例子)</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 加1后取反</span></span><br><span class="line">select ~100 </span><br><span class="line"><span class="meta">#</span><span class="bash"> 字符串替换</span></span><br><span class="line">select regexp_replace(&#x27;i have a dream&#x27;,&#x27;([^ ])a&#x27;,&#x27;$1**&#x27;)</span><br></pre></td></tr></table></figure><ol start="4"><li>wordcount</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 创建库</span><br><span class="line">use default;</span><br><span class="line">create table wc(str String);</span><br><span class="line"># 造数据上传到表</span><br><span class="line">vim a  ....</span><br><span class="line">hdfs dfs -put a &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;wc&#x2F;</span><br><span class="line"># 使用函数 实现wordcount</span><br><span class="line"># split(str,分隔符); str &#x3D;&gt; array</span><br><span class="line"># explode(array); array &#x3D;&gt; 将每个单元编程一行，展平</span><br><span class="line">select t1.str,sum(1) count </span><br><span class="line">from (select explode(split(str,&#39; &#39;)) str from wc) t1 </span><br><span class="line">group by t1.str order by count;</span><br></pre></td></tr></table></figure><p>5.临时结果集</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">with temp as (select * from a)</span><br><span class="line">select * from temp group by id;</span><br></pre></td></tr></table></figure><h1 id="5-开窗函数"><a href="#5-开窗函数" class="headerlink" title="5.  开窗函数"></a>5.  <a href="./Hive(3)%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0.md">开窗函数</a></h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><ul><li><p>一般分为聚合和排序两种</p></li><li><p>普通的聚合函数只返回一个值，开窗函数为每一行都返回一个值</p></li><li><p>count</p></li><li><p>sum</p></li><li><p>min/max</p></li><li><p>avg</p></li><li><p>first_value/last_value</p></li><li><p>lag/lead(列名，第n行,默认值) 向上、下第n个值</p></li><li><p>cume_dist 百分比=小于等于当前值/总行数</p></li></ul><ul><li>rank 排名，不连续</li><li>dense_rank 排名，连续</li><li>ntile(组数) 分组，组数</li><li>row_number 排序，从1开始</li><li>percent_rank 超过的百分比 =(当前行rank值-1)/(总行-1)</li></ul><h3 id="使用举例"><a href="#使用举例" class="headerlink" title="使用举例"></a>使用举例</h3><ol><li><p>求成绩排名，成绩在全校的百分比超过了多少</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">idscorerankpercent_rank</span><br><span class="line">188</span><br><span class="line">299</span><br><span class="line">377</span><br><span class="line">466</span><br><span class="line">5100</span><br><span class="line">623</span><br><span class="line">732</span><br><span class="line">867</span><br><span class="line">977</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select id,score,</span><br><span class="line">rank() over(order by score) rank,</span><br><span class="line">percent_rank() over(order by score) precent_rank</span><br><span class="line">from res; </span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200528180135.png" alt="image-20200528180127453"></p></li></ol><h1 id="6-自定义函数"><a href="#6-自定义函数" class="headerlink" title="6. 自定义函数"></a>6. 自定义函数</h1><ol><li><p>创建maven项目，导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>2.7.1<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-jobclient<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.8.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>$&#123;project.build.sourceEncoding&#125;<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>创建类继承hive的UDF类</p></li><li><p>重写执行方法[可重载] evaluate</p></li><li><p>添加注解[介绍和实例]</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.Description;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Description(</span></span><br><span class="line"><span class="meta">value = &quot;_FUNC_(xxx1,xxx2.....) - from xxx &quot; + </span></span><br><span class="line"><span class="meta">        &quot;returns you result.....&quot;,</span></span><br><span class="line"><span class="meta">extended = &quot;Example:\n&quot; +</span></span><br><span class="line"><span class="meta">            &quot; &gt; SELECT _FUNC_(xxx1,xxx2.....) FROM src;&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestUDF01</span> <span class="keyword">extends</span> <span class="title">UDF</span></span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">&quot;hello use defined function&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> Integer <span class="title">evaluate</span><span class="params">(Integer...nums)</span></span>&#123;</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (Integer i : nums) &#123;</span><br><span class="line">sum += i;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><ol start="5"><li><p>打包，上传</p></li><li><p>将jar包添加linux环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar xxx.jar</span><br></pre></td></tr></table></figure></li><li><p>创建临时函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create temporary function udf1 as &#39;udf.MyUdf01&#39;;</span><br></pre></td></tr></table></figure></li><li><p>使用 select udf1()</p></li></ol><h1 id="x-异常"><a href="#x-异常" class="headerlink" title="==x. 异常=="></a>==x. 异常==</h1><ol><li><p>用beeline客户端连接hive时： ==org.apache.hadoop.security.authorize.AuthorizationException==</p><ul><li><p>hadoop2.0之后，默认只允许root用户访问,若其他用户访问，需要配置代理，hadoop/core-site.xml</p></li><li><p>修改之后，重启hadoop集群，重启hiverserver2和metastore</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.xing.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.xing.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-hive简介&quot;&gt;&lt;a href=&quot;#1-hive简介&quot; class=&quot;headerlink&quot; title=&quot;1. hive简介&quot;&gt;&lt;/a&gt;1. hive简介&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;官网 &lt;a href=&quot;http://hive.apache.org/&quot;&gt;http://hive.apache.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;文档 &lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/GettingStarted&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/GettingStarted&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;下载 &lt;a href=&quot;http://archive.apache.org/dist/hive/&quot;&gt;http://archive.apache.org/dist/hive/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;github &lt;a href=&quot;https://github.com/apache/hive&quot;&gt;https://github.com/apache/hive&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Hive" scheme="http://iscurry.com/categories/Hive/"/>
    
    
    <category term="Hive" scheme="http://iscurry.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive(2)详细</title>
    <link href="http://iscurry.com/2020/01/01/Hive(2)%E8%AF%A6%E7%BB%86/"/>
    <id>http://iscurry.com/2020/01/01/Hive(2)%E8%AF%A6%E7%BB%86/</id>
    <published>2020-01-01T11:20:20.000Z</published>
    <updated>2020-09-25T02:17:41.428Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第1章-Hive入门"><a href="#第1章-Hive入门" class="headerlink" title="第1章 Hive入门"></a>第1章 Hive入门</h1><h2 id="1-1-什么是Hive"><a href="#1-1-什么是Hive" class="headerlink" title="1.1 什么是Hive"></a>1.1 什么是Hive</h2><p>Hive：由Facebook开源用于解决海量结构化日志的数据统计。</p><p>Hive是基于Hadoop的一个==<strong>数据仓库工具</strong>==，可以将==<strong>结构化的数据文件映射为一张表</strong>==，并提供==<strong>类SQL</strong>==查询功能。</p><p>==<strong>本质是：将HQL转化成MapReduce程序</strong>==</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113926.png" alt="image-20200917232726291"></p><a id="more"></a><p>1）Hive处理的数据存储在HDFS</p><p>2）Hive分析数据底层的实现是MapReduce</p><p>3）执行程序运行在Yarn上</p><h2 id="1-2-Hive的优缺点"><a href="#1-2-Hive的优缺点" class="headerlink" title="1.2 Hive的优缺点"></a>1.2 Hive的优缺点</h2><h3 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h3><ol><li><p>操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）。</p></li><li><p>避免了去写MapReduce，减少开发人员的学习成本。</p></li><li><p>Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合。</p></li><li><p>Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高。</p></li><li><p>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。</p></li></ol><h3 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h3><p>1．Hive的HQL表达能力有限</p><blockquote><p>  （1）迭代式算法无法表达</p></blockquote><blockquote><p>  （2）数据挖掘方面不擅长</p></blockquote><p>2．Hive的效率比较低</p><blockquote><p>  （1）Hive自动生成的MapReduce作业，通常情况下不够智能化</p></blockquote><blockquote><p>  （2）Hive调优比较困难，粒度较粗</p></blockquote><h2 id="1-3-Hive架构原理"><a href="#1-3-Hive架构原理" class="headerlink" title="1.3 Hive架构原理"></a>1.3 Hive架构原理</h2><p>Hive架构原理</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113927.png" alt="image-20200917233006917"></p><p>1．用户接口：ClientCLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问hive）</p><p>2．元数据：Metastore</p><blockquote><p>  元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；</p></blockquote><blockquote><p>  <strong>==默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore==</strong></p></blockquote><p>3．Hadoop</p><blockquote><p>  使用HDFS进行存储，使用MapReduce进行计算。</p></blockquote><p>4．驱动器：Driver</p><blockquote><p>  （1）解析器（SQLParser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</p></blockquote><blockquote><p>  （2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。</p></blockquote><blockquote><p>  （3）优化器（Query Optimizer）：对逻辑执行计划进行优化。</p></blockquote><blockquote><p>（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。</p></blockquote><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113928.png" alt="image-20200917233230956"></p><p>Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。</p><h2 id="1-4-Hive和数据库比较"><a href="#1-4-Hive和数据库比较" class="headerlink" title="1.4 Hive和数据库比较"></a>1.4 Hive和数据库比较</h2><p>由于 Hive 采用了类似SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive理解为数据库。其实从结构上来看，Hive和数据库除了拥有类似的查询语言，再无类似之处。本文将从多个方面来阐述 Hive和数据库的差异。数据库可以用在 Online 的应用中，但是Hive是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。</p><h3 id="1-4-1-查询语言"><a href="#1-4-1-查询语言" class="headerlink" title="1.4.1 查询语言"></a>1.4.1 查询语言</h3><p>由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发。</p><h3 id="1-4-2-数据存储位置"><a href="#1-4-2-数据存储位置" class="headerlink" title="1.4.2 数据存储位置"></a>1.4.2 数据存储位置</h3><p>Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS中的。而数据库则可以将数据保存在块设备或者本地文件系统中。</p><h3 id="1-4-3-数据更新"><a href="#1-4-3-数据更新" class="headerlink" title="1.4.3 数据更新"></a>1.4.3 数据更新</h3><p>由于Hive是针对数据仓库应用设计的，而**==数据仓库的内容是读多写少的==<strong>。因此，</strong>==Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的==**。而数据库中的数据通常是需要经常进行修改的，因此可以使用<br>INSERT INTO … VALUES 添加数据，使用 UPDATE … SET修改数据。</p><h3 id="1-4-4-索引"><a href="#1-4-4-索引" class="headerlink" title="1.4.4 索引"></a>1.4.4 索引</h3><p>Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。Hive要访问数据中满足条件的特定值时，需要**==暴力扫描整个数据==**，因此访问延迟较高。由于MapReduce 的引入， Hive可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了Hive 不适合在线数据查询。</p><h3 id="1-4-5-执行"><a href="#1-4-5-执行" class="headerlink" title="1.4.5 执行"></a>1.4.5 执行</h3><p>Hive中大多数查询的执行是通过 Hadoop 提供的 MapReduce来实现的。而数据库通常有自己的执行引擎。执行引擎还有Tez,Spark等等</p><h3 id="1-4-6-执行延迟"><a href="#1-4-6-执行延迟" class="headerlink" title="1.4.6 执行延迟"></a>1.4.6 执行延迟</h3><p>Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce本身具有较高的延迟，因此在利用MapReduce执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。</p><h3 id="1-4-7-可扩展性"><a href="#1-4-7-可扩展性" class="headerlink" title="1.4.7 可扩展性"></a>1.4.7 可扩展性</h3><p>由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的（世界上最大Hadoop集群在 Yahoo!，2009年的规模在4000 台节点左右）。而数据库由于 ACID语义的严格限制，扩展行非常有限。目前最先进的并行数据库Oracle在理论上的扩展能力也只有100台左右。</p><h3 id="1-4-8-数据规模"><a href="#1-4-8-数据规模" class="headerlink" title="1.4.8 数据规模"></a>1.4.8 数据规模</h3><p>由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。</p><h1 id="第2章-Hive安装"><a href="#第2章-Hive安装" class="headerlink" title="第2章 Hive安装"></a>第2章 Hive安装</h1><h2 id="2-1-Hive安装地址"><a href="#2-1-Hive安装地址" class="headerlink" title="2.1 Hive安装地址"></a>2.1 Hive安装地址</h2><p>1．Hive官网地址</p><p><a href="http://hive.apache.org/">http://hive.apache.org/</a></p><p>2．文档查看地址</p><p><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></p><p>3．下载地址</p><p><a href="http://archive.apache.org/dist/hive/">http://archive.apache.org/dist/hive/</a></p><p>4．github地址</p><p><a href="https://github.com/apache/hive">https://github.com/apache/hive</a></p><h2 id="2-2-Hive安装部署"><a href="#2-2-Hive安装部署" class="headerlink" title="2.2 Hive安装部署"></a>2.2 Hive安装部署</h2><p>1．Hive安装及配置</p><blockquote><p>  （1）把apache-hive-1.2.1-bin.tar.gz上传到linux的/opt/software目录下</p></blockquote><blockquote><p>  （2）解压apache-hive-1.2.1-bin.tar.gz到/opt/module/目录下面</p></blockquote><blockquote><p>  [xing@hadoop102 software]$ tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/</p></blockquote><blockquote><p>  （3）修改apache-hive-1.2.1-bin.tar.gz的名称为hive</p></blockquote><blockquote><p>  [xing@hadoop102 module]$ mv apache-hive-1.2.1-bin/ hive</p></blockquote><blockquote><p>  （4）修改/opt/module/hive/conf目录下的hive-env.sh.template名称为hive-env.sh</p></blockquote><blockquote><p>  [xing@hadoop102 conf]$ mv hive-env.sh.template hive-env.sh</p></blockquote><p>（5）配置hive-env.sh文件</p><blockquote><p>  （a）配置HADOOP_HOME路径</p></blockquote><blockquote><p>  export HADOOP_HOME=/opt/module/hadoop-2.7.2</p></blockquote><blockquote><p>  （b）配置HIVE_CONF_DIR路径</p></blockquote><blockquote><p>  export HIVE_CONF_DIR=/opt/module/hive/conf</p></blockquote><p>2．Hadoop集群配置</p><p>（1）必须启动hdfs和yarn</p><blockquote><p>  [xing@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</p></blockquote><blockquote><p>  [xing@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</p></blockquote><p>（2）在HDFS上创建/tmp和/user/hive/warehouse两个目录并修改他们的同组权限可写</p><blockquote><p>  [xing@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir /tmp</p></blockquote><blockquote><p>  [xing@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir -p /user/hive/warehouse</p></blockquote><blockquote><p>  [xing@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /tmp</p></blockquote><blockquote><p>  [xing@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /user/hive/warehouse</p></blockquote><p>3．Hive基本操作</p><p>（1）启动hive</p><blockquote><p>  [xing@hadoop102 hive]$ bin/hive</p></blockquote><p>（2）查看数据库</p><blockquote><p>  hive&gt; show databases;</p></blockquote><p>（3）打开默认数据库</p><blockquote><p>  hive&gt; use default;</p></blockquote><p>（4）显示default数据库中的表</p><blockquote><p>  hive&gt; show tables;</p></blockquote><p>（5）创建一张表</p><blockquote><p>  hive&gt; create table student(id int, name string);</p></blockquote><p>（6）显示数据库中有几张表</p><blockquote><p>  hive&gt; show tables;</p></blockquote><p>（7）查看表的结构</p><blockquote><p>  hive&gt; desc student;</p></blockquote><p>（8）向表中插入数据</p><blockquote><p>  hive&gt; insert into student values(1000,”ss”);</p></blockquote><p>（9）查询表中数据</p><blockquote><p>  hive&gt; select * from student;</p></blockquote><p>（10）退出hive</p><blockquote><p>  hive&gt; quit;</p></blockquote><h2 id="2-3-将本地文件导入Hive案例"><a href="#2-3-将本地文件导入Hive案例" class="headerlink" title="2.3 将本地文件导入Hive案例"></a>2.3 将本地文件导入Hive案例</h2><p>需求</p><p>将本地/opt/module/datas/student.txt这个目录下的数据导入到hive的student(id int,name string)表中。</p><p>1．数据准备</p><blockquote><p>  在/opt/module/datas这个目录下准备数据</p></blockquote><blockquote><p>  （1）在/opt/module/目录下创建datas</p></blockquote><blockquote><p>  [xing@hadoop102 module]$ mkdir datas</p></blockquote><blockquote><p>  （2）在/opt/module/datas/目录下创建student.txt文件并添加数据</p></blockquote><blockquote><p>  [xing@hadoop102 datas]$ touch student.txt</p></blockquote><blockquote><p>  [xing@hadoop102 datas]$ vi student.txt</p></blockquote><blockquote><p>  1001 zhangshan</p></blockquote><blockquote><p>  1002 lishi</p></blockquote><blockquote><p>  1003 zhaoliu</p></blockquote><p><strong>==注意以tab键间隔==</strong></p><p>2．Hive实际操作</p><blockquote><p>  （1）启动hive</p></blockquote><blockquote><p>  [xing@hadoop102 hive]$ bin/hive</p></blockquote><blockquote><p>  （2）显示数据库</p></blockquote><blockquote><p>  hive&gt; show databases;</p></blockquote><blockquote><p>  （3）使用default数据库</p></blockquote><blockquote><p>  hive&gt; use default;</p></blockquote><blockquote><p>  （4）显示default数据库中的表</p></blockquote><blockquote><p>  hive&gt; show tables;</p></blockquote><blockquote><p>  （5）删除已创建的student表</p></blockquote><blockquote><p>  hive&gt; drop table student;</p></blockquote><blockquote><p>  （6）创建student表, 并声明文件分隔符’\t’</p></blockquote><blockquote><p>  hive&gt; create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</p></blockquote><blockquote><p>  （7）加载/opt/module/datas/student.txt 文件到student数据库表中。</p></blockquote><blockquote><p>  hive&gt; load data local inpath ‘/opt/module/datas/student.txt’ into table student;</p></blockquote><blockquote><p>  （8）Hive查询结果</p></blockquote><blockquote><p>  hive&gt; select * from student;</p></blockquote><blockquote><p>  OK</p></blockquote><blockquote><p>  1001 zhangshan</p></blockquote><blockquote><p>  1002 lishi</p></blockquote><blockquote><p>  1003 zhaoliu</p></blockquote><blockquote><p>  Time taken: 0.266 seconds, Fetched: 3 row(s)</p></blockquote><p>3．遇到的问题</p><p>再打开一个客户端窗口启动hive，会产生java.sql.SQLException异常。</p><blockquote><p>  Exception in thread “main” java.lang.RuntimeException: java.lang.RuntimeException:<br>   Unable to instantiate<br>   org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient<br>          at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)<br>          at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)<br>          at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)<br>          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>          at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)<br>          at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>          at java.lang.reflect.Method.invoke(Method.java:606)<br>          at org.apache.hadoop.util.RunJar.run(RunJar.java:221)<br>          at org.apache.hadoop.util.RunJar.main(RunJar.java:136)<br>  Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient<br>          at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)<br>          at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)<br>          at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)<br>          at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)<br>          at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)<br>          at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)<br>          at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)<br>  … 8 more</p></blockquote><p>原因是，**==Metastore默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore;==**</p><h2 id="2-4-MySql安装-root"><a href="#2-4-MySql安装-root" class="headerlink" title="2.4 MySql安装(==root==)"></a>2.4 MySql安装(==root==)</h2><h3 id="2-4-1-安装包准备"><a href="#2-4-1-安装包准备" class="headerlink" title="2.4.1 安装包准备"></a>2.4.1 安装包准备</h3><p>1．查看mysql是否安装，如果安装了，卸载mysql</p><p>（1）查看</p><blockquote><p>  [root@hadoop102 桌面]# rpm -qa|grep mysql</p></blockquote><blockquote><p>  mysql-libs-5.1.73-7.el6.x86_64</p></blockquote><p>（2）卸载</p><blockquote><p>  [root@hadoop102 桌面]# rpm -e –nodeps mysql-libs-5.1.73-7.el6.x86_64</p></blockquote><p>2．解压mysql-libs.zip文件到当前目录</p><blockquote><p>  [root@hadoop102 software]# unzip mysql-libs.zip</p></blockquote><blockquote><p>  [root@hadoop102 software]# ls</p></blockquote><blockquote><p>  mysql-libs.zip</p></blockquote><blockquote><p>  mysql-libs</p></blockquote><p>3．进入到mysql-libs文件夹下</p><blockquote><p>  [root@hadoop102 mysql-libs]# ll</p></blockquote><blockquote><p>  总用量 76048</p></blockquote><blockquote><p>  -rw-r–r–. 1 root root 18509960 3月 26 2015 MySQL-client-5.6.24-1.el6.x86_64.rpm</p></blockquote><blockquote><p>  -rw-r–r–. 1 root root 3575135 12月 1 2013 mysql-connector-java-5.1.27.tar.gz</p></blockquote><blockquote><p>  -rw-r–r–. 1 root root 55782196 3月 26 2015 MySQL-server-5.6.24-1.el6.x86_64.rpm</p></blockquote><h3 id="2-4-2-安装MySql服务器"><a href="#2-4-2-安装MySql服务器" class="headerlink" title="2.4.2 安装MySql服务器"></a>2.4.2 安装MySql服务器</h3><p>1．安装mysql服务端</p><blockquote><p>  [root@hadoop102 mysql-libs]# rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm</p></blockquote><p>2．查看产生的随机密码</p><blockquote><p>  [root@hadoop102 mysql-libs]# cat /root/.mysql_secret</p></blockquote><blockquote><p>  OEXaQuS8IWkG19Xs</p></blockquote><p>3．查看mysql状态</p><blockquote><p>  [root@hadoop102 mysql-libs]# service mysql status</p></blockquote><p>4．启动mysql</p><blockquote><p>  [root@hadoop102 mysql-libs]# service mysql start</p></blockquote><h3 id="2-4-3-安装MySql客户端"><a href="#2-4-3-安装MySql客户端" class="headerlink" title="2.4.3 安装MySql客户端"></a>2.4.3 安装MySql客户端</h3><p>1．安装mysql客户端</p><blockquote><p>  [root@hadoop102 mysql-libs]# rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm</p></blockquote><p>2．链接mysql</p><blockquote><p>  [root@hadoop102 mysql-libs]# mysql -uroot -pOEXaQuS8IWkG19Xs</p></blockquote><p>3．修改密码</p><blockquote><p>  mysql&gt;SET PASSWORD=PASSWORD(‘000000’);</p></blockquote><p>4．退出mysql</p><blockquote><p>  mysql&gt;exit</p></blockquote><h3 id="2-4-4-MySql中user表中主机配置"><a href="#2-4-4-MySql中user表中主机配置" class="headerlink" title="2.4.4 MySql中user表中主机配置"></a>2.4.4 MySql中user表中主机配置</h3><p>配置只要是root用户+密码，在任何主机上都能登录MySQL数据库。</p><p>1．进入mysql</p><blockquote><p>  [root@hadoop102 mysql-libs]# mysql -uroot -p000000</p></blockquote><p>2．显示数据库</p><blockquote><p>  mysql&gt;show databases;</p></blockquote><p>3．使用mysql数据库</p><blockquote><p>  mysql&gt;use mysql;</p></blockquote><p>4．展示mysql数据库中的所有表</p><blockquote><p>  mysql&gt;show tables;</p></blockquote><p>5．展示user表的结构</p><blockquote><p>  mysql&gt;desc user;</p></blockquote><p>6．查询user表</p><blockquote><p>  mysql&gt;select User, Host, Password from user;</p></blockquote><p>7．修改user表，把Host表内容修改为%</p><blockquote><p>  mysql&gt;update user set host=’%’ where host=’localhost’;</p></blockquote><p>8．删除root用户的其他host</p><blockquote><p>  mysql&gt;delete from user where Host=’hadoop102’;</p></blockquote><blockquote><p>  mysql&gt;delete from user where Host=’127.0.0.1’;</p></blockquote><blockquote><p>  mysql&gt;delete from user where Host=’::1’;</p></blockquote><p>9．刷新</p><blockquote><p>  mysql&gt;flush privileges;</p></blockquote><p>10．退出</p><blockquote><p>  mysql&gt;quit;</p></blockquote><h2 id="2-5-Hive元数据配置到MySql"><a href="#2-5-Hive元数据配置到MySql" class="headerlink" title="2.5 Hive元数据配置到MySql"></a>2.5 Hive元数据配置到MySql</h2><h3 id="2-5-1-驱动拷贝"><a href="#2-5-1-驱动拷贝" class="headerlink" title="2.5.1 驱动拷贝"></a>2.5.1 驱动拷贝</h3><p>1．在/opt/software/mysql-libs目录下解压mysql-connector-java-5.1.27.tar.gz驱动包</p><blockquote><p>[root@hadoop102 mysql-libs]# tar -zxvf mysql-connector-java-5.1.27.tar.gz</p></blockquote><p>2．拷贝/opt/software/mysql-libs/mysql-connector-java-5.1.27目录下的mysql-connector-java-5.1.27-bin.jar到/opt/module/hive/lib/</p><blockquote><p>[root@hadoop102 mysql-connector-java-5.1.27]# </p><p>cp mysql-connector-java-5.1.27-bin.jar /opt/module/hive/lib/</p></blockquote><h3 id="2-5-2-配置Metastore到MySql"><a href="#2-5-2-配置Metastore到MySql" class="headerlink" title="2.5.2 配置Metastore到MySql"></a>2.5.2 配置Metastore到MySql</h3><p>1．在/opt/module/hive/conf目录下创建一个hive-site.xml</p><blockquote><p>  [xing@hadoop102 conf]$ touch hive-site.xml</p></blockquote><blockquote><p>  [xing@hadoop102 conf]$ vi hive-site.xml</p></blockquote><p>2．根据官方文档配置参数，拷贝数据到hive-site.xml文件中</p><p><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin">https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin</a></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>3．配置完毕后，如果启动hive异常，可以重新启动虚拟机。（重启后，别忘了启动hadoop集群）</p><h3 id="2-5-3-多窗口启动Hive测试"><a href="#2-5-3-多窗口启动Hive测试" class="headerlink" title="2.5.3 多窗口启动Hive测试"></a>2.5.3 多窗口启动Hive测试</h3><p>1．先启动MySQL</p><blockquote><p>  [xing@hadoop102 mysql-libs]$ mysql -uroot -p000000</p></blockquote><p>查看有几个数据库</p><blockquote><p>  mysql&gt; show databases;</p><p>  +——————–+</p><p>  | Database      |</p><p>  +——————–+</p><p>  | information_schema |</p><p>  | mysql       |</p><p>  | performance_schema |</p><p>  | test        |</p><p>  +——————–+</p></blockquote><p>2．再次打开多个窗口，分别启动hive</p><blockquote><p>  [xing@hadoop102 hive]$ bin/hive</p></blockquote><p>3．启动hive后，回到MySQL窗口查看数据库，显示增加了metastore数据库</p><blockquote><p>  mysql&gt; show databases;</p><p>  +——————–+</p><p>  | Database      |</p><p>  +——————–+</p><p>  | information_schema |</p><p>  | metastore     |</p><p>  | mysql       |</p><p>  | performance_schema |</p><p>  | test        |</p><p>  +——————–+</p></blockquote><h2 id="2-6-HiveJDBC访问"><a href="#2-6-HiveJDBC访问" class="headerlink" title="2.6 HiveJDBC访问"></a>2.6 HiveJDBC访问</h2><h3 id="2-6-1-启动hiveserver2服务"><a href="#2-6-1-启动hiveserver2服务" class="headerlink" title="2.6.1 启动hiveserver2服务"></a>2.6.1 启动hiveserver2服务</h3><blockquote><p>  [xing@hadoop102 hive]$ bin/hiveserver2</p></blockquote><h3 id="2-6-2-启动beeline"><a href="#2-6-2-启动beeline" class="headerlink" title="2.6.2 启动beeline"></a>2.6.2 启动beeline</h3><blockquote><p>  [xing@hadoop102 hive]$ bin/beeline</p><p>  Beeline version 1.2.1 by Apache Hive</p><p>  beeline&gt;</p></blockquote><h3 id="2-6-3-连接hiveserver2"><a href="#2-6-3-连接hiveserver2" class="headerlink" title="2.6.3 连接hiveserver2"></a>2.6.3 连接hiveserver2</h3><blockquote><p>  beeline&gt; !connect jdbc:hive2://hadoop102:10000（回车）</p><p>  Connecting to jdbc:hive2://hadoop102:10000</p><p>  Enter username for jdbc:hive2://hadoop102:10000: xing（回车）</p><p>  Enter password for jdbc:hive2://hadoop102:10000: （直接回车）</p><p>  Connected to: Apache Hive (version 1.2.1)</p><p>  Driver: Hive JDBC (version 1.2.1)</p><p>  Transaction isolation: TRANSACTION_REPEATABLE_READ</p><p>  0: jdbc:hive2://hadoop102:10000&gt; show databases;</p><p>  +—————-+–+</p><p>  | database_name |</p><p>  +—————-+–+</p><p>  | default    |</p><p>  | hive_db2    |</p><p>  +—————-+–+</p></blockquote><h2 id="2-7-Hive常用交互命令"><a href="#2-7-Hive常用交互命令" class="headerlink" title="2.7 Hive常用交互命令"></a>2.7 Hive常用交互命令</h2><blockquote><p>[xing@hadoop102 hive]$ bin/hive -help  usage: hive  </p><p>-d,–define  &lt;key=value&gt;     Variable  subsitution to apply to hive                   </p><p>​                                             commands. e.g. -d A=B or  –define A=B      </p><p>–database <databasename>    Specify the database to use </p><p>-e  <quoted-query-string>     SQL  from command line   </p><p>-f  <filename>          SQL  from files   </p><p>-H,</p><p>​    –help            Print help information      </p><p>​    –hiveconf &lt;property=value&gt;   Use value for given property    </p><p>​    –hivevar  &lt;key=value&gt;     Variable  subsitution to apply to hive                   </p><p>​                                    commands.  e.g. –hivevar A=B  </p><p>-i  <filename>           Initialization SQL file   </p><p>-S,–silent           Silent mode in  interactive shell  </p><p>-v,–verbose           Verbose mode (echo  executed SQL to the console)  </p></blockquote><p>1．“-e”不进入hive的交互窗口执行sql语句</p><blockquote><p>  [xing@hadoop102 hive]$ bin/hive -e “select id from student;”</p></blockquote><p>2．“-f”执行脚本中sql语句</p><p>（1）在/opt/module/datas目录下创建hivef.sql文件</p><blockquote><p>  [xing@hadoop102 datas]$ touch hivef.sql</p></blockquote><p>文件中写入正确的sql语句</p><blockquote><p>select *from student;</p></blockquote><p>（2）执行文件中的sql语句</p><blockquote><p>  [xing@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql</p></blockquote><p>（3）执行文件中的sql语句并将结果写入文件中</p><blockquote><p>  [xing@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql &gt; /opt/module/datas/hive_result.txt</p></blockquote><h2 id="2-8-Hive其他命令操作"><a href="#2-8-Hive其他命令操作" class="headerlink" title="2.8 Hive其他命令操作"></a>2.8 Hive其他命令操作</h2><p>1．退出hive窗口：</p><blockquote><p>  hive(default)&gt;exit;</p></blockquote><blockquote><p>  hive(default)&gt;quit;</p></blockquote><blockquote><p>  在新版的hive中没区别了，在以前的版本是有的：</p></blockquote><blockquote><p>  exit:先隐性提交数据，再退出；</p></blockquote><blockquote><p>  quit:不提交数据，退出；</p></blockquote><p>2．在hive cli命令窗口中如何查看hdfs文件系统</p><blockquote><p>  hive(default)&gt;dfs -ls /;</p></blockquote><p>3．在hive cli命令窗口中如何查看本地文件系统</p><blockquote><p>  hive(default)&gt;! ls /opt/module/datas;</p></blockquote><p>4．查看在hive中输入的所有历史命令</p><p>（1）进入到当前用户的根目录/root或/home/xing</p><p>（2）查看. hivehistory文件</p><blockquote><p>  [xing@hadoop102 ~]$ cat .hivehistory</p></blockquote><h2 id="2-9-Hive常见属性配置"><a href="#2-9-Hive常见属性配置" class="headerlink" title="2.9 Hive常见属性配置"></a>2.9 Hive常见属性配置</h2><h3 id="2-9-1-Hive数据仓库位置配置"><a href="#2-9-1-Hive数据仓库位置配置" class="headerlink" title="2.9.1 Hive数据仓库位置配置"></a>2.9.1 Hive数据仓库位置配置</h3><p>1）Default数据仓库的最原始位置是在hdfs上的：/user/hive/warehouse路径下。</p><p>2）**==在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹。==**</p><p>3）修改default数据仓库原始位置（将hive-default.xml.template如下配置信息拷贝到hive-site.xml文件中）。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置同组用户有执行权限</p><blockquote><p>  bin/hdfs dfs -chmod g+w /user/hive/warehouse</p></blockquote><h3 id="2-9-2-查询后信息显示配置"><a href="#2-9-2-查询后信息显示配置" class="headerlink" title="2.9.2 查询后信息显示配置"></a>2.9.2 查询后信息显示配置</h3><p>1）在hive-site.xml文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>2）重新启动hive，对比配置前后差异。</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113929.png" alt="image-20200917235726107"></p><h3 id="2-9-3-Hive运行日志信息配置"><a href="#2-9-3-Hive运行日志信息配置" class="headerlink" title="2.9.3 Hive运行日志信息配置"></a>2.9.3 Hive运行日志信息配置</h3><p>1．Hive的log默认存放在/tmp/xing/hive.log目录下（当前用户名下）</p><p>2．修改hive的log存放日志到/opt/module/hive/logs</p><p>（1）修改/opt/module/hive/conf/hive-log4j.properties.template文件名称为</p><p>hive-log4j.properties</p><blockquote><p>  [xing@hadoop102 conf]$ pwd</p></blockquote><blockquote><p>  /opt/module/hive/conf</p></blockquote><blockquote><p>  [xing@hadoop102 conf]$ mv hive-log4j.properties.template<br>  hive-log4j.properties</p></blockquote><p>（2）在hive-log4j.properties文件中修改log存放位置</p><blockquote><p>  hive.log.dir=/opt/module/hive/logs</p></blockquote><h3 id="2-9-4-参数配置方式"><a href="#2-9-4-参数配置方式" class="headerlink" title="2.9.4 参数配置方式"></a>2.9.4 参数配置方式</h3><p>1．查看当前所有的配置信息</p><blockquote><p>  hive&gt;set;</p></blockquote><p>2．参数的配置三种方式</p><p>（1）配置文件方式</p><blockquote><p>  默认配置文件：hive-default.xml</p></blockquote><blockquote><p>  用户自定义配置文件：hive-site.xml</p></blockquote><blockquote><p>  注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。</p></blockquote><p>（2）命令行参数方式</p><blockquote><p>  启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。</p></blockquote><blockquote><p>  例如：</p></blockquote><blockquote><p>  [xing@hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;</p></blockquote><blockquote><p>  注意：仅对本次hive启动有效</p></blockquote><blockquote><p>  查看参数设置：</p></blockquote><blockquote><p>  hive (default)&gt; set mapred.reduce.tasks;</p></blockquote><p>（3）参数声明方式</p><blockquote><p>  可以在HQL中使用SET关键字设定参数</p></blockquote><blockquote><p>  例如：</p></blockquote><blockquote><p>  hive (default)&gt; set mapred.reduce.tasks=100;</p></blockquote><blockquote><p>  注意：仅对本次hive启动有效。</p></blockquote><blockquote><p>  查看参数设置</p></blockquote><blockquote><p>  hive (default)&gt; set mapred.reduce.tasks;</p></blockquote><p>上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</p><h1 id="第3章-Hive数据类型"><a href="#第3章-Hive数据类型" class="headerlink" title="第3章 Hive数据类型"></a>第3章 Hive数据类型</h1><h2 id="3-1-基本数据类型"><a href="#3-1-基本数据类型" class="headerlink" title="3.1 基本数据类型"></a>3.1 基本数据类型</h2><table><thead><tr><th>Hive数据类型</th><th>Java数据类型</th><th>长度</th><th>例子</th></tr></thead><tbody><tr><td>TINYINT</td><td>byte</td><td>1byte有符号整数</td><td>20</td></tr><tr><td>SMALINT</td><td>short</td><td>2byte有符号整数</td><td>20</td></tr><tr><td>INT</td><td>int</td><td>4byte有符号整数</td><td>20</td></tr><tr><td>BIGINT</td><td>long</td><td>8byte有符号整数</td><td>20</td></tr><tr><td>BOOLEAN</td><td>boolean</td><td>布尔类型，true或者false</td><td>TRUE FALSE</td></tr><tr><td>FLOAT</td><td>float</td><td>单精度浮点数</td><td>3.14159</td></tr><tr><td>DOUBLE</td><td>double</td><td>双精度浮点数</td><td>3.14159</td></tr><tr><td>STRING</td><td>string</td><td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td><td>‘now is the time’ “for all good men”</td></tr><tr><td>TIMESTAMP</td><td></td><td>时间类型</td><td></td></tr><tr><td>BINARY</td><td></td><td>字节数组</td><td></td></tr></tbody></table><p>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</p><h2 id="3-2-集合数据类型"><a href="#3-2-集合数据类型" class="headerlink" title="3.2 集合数据类型"></a>3.2 集合数据类型</h2><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td><td>struct()</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td><td>map()</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td><td>Array()</td></tr></tbody></table><p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p><p>案例实操</p><ol><li>假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问的格式为</li></ol><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;songsong&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;friends&quot;</span>: [<span class="string">&quot;bingbing&quot;</span> , <span class="string">&quot;lili&quot;</span>] ,       <span class="comment">//列表Array, </span></span><br><span class="line">    <span class="attr">&quot;children&quot;</span>: &#123;                      <span class="comment">//键值Map,</span></span><br><span class="line">        <span class="attr">&quot;xiao song&quot;</span>: <span class="number">18</span> ,</span><br><span class="line">        <span class="attr">&quot;xiaoxiao song&quot;</span>: <span class="number">19</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">&quot;address&quot;</span>: &#123;                      <span class="comment">//结构Struct,</span></span><br><span class="line">        <span class="attr">&quot;street&quot;</span>: <span class="string">&quot;hui long guan&quot;</span> ,</span><br><span class="line">        <span class="attr">&quot;city&quot;</span>: <span class="string">&quot;beijing&quot;</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2）基于上述数据结构，我们在Hive里创建对应的表，并导入数据。</p><p>创建本地测试文件test.txt</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</span><br></pre></td></tr></table></figure><p><strong>==注意==：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。</strong></p><p>3）Hive上创建测试表test</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">test</span>(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">friends <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">children <span class="keyword">map</span>&lt;<span class="keyword">string</span>, <span class="built_in">int</span>&gt;,</span><br><span class="line">address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>, city:<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;_&#x27;</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;:&#x27;</span></span><br><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>字段解释</p></blockquote><blockquote><p>  row format delimited fields terminated by ‘,’ – 列分隔符</p></blockquote><blockquote><p>  collection items terminated by ‘_’ –MAP STRUCT 和 ARRAY的分隔符(数据分割符号)</p></blockquote><blockquote><p>  map keys terminated by ‘:’ – MAP中的key与value的分隔符</p></blockquote><blockquote><p>  lines terminated by ‘\n’; – 行分隔符</p></blockquote><p>4）导入文本数据到测试表</p><blockquote><p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/test.txt’ into table test</p></blockquote><p>5）访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式</p><blockquote><p>hive (default)&gt; select friends[1],children[‘xiao song’],address.city from test</p><p>where name=”songsong”;</p><p>OK</p><p>_c0   _c1   city</p><p>lili  18   beijing</p><p>Time taken: 0.076 seconds, Fetched: 1 row(s)</p></blockquote><h2 id="3-3-类型转化"><a href="#3-3-类型转化" class="headerlink" title="3.3 类型转化"></a>3.3 类型转化</h2><p>Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。</p><p>1．隐式类型转换规则如下</p><p>（1）任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换BIGINT。</p><p>（2）所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。</p><p>（3）TINYINT、SMALLINT、INT都可以转换为FLOAT。</p><p>（4）BOOLEAN类型不可以转换为任何其它的类型。</p><p>2．可以使用CAST操作显示进行数据类型转换</p><p>例如CAST(‘1’ AS INT)将把字符串’1’转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。</p><h1 id="第4章-DDL数据定义"><a href="#第4章-DDL数据定义" class="headerlink" title="第4章 DDL数据定义"></a>第4章 DDL数据定义</h1><h2 id="4-1-创建数据库"><a href="#4-1-创建数据库" class="headerlink" title="4.1 创建数据库"></a>4.1 创建数据库</h2><p>1）创建一个数据库，数据库在HDFS上的默认存储路径是/user/hive/warehouse/*.db。</p><blockquote><p>hive (default)&gt; create database db_hive;</p></blockquote><p>2）避免要创建的数据库已经存在错误，增加if not exists判断。（标准写法）</p><blockquote><p>hive (default)&gt; create database db_hive;</p><p>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exists</p><p>hive (default)&gt; create database if not exists db_hive;</p></blockquote><p>3）创建一个数据库，指定数据库在HDFS上存放的位置</p><p>hive (default)&gt; create database db_hive2 location ‘/db_hive2.db’;</p><h2 id="4-2-查询数据库"><a href="#4-2-查询数据库" class="headerlink" title="4.2 查询数据库"></a>4.2 查询数据库</h2><h3 id="4-2-1-显示数据库"><a href="#4-2-1-显示数据库" class="headerlink" title="4.2.1 显示数据库"></a>4.2.1 显示数据库</h3><p>1．显示数据库</p><blockquote><p>  hive&gt; show databases;</p></blockquote><p>2．过滤显示查询的数据库</p><blockquote><p>  hive&gt; show databases like ‘db_hive*’;</p><p>  OK</p><p>  db_hive</p><p>  db_hive_1</p></blockquote><h3 id="4-2-2-查看数据库详情"><a href="#4-2-2-查看数据库详情" class="headerlink" title="4.2.2 查看数据库详情"></a>4.2.2 查看数据库详情</h3><p>1．显示数据库信息</p><blockquote><p>  hive&gt; desc database db_hive;</p><p>  OK</p><p>  db_hive hdfs://hadoop102:9000/user/hive/warehouse/db_hive.db xingUSER</p></blockquote><p>2．显示数据库详细信息，extended</p><blockquote><p>  hive&gt; desc database extended db_hive;</p><p>  OK</p><p>  db_hive   hdfs://hadoop102:9000/user/hive/warehouse/db_hive.db xingUSER </p><p>  40.3.3 切换当前数据库</p><p>  hive (default)&gt; use db_hive;</p></blockquote><h3 id="4-3-3-切换当前数据库"><a href="#4-3-3-切换当前数据库" class="headerlink" title="4.3.3 切换当前数据库"></a>4.3.3 切换当前数据库</h3><blockquote><p>  hive (default)&gt; use db_hive;</p></blockquote><h2 id="4-3-修改数据库"><a href="#4-3-修改数据库" class="headerlink" title="4.3 修改数据库"></a>4.3 修改数据库</h2><p>用户可以使用ALTERDATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。**==数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。==**</p><blockquote><p>  hive (default)&gt; alter database db_hive set dbproperties(‘createtime’=’20170830’);</p></blockquote><p>在hive中查看修改结果</p><blockquote><p>  hive&gt; desc database extended db_hive;<br>db_name comment location owner_name owner_type parameters<br> db_hive hdfs://hadoop102:8020/user/hive/warehouse/db_hive.db xing USER{createtime=20170830}</p></blockquote><h2 id="4-4-删除数据库"><a href="#4-4-删除数据库" class="headerlink" title="4.4 删除数据库"></a>4.4 删除数据库</h2><p>1．删除空数据库</p><blockquote><p>  hive&gt;drop database db_hive2;</p></blockquote><p>2．如果删除的数据库不存在，最好采用 if exists判断数据库是否存在</p><blockquote><p>  hive&gt; drop database db_hive;</p></blockquote><blockquote><p>  FAILED: SemanticException [Error 10072]: Database does not exist: db_hive</p></blockquote><blockquote><p>  hive&gt; drop database if exists db_hive2;</p></blockquote><p>3．如果数据库不为空，可以采用cascade命令，强制删除</p><blockquote><p>  hive&gt; drop database db_hive;</p></blockquote><blockquote><p>  FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask.nvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)</p></blockquote><blockquote><p>  hive&gt; drop database db_hive cascade;</p></blockquote><h2 id="4-5-创建表"><a href="#4-5-创建表" class="headerlink" title="4.5 创建表"></a>4.5 创建表</h2><p>1．建表语法</p><blockquote><p>  CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name</p><p>  [(col_name data_type [COMMENT col_comment], …)] </p><p>  [COMMENT table_comment]</p><p>  [PARTITIONED BY (col_name data_type [COMMENT col_comment], …)] </p><p>  [CLUSTERED BY (col_name, col_name, …) </p><p>  [SORTED BY (col_name [ASC|DESC], …)] INTO num_buckets BUCKETS] </p><p>  [ROW FORMAT row_format] </p><p>  [STORED AS file_format]</p><p>  [LOCATION hdfs_path]</p></blockquote><p>2．字段解释说明</p><blockquote><p>  （1）CREATE TABLE<br>  创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IFNOT EXISTS 选项来忽略这个异常。</p></blockquote><blockquote><p>  （2）EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</p></blockquote><blockquote><p>（3）COMMENT：为表和列添加注释。</p></blockquote><blockquote><p>（4）PARTITIONED BY创建分区表</p></blockquote><blockquote><p>（5）CLUSTERED BY创建分桶表</p></blockquote><blockquote><p>（6）SORTED BY不常用</p></blockquote><blockquote><p>  （7）ROW FORMAT</p></blockquote><blockquote><p>  DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char]</p></blockquote><blockquote><p>  [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]</p></blockquote><blockquote><p>  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value,<br>  property_name=property_value, …)]</p></blockquote><blockquote><p>用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT<br>或者ROW FORMAT<br>DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。</p><p>SerDe是Serialize/Deserilize的简称，目的是用于序列化和反序列化。</p></blockquote><blockquote><p>（8）STORED AS指定存储文件类型</p></blockquote><blockquote><p>  常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）</p></blockquote><blockquote><p>  如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用STORED AS SEQUENCEFILE。</p></blockquote><blockquote><p>（9）LOCATION ：指定表在HDFS上的存储位置。</p></blockquote><blockquote><p>（10）LIKE允许用户复制现有的表结构，但是不复制数据。</p></blockquote><h3 id="4-5-1-管理表"><a href="#4-5-1-管理表" class="headerlink" title="4.5.1 管理表"></a>4.5.1 管理表</h3><p>1．理论</p><p>默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置hive.metastore.warehouse.dir(如，/user/hive/warehouse)所定义的目录的子目录下。**==当我们删除一个管理表时，Hive也会删除这个表中数据==**。管理表不适合和其他工具共享数据。</p><p>2．案例实操</p><p>（1）普通创建表</p><blockquote><p>create table if not exists student2(</p><p>​    id int,</p><p>​    name string</p><p>)</p><p>row format delimited fields terminated by ‘\t’</p><p>stored as textfile</p><p>location ‘/user/hive/warehouse/student2’;</p></blockquote><p>（2）根据查询结果创建表（查询的结果会添加到新创建的表中）</p><blockquote><p>create table if not exists student3 as select id, name from student;</p></blockquote><p>（3）根据已经存在的表结构创建表</p><blockquote><p>create  table if not exists student4 like student;    </p></blockquote><p>（4）查询表的类型</p><blockquote><p>  hive (default)&gt; desc formatted student2;</p></blockquote><blockquote><p>  Table Type: MANAGED_TABLE</p></blockquote><h3 id="4-5-2-外部表"><a href="#4-5-2-外部表" class="headerlink" title="4.5.2 外部表"></a>4.5.2 外部表</h3><p>1．理论</p><p>因为表是外部表，所以Hive并非认为其完全拥有这份数据。**==删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。==**</p><p>2．管理表和外部表的使用场景</p><p>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p><p>3．案例实操</p><p>分别创建部门和员工外部表，并向表中导入数据。</p><p>（1）原始数据 </p><p>​    <a href="./dept.txt">dept.txt</a></p><p>​    <a href="./emp.txt">emp.txt</a></p><p>（2）建表语句</p><p>创建部门表</p><blockquote><p>create external table if not exists default.dept( </p><p>deptno int,</p><p>dname string, </p><p>loc int </p><p>) </p><p>row format delimited fields terminated by ‘\t’; </p></blockquote><p>创建员工表</p><blockquote><p>create external table if not exists default.emp( </p><p>empno int,</p><p> ename string, </p><p>job string,</p><p> mgr int, </p><p>hiredate string,  </p><p>sal double, </p><p> comm double, </p><p>deptno int</p><p>) </p><p>row format delimited fields terminated by ‘\t’; |</p></blockquote><p>（3）查看创建的表</p><blockquote><p>  hive (default)&gt; show tables;</p><p>  OK</p><p>  tab_name</p><p>  dept</p><p>  emp</p></blockquote><p>（4）向外部表中导入数据</p><p>导入数据</p><blockquote><p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/dept.txt’ into table default.dept;</p></blockquote><blockquote><p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/emp.txt’ into table default.emp;</p></blockquote><p>查询结果</p><blockquote><p>  hive (default)&gt; select * from emp;</p></blockquote><blockquote><p>  hive (default)&gt; select * from dept;</p></blockquote><p>（5）查看表格式化数据</p><blockquote><p>  hive (default)&gt; desc formatted dept;</p></blockquote><blockquote><p>  Table Type: EXTERNAL_TABLE</p></blockquote><h3 id="4-5-3-管理表与外部表的互相转换"><a href="#4-5-3-管理表与外部表的互相转换" class="headerlink" title="4.5.3 管理表与外部表的互相转换"></a>4.5.3 管理表与外部表的互相转换</h3><p>（1）查询表的类型</p><blockquote><p>  hive (default)&gt; desc formatted student2;</p></blockquote><blockquote><p>  Table Type: MANAGED_TABLE</p></blockquote><p>（2）修改内部表student2为外部表</p><blockquote><p>  alter table student2 set tblproperties(‘EXTERNAL’=’TRUE’);</p></blockquote><p>（3）查询表的类型</p><blockquote><p>  hive (default)&gt; desc formatted student2;</p></blockquote><blockquote><p>  Table Type: EXTERNAL_TABLE</p></blockquote><p>（4）修改外部表student2为内部表</p><blockquote><p>  alter table student2 set tblproperties(‘EXTERNAL’=’FALSE’);</p></blockquote><p>（5）查询表的类型</p><blockquote><p>  hive (default)&gt; desc formatted student2;</p></blockquote><blockquote><p>  Table Type: MANAGED_TABLE</p></blockquote><p><strong>==注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！==</strong></p><h2 id="4-6-分区表"><a href="#4-6-分区表" class="headerlink" title="4.6 分区表"></a>4.6 <a name="4.6">分区表</a></h2><p>分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。**==Hive中的分区就是分目录==**，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。</p><h3 id="4-6-1-分区表基本操作"><a href="#4-6-1-分区表基本操作" class="headerlink" title="4.6.1 分区表基本操作"></a>4.6.1 分区表基本操作</h3><p>1．引入分区表（需要根据日期对日志进行管理）</p><blockquote><p>  /user/hive/warehouse/log_partition/20170702/20170702.log</p></blockquote><blockquote><p>  /user/hive/warehouse/log_partition/20170703/20170703.log</p></blockquote><blockquote><p>  /user/hive/warehouse/log_partition/20170704/20170704.log</p></blockquote><p>2．创建分区表语法</p><blockquote><p>hive (default)&gt; create table dept_partition(</p><p>deptno int, </p><p>dname string, </p><p>loc string</p><p>)</p><p>partitioned by (month string)</p><p>row format delimited fields terminated by ‘\t’;</p></blockquote><p>3．加载数据到分区表中</p><blockquote><p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/dept.txt’ into table default.dept_partition partition(month=’201709’);</p></blockquote><blockquote><p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/dept.txt’ into table default.dept_partition partition(month=’201708’);</p></blockquote><blockquote><p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/dept.txt’ into table default.dept_partition partition(month=’201707’);</p></blockquote><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113930.png" alt="image-20200918001733383"></p><p>4．查询分区表中数据</p><p>单分区查询</p><blockquote><p>  hive (default)&gt; select * from dept_partition where month=’201709’;</p></blockquote><blockquote><p>  多分区联合查询</p></blockquote><blockquote><p>  hive (default)&gt; select * from dept_partition where month=’201709’</p><p>  ​       union</p><p>  ​       select * from dept_partition where month=’201708’</p><p>  ​       union</p><p>  ​       select * from dept_partition where month=’201707’;</p><p>  _u3.deptno   _u3.dname    _u3.loc _u3.month</p><p>  10   ACCOUNTING   NEW YORK    201707</p><p>  10   ACCOUNTING   NEW YORK    201708</p><p>  10   ACCOUNTING   NEW YORK    201709</p><p>  20   RESEARCH    DALLAS 201707</p><p>  20   RESEARCH    DALLAS 201708</p><p>  20   RESEARCH    DALLAS 201709</p><p>  30   SALES  CHICAGO 201707</p><p>  30   SALES  CHICAGO 201708</p><p>  30   SALES  CHICAGO 201709</p><p>  40   OPERATIONS   BOSTON 201707</p><p>  40   OPERATIONS   BOSTON 201708</p><p>  40   OPERATIONS   BOSTON 201709</p></blockquote><p>5．增加分区</p><p>创建单个分区</p><blockquote><p>  hive (default)&gt; alter table dept_partition add partition(month=’201706’) ;</p></blockquote><p>同时创建多个分区</p><blockquote><p>  hive (default)&gt; alter table dept_partition add partition(month=’201705’) partition(month=’201704’);</p></blockquote><p>6．删除分区</p><p>删除单个分区</p><blockquote><p>  hive (default)&gt; alter table dept_partition drop partition (month=’201704’);</p></blockquote><p>同时删除多个分区</p><blockquote><p>  hive (default)&gt; alter table dept_partition drop partition (month=’201705’), partition (month=’201706’);</p></blockquote><p>7．查看分区表有多少分区</p><blockquote><p>  hive&gt; show partitions dept_partition;</p></blockquote><p>8．查看分区表结构</p><blockquote><p>  hive&gt; desc formatted dept_partition;</p></blockquote><blockquote><p>  # Partition Information</p></blockquote><blockquote><p>  # col_name data_type comment</p></blockquote><blockquote><p>  month string</p></blockquote><h3 id="4-6-2-分区表注意事项"><a href="#4-6-2-分区表注意事项" class="headerlink" title="4.6.2 分区表注意事项"></a>4.6.2 分区表注意事项</h3><p>1．创建二级分区表</p><blockquote><p>hive (default)&gt; create table dept_partition2(</p><p>​        deptno int, dname string, loc string</p><p>)</p><p>partitioned by (month string, day string)</p><p>row format delimited fields terminated by ‘\t’;</p></blockquote><p>2．正常的加载数据</p><p>（1）加载数据到二级分区表中</p><blockquote><p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/dept.txt’ into table default.dept_partition2 partition(month=’201709’, day=’13’);</p></blockquote><p>（2）查询分区数据</p><blockquote><p>  hive (default)&gt; select * from dept_partition2 where month=’201709’ and day=’13’;</p></blockquote><p>3．把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式</p><p>（1）方式一：上传数据后修复</p><p>上传数据</p><blockquote><p>  hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=12;</p></blockquote><blockquote><p>  hive (default)&gt; dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=12;</p></blockquote><p>查询数据（查询不到刚上传的数据）</p><blockquote><p>  hive (default)&gt; select * from dept_partition2 where month=’201709’ and day=’12’;</p></blockquote><p>执行修复命令</p><blockquote><p>  hive&gt; msck repair table dept_partition2;</p></blockquote><p>再次查询数据</p><blockquote><p>  hive (default)&gt; select * from dept_partition2 where month=’201709’ and day=’12’;</p></blockquote><p>（2）方式二：上传数据后添加分区</p><blockquote><p>  上传数据</p></blockquote><blockquote><p>  hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=11;</p></blockquote><blockquote><p>  hive (default)&gt; dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=11;</p></blockquote><blockquote><p>  执行添加分区</p></blockquote><blockquote><p>  hive (default)&gt; alter table dept_partition2 add partition(month=’201709’,day=’11’);</p></blockquote><blockquote><p>  查询数据</p></blockquote><blockquote><p>  hive (default)&gt; select * from dept_partition2 where month=’201709’ and day=’11’;</p></blockquote><p>（3）方式三：创建文件夹后load数据到分区</p><p>创建目录</p><blockquote><p>  hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=10;</p></blockquote><p>上传数据</p><blockquote><p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/dept.txt’ into table dept_partition2 partition(month=’201709’,day=’10’);</p></blockquote><p>查询数据</p><blockquote><p>  hive (default)&gt; select * from dept_partition2 where month=’201709’ and day=’10’;</p></blockquote><h2 id="4-7-修改表"><a href="#4-7-修改表" class="headerlink" title="4.7 修改表"></a>4.7 修改表</h2><h3 id="4-7-1-重命名表"><a href="#4-7-1-重命名表" class="headerlink" title="4.7.1 重命名表"></a>4.7.1 重命名表</h3><p>1．语法</p><blockquote><p>  ALTER TABLE table_name RENAME TO new_table_name</p></blockquote><p>2．实操案例</p><blockquote><p>  hive (default)&gt; alter table dept_partition2 rename to dept_partition3;</p></blockquote><h3 id="4-7-2-增加、修改和删除表分区"><a href="#4-7-2-增加、修改和删除表分区" class="headerlink" title="4.7.2 增加、修改和删除表分区"></a>4.7.2 增加、修改和删除表分区</h3><p>详见4.6.1分区表基本操作。</p><h3 id="4-7-3-增加-修改-替换列信息"><a href="#4-7-3-增加-修改-替换列信息" class="headerlink" title="4.7.3 增加/修改/替换列信息"></a>4.7.3 增加/修改/替换列信息</h3><p>1．语法</p><p>更新列</p><blockquote><p>  ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type<br>  [COMMENT col_comment] [FIRST|AFTER column_name]</p></blockquote><p>增加和替换列</p><blockquote><p>  ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT<br>  col_comment], …)</p></blockquote><p>注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。</p><p>2．实操案例</p><p>（1）查询表结构</p><blockquote><p>  hive&gt; desc dept_partition;</p></blockquote><p>（2）添加列</p><blockquote><p>  hive (default)&gt; alter table dept_partition add columns(deptdesc string);</p></blockquote><p>（3）查询表结构</p><blockquote><p>  hive&gt; desc dept_partition;</p></blockquote><p>（4）更新列</p><blockquote><p>  hive (default)&gt; alter table dept_partition change column deptdesc desc int;</p></blockquote><p>（5）查询表结构</p><blockquote><p>  hive&gt; desc dept_partition;</p></blockquote><p>（6）替换列</p><blockquote><p>  hive (default)&gt; alter table dept_partition replace columns(deptno string,dname string, loc string);</p></blockquote><p>（7）查询表结构</p><blockquote><p>  hive&gt; desc dept_partition;</p></blockquote><h2 id="4-8-删除表"><a href="#4-8-删除表" class="headerlink" title="4.8 删除表"></a>4.8 删除表</h2><blockquote><p>  hive (default)&gt; drop table dept_partition;</p></blockquote><h1 id="第5章-DML数据操作"><a href="#第5章-DML数据操作" class="headerlink" title="第5章 DML数据操作"></a>第5章 DML数据操作</h1><h2 id="5-1-数据导入"><a href="#5-1-数据导入" class="headerlink" title="5.1 数据导入"></a>5.1 数据导入</h2><h3 id="5-1-1-向表中装载数据（Load）"><a href="#5-1-1-向表中装载数据（Load）" class="headerlink" title="5.1.1 向表中装载数据（Load）"></a>5.1.1 向表中装载数据（Load）</h3><p>1．语法</p><blockquote><p>  hive&gt; load data [local] inpath ‘/opt/module/datas/student.txt’ overwrite |<br>  into table student [partition (partcol1=val1,…)];</p></blockquote><blockquote><p>  （1）load data:表示加载数据</p></blockquote><blockquote><p>  （2）local:表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</p></blockquote><blockquote><p>  （3）inpath:表示加载数据的路径</p></blockquote><blockquote><p>  （4）overwrite:表示覆盖表中已有数据，否则表示追加</p></blockquote><blockquote><p>  （5）into table:表示加载到哪张表</p></blockquote><blockquote><p>  （6）student:表示具体的表</p></blockquote><blockquote><p>  （7）partition:表示上传到指定分区</p></blockquote><p>2．实操案例</p><p>（0）创建一张表</p><blockquote><p>  hive (default)&gt; create table student(id string, name string) row format<br>  delimited fields terminated by ‘\t’;</p></blockquote><p>（1）加载本地文件到hive</p><blockquote><p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/student.txt’ into table default.student;</p></blockquote><p>（2）加载HDFS文件到hive中</p><p>上传文件到HDFS</p><blockquote><p>  hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/xing/hive;</p></blockquote><p>加载HDFS上数据</p><blockquote><p>  hive (default)&gt; load data inpath ‘/user/xing/hive/student.txt’ into table default.student;</p></blockquote><p>（3）加载数据覆盖表中已有的数据</p><p>上传文件到HDFS</p><blockquote><p>  hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/xing/hive;</p></blockquote><p>加载数据覆盖表中已有的数据</p><blockquote><p>  hive (default)&gt; load data inpath ‘/user/xing/hive/student.txt’ overwrite into table default.student;</p></blockquote><h3 id="5-1-2-通过查询语句向表中插入数据（Insert）"><a href="#5-1-2-通过查询语句向表中插入数据（Insert）" class="headerlink" title="5.1.2 通过查询语句向表中插入数据（Insert）"></a>5.1.2 通过查询语句向表中插入数据（Insert）</h3><p>1．创建一张分区表</p><blockquote><p>  hive (default)&gt; create table student(id int, name string) partitioned by (month string) row format delimited fields terminated by ‘\t’;</p></blockquote><blockquote><p>  2．基本插入数据</p></blockquote><blockquote><p>  hive (default)&gt; insert into table student partition(month=’201709’) values(1,’wangwu’);</p></blockquote><p>3．基本模式插入（根据单张表查询结果）</p><blockquote><p>  hive (default)&gt; insert overwrite table student partition(month=’201708’) select id, name from student where month=’201709’;</p></blockquote><p>4．多插入模式（根据多张表查询结果）</p><blockquote><p>  hive (default)&gt; from student insert overwrite table student partition(month=’201707’) select id, name where month=’201709’ insert overwrite table student partition(month=’201706’) select id, name where month=’201709’;</p></blockquote><h3 id="5-1-3-查询语句中创建表并加载数据（As-Select）"><a href="#5-1-3-查询语句中创建表并加载数据（As-Select）" class="headerlink" title="5.1.3 查询语句中创建表并加载数据（As Select）"></a>5.1.3 查询语句中创建表并加载数据（As Select）</h3><p>详见4.5.1章创建表。</p><p>根据查询结果创建表（查询的结果会添加到新创建的表中）</p><blockquote><p>  create table if not exists student3 as select id, name from student;</p></blockquote><h3 id="5-1-4-创建表时通过Location指定加载数据路径"><a href="#5-1-4-创建表时通过Location指定加载数据路径" class="headerlink" title="5.1.4 创建表时通过Location指定加载数据路径"></a>5.1.4 创建表时通过Location指定加载数据路径</h3><p>1．创建表，并指定在hdfs上的位置</p><blockquote><p>  hive (default)&gt; create table if not exists student5(</p><p>  ​       id int, name string</p><p>  )</p><p>   row format delimited fields terminated by ‘\t’</p><pre><code>location &#39;/user/hive/warehouse/student5&#39;;</code></pre></blockquote><p>2．上传数据到hdfs上</p><blockquote><p>  hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/hive/warehouse/student5;</p></blockquote><p>3．查询数据</p><blockquote><p>  hive (default)&gt; select * from student5;</p></blockquote><h3 id="5-1-5-Import数据到指定Hive表中"><a href="#5-1-5-Import数据到指定Hive表中" class="headerlink" title="5.1.5 Import数据到指定Hive表中"></a>5.1.5 Import数据到指定Hive表中</h3><p><strong>==注意：先用export导出后，再将数据导入。==</strong></p><blockquote><p>  hive (default)&gt; import table student2 partition(month=’201709’) from ‘/user/hive/warehouse/export/student’;</p></blockquote><h2 id="5-2-数据导出"><a href="#5-2-数据导出" class="headerlink" title="5.2 数据导出"></a>5.2 数据导出</h2><h3 id="5-2-1-Insert导出"><a href="#5-2-1-Insert导出" class="headerlink" title="5.2.1 Insert导出"></a>5.2.1 Insert导出</h3><p>1．将查询的结果导出到本地</p><blockquote><p>  hive (default)&gt; insert overwrite local directory ‘/opt/module/datas/export/student’ select * from student;</p></blockquote><blockquote><p>  2．将查询的结果格式化导出到本地</p></blockquote><blockquote><p>  hive(default)&gt;insert overwrite local directory ‘/opt/module/datas/export/student1’ ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’ select * from student;</p></blockquote><p>3．将查询的结果导出到HDFS上(没有local)</p><blockquote><p>  hive (default)&gt; insert overwrite directory ‘/user/xing/student2’ ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’ select * from student;</p></blockquote><h3 id="5-2-2-Hadoop命令导出到本地"><a href="#5-2-2-Hadoop命令导出到本地" class="headerlink" title="5.2.2 Hadoop命令导出到本地"></a>5.2.2 Hadoop命令导出到本地</h3><blockquote><p>  hive (default)&gt; dfs -get /user/hive/warehouse/student/month=201709/000000_0 /opt/module/datas/export/student3.txt;</p></blockquote><h3 id="5-2-3-Hive-Shell-命令导出"><a href="#5-2-3-Hive-Shell-命令导出" class="headerlink" title="5.2.3 Hive Shell 命令导出"></a>5.2.3 Hive Shell 命令导出</h3><p>基本语法：（hive -f/-e 执行语句或者脚本 &gt; file）</p><blockquote><p>  [xing@hadoop102 hive]$ bin/hive -e ‘select * from default.student;’ &gt; /opt/module/datas/export/student4.txt;</p></blockquote><h3 id="5-2-4-Export导出到HDFS上"><a href="#5-2-4-Export导出到HDFS上" class="headerlink" title="5.2.4 Export导出到HDFS上"></a>5.2.4 Export导出到HDFS上</h3><blockquote><p>  (defahiveult)&gt; export table default.student to ‘/user/hive/warehouse/export/student’;</p></blockquote><h3 id="5-2-5-Sqoop导出"><a href="#5-2-5-Sqoop导出" class="headerlink" title="5.2.5 Sqoop导出"></a>5.2.5 Sqoop导出</h3><p>详见Sqooop</p><h2 id="5-3-清除表中数据（Truncate）"><a href="#5-3-清除表中数据（Truncate）" class="headerlink" title="5.3 清除表中数据（Truncate）"></a>5.3 清除表中数据（Truncate）</h2><p>注意：Truncate只能删除管理表，不能删除外部表中数据</p><blockquote><p>  hive (default)&gt; truncate table student;</p></blockquote><h1 id="第6章-查询"><a href="#第6章-查询" class="headerlink" title="第6章 查询"></a>第6章 查询</h1><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select</a></p><p>查询语句语法：</p><blockquote><p>[WITH CommonTableExpression (, CommonTableExpression)*]  (Note: Only available</p><p>starting with Hive 0.13.0)</p><p>SELECT [ALL | DISTINCT] select_expr, select_expr, …</p><p>FROM table_reference</p><p>[WHERE where_condition]</p><p>[GROUP BY col_list]</p><p>[ORDER BY col_list]</p><p>[CLUSTER BY col_list</p><p> | [DISTRIBUTE BY col_list] [SORT BY col_list]</p><p>]</p><p>[LIMIT number]</p></blockquote><h2 id="6-1-基本查询（Select…From）"><a href="#6-1-基本查询（Select…From）" class="headerlink" title="6.1 基本查询（Select…From）"></a>6.1 基本查询（Select…From）</h2><h3 id="6-1-1-全表和特定列查询"><a href="#6-1-1-全表和特定列查询" class="headerlink" title="6.1.1 全表和特定列查询"></a>6.1.1 全表和特定列查询</h3><p>1．全表查询</p><blockquote><p>  hive (default)&gt; select * from emp;</p></blockquote><p>2．选择特定列查询</p><blockquote><p>  hive (default)&gt; select empno, ename from emp;</p></blockquote><blockquote><p>  注意：</p></blockquote><blockquote><p>  （1）SQL 语言**==大小写不敏感。==**</p></blockquote><blockquote><p>  （2）SQL 可以写在一行或者多行</p></blockquote><blockquote><p>  （3）**==关键字不能被缩写也不能分行==**</p></blockquote><blockquote><p>  （4）各子句一般要分行写。</p></blockquote><blockquote><p>  （5）使用缩进提高语句的可读性。</p></blockquote><h3 id="6-1-2-列别名"><a href="#6-1-2-列别名" class="headerlink" title="6.1.2 列别名"></a>6.1.2 列别名</h3><p>1．重命名一个列</p><p>2．便于计算</p><p>3．紧跟列名，**==也可以在列名和别名之间加入关键字‘AS’==**</p><p>4．案例实操</p><blockquote><p>  查询名称和部门</p><p>  hive (default)&gt; select ename AS name, deptno dn from emp;</p></blockquote><h3 id="6-1-3-算术运算符"><a href="#6-1-3-算术运算符" class="headerlink" title="6.1.3 算术运算符"></a>6.1.3 算术运算符</h3><table><thead><tr><th>运算符</th><th>描述</th></tr></thead><tbody><tr><td>A+B</td><td>A和B 相加</td></tr><tr><td>A-B</td><td>A减去B</td></tr><tr><td>A*B</td><td>A和B 相乘</td></tr><tr><td>A/B</td><td>A除以B</td></tr><tr><td>A%B</td><td>A对B取余</td></tr><tr><td>A&amp;B</td><td>A和B按位取与</td></tr><tr><td>A|B</td><td>A和B按位取或</td></tr><tr><td>A^B</td><td>A和B按位取异或</td></tr><tr><td>~A</td><td>A按位取反</td></tr></tbody></table><p>案例实操</p><p>查询出所有员工的薪水后加1显示。</p><blockquote><p>  hive (default)&gt; select sal +1 from emp;</p></blockquote><h3 id="6-1-4-常用函数"><a href="#6-1-4-常用函数" class="headerlink" title="6.1.4 常用函数"></a>6.1.4 常用函数</h3><p>1．求总行数（count）</p><blockquote><p>  hive (default)&gt; select count(*) cnt from emp;</p></blockquote><p>2．求工资的最大值（max）</p><blockquote><p>  hive (default)&gt; select max(sal) max_sal from emp;</p></blockquote><p>3．求工资的最小值（min）</p><blockquote><p>  hive (default)&gt; select min(sal) min_sal from emp;</p></blockquote><p>4．求工资的总和（sum）</p><blockquote><p>  hive (default)&gt; select sum(sal) sum_sal from emp;</p></blockquote><p>5．求工资的平均值（avg）</p><blockquote><p>  hive (default)&gt; select avg(sal) avg_sal from emp;</p></blockquote><h3 id="6-1-5-Limit语句"><a href="#6-1-5-Limit语句" class="headerlink" title="6.1.5 Limit语句"></a>6.1.5 Limit语句</h3><p>典型的查询会返回多行数据。LIMIT子句用于限制返回的行数。</p><blockquote><p>  hive (default)&gt; select * from emp limit 5;</p></blockquote><h2 id="6-2-Where语句"><a href="#6-2-Where语句" class="headerlink" title="6.2 Where语句"></a>6.2 Where语句</h2><p>1．使用WHERE子句，将不满足条件的行过滤掉</p><p>2．WHERE子句紧随FROM子句</p><p>3．案例实操</p><p>查询出薪水大于1000的所有员工</p><blockquote><p>  hive (default)&gt; select * from emp where sal &gt;1000;</p></blockquote><h3 id="6-2-1-比较运算符（Between-In-Is-Null）"><a href="#6-2-1-比较运算符（Between-In-Is-Null）" class="headerlink" title="6.2.1 比较运算符（Between/In/ Is Null）"></a>6.2.1 比较运算符（Between/In/ Is Null）</h3><p>1）下面表中描述了谓词操作符，这些操作符同样可以用于JOIN…ON和HAVING语句中。</p><p>表6-4</p><table><thead><tr><th>操作符</th><th>支持的数据类型</th><th>描述</th></tr></thead><tbody><tr><td>A=B</td><td>基本数据类型</td><td>如果A等于B则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=&gt;B</td><td>基本数据类型</td><td>如果A和B都为NULL，则返回TRUE，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL</td></tr><tr><td>A&lt;&gt;B, A!=B</td><td>基本数据类型</td><td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&gt;B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&gt;=B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A [NOT] BETWEEN B AND C</td><td>基本数据类型</td><td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A IS NULL</td><td>所有数据类型</td><td>如果A等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>A IS NOT NULL</td><td>所有数据类型</td><td>如果A不等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>IN(数值1, 数值2)</td><td>所有数据类型</td><td>使用 IN运算显示列表中的值</td></tr><tr><td>A [NOT] LIKE B</td><td>STRING 类型</td><td>B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A RLIKE B, A REGEXP B</td><td>STRING 类型</td><td>B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td></tr></tbody></table><p>2）案例实操</p><p>（1）查询出薪水等于5000的所有员工</p><blockquote><p>  hive (default)&gt; select * from emp where sal =5000;</p></blockquote><p>（2）查询工资在500到1000的员工信息</p><blockquote><p>  hive (default)&gt; select * from emp where sal between 500 and 1000;</p></blockquote><p>（3）查询comm为空的所有员工信息</p><blockquote><p>  hive (default)&gt; select * from emp where comm is null;</p></blockquote><p>（4）查询工资是1500或5000的员工信息</p><blockquote><p>  hive (default)&gt; select * from emp where sal IN (1500, 5000);</p></blockquote><h3 id="6-2-2-Like和RLike"><a href="#6-2-2-Like和RLike" class="headerlink" title="6.2.2 Like和RLike"></a>6.2.2 Like和RLike</h3><blockquote><p>  1）使用LIKE运算选择类似的值</p><p>  2）选择条件可以包含字符或数字:</p><p>  % 代表零个或多个字符(任意个字符)。</p><p>  _ 代表一个字符。</p><p>  3）RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。</p></blockquote><blockquote><p>4）案例实操</p></blockquote><p>（1）查找以2开头薪水的员工信息</p><blockquote><p>  hive (default)&gt; select * from emp where sal LIKE ‘2%’;</p></blockquote><p>（2）查找第二个数值为2的薪水的员工信息</p><blockquote><p>  hive (default)&gt; select * from emp where sal LIKE ‘_2%’;</p></blockquote><p>（3）查找薪水中含有2的员工信息</p><blockquote><p>  hive (default)&gt; select * from emp where sal RLIKE ‘[2]’;</p></blockquote><h3 id="6-2-3-逻辑运算符（And-Or-Not）"><a href="#6-2-3-逻辑运算符（And-Or-Not）" class="headerlink" title="6.2.3 逻辑运算符（And/Or/Not）"></a>6.2.3 逻辑运算符（And/Or/Not）</h3><table><thead><tr><th>操作符</th><th>含义</th></tr></thead><tbody><tr><td>AND</td><td>逻辑并</td></tr><tr><td>OR</td><td>逻辑或</td></tr><tr><td>NOT</td><td>逻辑否</td></tr></tbody></table><p>案例实操</p><p>（1）查询薪水大于1000，部门是30</p><blockquote><p>  hive (default)&gt; select * from emp where sal&gt;1000 and deptno=30;</p></blockquote><p>（2）查询薪水大于1000，或者部门是30</p><blockquote><p>  hive (default)&gt; select * from emp where sal&gt;1000 or deptno=30;</p></blockquote><p>（3）查询除了20部门和30部门以外的员工信息</p><blockquote><p>  hive (default)&gt; select * from emp where deptno not IN(30, 20);</p></blockquote><h2 id="6-3-分组"><a href="#6-3-分组" class="headerlink" title="6.3 分组"></a>6.3 分组</h2><h3 id="6-3-1-Group-By语句"><a href="#6-3-1-Group-By语句" class="headerlink" title="6.3.1 Group By语句"></a>6.3.1 Group By语句</h3><p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。</p><p>案例实操：</p><p>（1）计算emp表每个部门的平均工资</p><blockquote><p>  hive (default)&gt; select t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno;</p></blockquote><p>（2）计算emp每个部门中每个岗位的最高薪水</p><blockquote><p>  hive (default)&gt; select t.deptno, t.job, max(t.sal) max_sal from emp t group by t.deptno, t.job;</p></blockquote><h3 id="6-3-2-Having语句"><a href="#6-3-2-Having语句" class="headerlink" title="6.3.2 Having语句"></a>6.3.2 Having语句</h3><p>1．having与where不同点</p><p>（1）where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据。</p><p>（2）where后面不能写分组函数，而having后面可以使用分组函数。</p><p>（3）having只用于group by分组统计语句。</p><p>2．案例实操</p><p>（1）求每个部门的平均薪水大于2000的部门</p><blockquote><p>  求每个部门的平均工资</p></blockquote><blockquote><p>  hive (default)&gt; select deptno, avg(sal) from emp group by deptno;</p></blockquote><p>求每个部门的平均薪水大于2000的部门</p><blockquote><p>  hive (default)&gt; select deptno, avg(sal) avg_sal from emp group by deptno having avg_sal &gt; 2000;</p></blockquote><h2 id="6-4-Join语句"><a href="#6-4-Join语句" class="headerlink" title="6.4 Join语句"></a>6.4 Join语句</h2><h3 id="6-4-1-等值Join"><a href="#6-4-1-等值Join" class="headerlink" title="6.4.1 等值Join"></a>6.4.1 等值Join</h3><p>Hive支持通常的SQL JOIN语句，但是**==只支持等值连接，不支持非等值连接。==**</p><p>案例实操</p><p>（1）根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称；</p><blockquote><p>  hive (default)&gt; select e.empno, e.ename, d.deptno, d.dname from emp e join dept d on e.deptno = d.deptno;</p></blockquote><h3 id="6-4-2-表的别名"><a href="#6-4-2-表的别名" class="headerlink" title="6.4.2 表的别名"></a>6.4.2 表的别名</h3><p>1．好处</p><p>（1）使用别名可以简化查询。</p><p>（2）使用表名前缀可以提高执行效率。</p><p>2．案例实操</p><p>合并员工表和部门表</p><blockquote><p>  hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on<br>  e.deptno</p></blockquote><blockquote><p>  = d.deptno;</p></blockquote><h3 id="6-4-3-内连接"><a href="#6-4-3-内连接" class="headerlink" title="6.4.3 内连接"></a>6.4.3 内连接</h3><p>内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p><blockquote><p>  hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno;</p></blockquote><h3 id="6-4-4-左外连接"><a href="#6-4-4-左外连接" class="headerlink" title="6.4.4 左外连接"></a>6.4.4 左外连接</h3><p>左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。</p><blockquote><p>  hive (default)&gt; select e.empno, e.ename, d.deptno from emp e left join dept<br>  d on e.deptno = d.deptno;</p></blockquote><h3 id="6-4-5-右外连接"><a href="#6-4-5-右外连接" class="headerlink" title="6.4.5 右外连接"></a>6.4.5 右外连接</h3><p>右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。</p><blockquote><p>  hive (default)&gt; select e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno = d.deptno;</p></blockquote><h3 id="6-4-6-满外连接"><a href="#6-4-6-满外连接" class="headerlink" title="6.4.6 满外连接"></a>6.4.6 满外连接</h3><p>满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代。</p><blockquote><p>  hive (default)&gt; select e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno = d.deptno;</p></blockquote><h3 id="6-4-7-多表连接"><a href="#6-4-7-多表连接" class="headerlink" title="6.4.7 多表连接"></a>6.4.7 多表连接</h3><p><strong>==注意：连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。==</strong></p><p>数据准备 <a href="./location.txt">location.txt</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1700Beijing</span><br><span class="line">1800London</span><br><span class="line">1900Tokyo</span><br></pre></td></tr></table></figure><p>1．创建位置表 </p><blockquote><p>create table if not exists default.location(</p><p>loc int,</p><p>loc_name string</p><p>)</p><p>row format delimited fields terminated by ‘\t’;</p></blockquote><p>2．导入数据</p><blockquote><p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/location.txt’<br>  into table default.location;</p></blockquote><p>3．多表连接查询</p><blockquote><p>  hive (default)&gt;SELECT e.ename, d.deptno, l. loc_name</p><p>  FROM  emp e </p><p>  JOIN  dept d</p><p>  ON   d.deptno = e.deptno </p><p>  JOIN  location l</p><p>  ON   d.loc = l.loc;</p></blockquote><p>大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce<br>job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce<br>job的输出和表l;进行连接操作。</p><p><strong>==注意：为什么不是表d和表l先进行连接操作呢？这是因为Hive总是按照从左到右的顺序执行的。==</strong></p><h3 id="6-4-8-笛卡尔积"><a href="#6-4-8-笛卡尔积" class="headerlink" title="6.4.8 笛卡尔积"></a>6.4.8 笛卡尔积</h3><p>1．笛卡尔集会在下面条件下产生</p><p>（1）省略连接条件</p><p>（2）连接条件无效</p><p>（3）所有表中的所有行互相连接</p><p>2．案例实操</p><blockquote><p>  hive (default)&gt; select empno, dname from emp, dept;</p></blockquote><h3 id="6-4-9-连接谓词中不支持or"><a href="#6-4-9-连接谓词中不支持or" class="headerlink" title="6.4.9 连接谓词中不支持or"></a>6.4.9 连接谓词中不支持or</h3><blockquote><p>  hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on<br>  e.deptno</p></blockquote><blockquote><p>  = d.deptno or e.ename=d.ename; 错误的</p></blockquote><h2 id="6-5-排序"><a href="#6-5-排序" class="headerlink" title="6.5 排序"></a>6.5 排序</h2><h3 id="6-5-1-全局排序（Order-By）"><a href="#6-5-1-全局排序（Order-By）" class="headerlink" title="6.5.1 全局排序（Order By）"></a>6.5.1 全局排序（Order By）</h3><p><strong>==Order By：全局排序，一个Reducer==</strong></p><p>1．使用 ORDER BY 子句排序</p><p><strong>==SC（ascend）: 升序（默认）==</strong></p><p><strong>==DESC（descend）: 降序==</strong></p><p>2．ORDER BY 子句在SELECT语句的结尾</p><p>3．案例实操</p><p>（1）查询员工信息按工资升序排列</p><blockquote><p>  hive (default)&gt; select * from emp order by sal;</p></blockquote><p>（2）查询员工信息按工资降序排列</p><blockquote><p>  hive (default)&gt; select * from emp order by sal desc;</p></blockquote><h3 id="6-5-2-按照别名排序"><a href="#6-5-2-按照别名排序" class="headerlink" title="6.5.2 按照别名排序"></a>6.5.2 按照别名排序</h3><p>按照员工薪水的2倍排序</p><blockquote><p>  hive (default)&gt; select ename, sal*2 twosal from emp order by twosal;</p></blockquote><h3 id="6-5-3-多个列排序"><a href="#6-5-3-多个列排序" class="headerlink" title="6.5.3 多个列排序"></a>6.5.3 多个列排序</h3><p>按照部门和工资升序排序</p><blockquote><p>  hive (default)&gt; select ename, deptno, sal from emp order by deptno, sal ;</p></blockquote><h3 id="6-5-4-每个MapReduce内部排序（Sort-By）"><a href="#6-5-4-每个MapReduce内部排序（Sort-By）" class="headerlink" title="6.5.4 每个MapReduce内部排序（Sort By）"></a>6.5.4 每个MapReduce内部排序（Sort By）</h3><p>Sort By：每个Reducer内部进行排序，对全局结果集来说不是排序。</p><p>1．设置reduce个数</p><blockquote><p>  hive (default)&gt; set mapreduce.job.reduces=3;</p></blockquote><p>2．查看设置reduce个数</p><blockquote><p>  hive (default)&gt; set mapreduce.job.reduces;</p></blockquote><p>3．根据部门编号降序查看员工信息</p><blockquote><p>  hive (default)&gt; select * from emp sort by empno desc;</p></blockquote><p>4．将查询结果导入到文件中（按照部门编号降序排序）</p><blockquote><p>  hive (default)&gt; insert overwrite local directory ‘/opt/module/datas/sortby-result’</p></blockquote><blockquote><p>  select * from emp sort by deptno desc;</p></blockquote><h3 id="6-5-5-分区排序（Distribute-By）"><a href="#6-5-5-分区排序（Distribute-By）" class="headerlink" title="6.5.5 分区排序（Distribute By）"></a>6.5.5 分区排序（Distribute By）</h3><p>Distribute By：类似MR中partition，进行分区，结合sort by使用。</p><p><strong>==注意，Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。==</strong></p><p>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute<br>by的效果。</p><p>案例实操：</p><p>（1）先按照部门编号分区，再按照员工编号降序排序。</p><blockquote><p>  hive (default)&gt; set mapreduce.job.reduces=3;</p></blockquote><blockquote><p>  hive (default)&gt; insert overwrite local directory<br>  ‘/opt/module/datas/distribute-result’ select * from emp distribute by<br>  deptno sort by empno desc;</p></blockquote><h3 id="6-5-6-Cluster-By"><a href="#6-5-6-Cluster-By" class="headerlink" title="6.5.6 Cluster By"></a>6.5.6 Cluster By</h3><p>当distribute by和sorts by字段相同时，可以使用cluster by方式。</p><p>cluster by除了具有distribute by的功能外还兼具sort<br>by的功能。但是排序**==只能是升序排序==**，不能指定排序规则为ASC或者DESC。</p><p>1）以下两种写法等价</p><blockquote><p>  hive (default)&gt; select * from emp cluster by deptno;</p></blockquote><blockquote><p>  hive (default)&gt; select * from emp distribute by deptno sort by deptno;</p></blockquote><p>注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</p><h2 id="6-6-分桶及抽样查询"><a href="#6-6-分桶及抽样查询" class="headerlink" title="6.6 分桶及抽样查询"></a>6.6 <a name="6.6">分桶</a>及抽样查询</h2><h3 id="6-6-1-分桶表数据存储"><a href="#6-6-1-分桶表数据存储" class="headerlink" title="6.6.1 分桶表数据存储"></a>6.6.1 分桶表数据存储</h3><p><strong>分区针对的是数据的存储路径；==分桶针对的是数据文件==。</strong></p><p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区，特别是之前所提到过的要确定合适的划分大小这个疑虑。</p><p>分桶是将数据集分解成更容易管理的若干部分的另一个技术。</p><p>1．先创建分桶表，通过直接导入数据文件的方式</p><p>（1）数据准备 <a href="./student.txt">student.txt</a></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1001ss1</span><br><span class="line">1002ss2</span><br><span class="line">1003ss3</span><br><span class="line">1004ss4</span><br><span class="line">1005ss5</span><br><span class="line">1006ss6</span><br><span class="line">1007ss7</span><br><span class="line">1008ss8</span><br><span class="line">1009ss9</span><br><span class="line">1010ss10</span><br><span class="line">1011ss11</span><br><span class="line">1012ss12</span><br><span class="line">1013ss13</span><br><span class="line">1014ss14</span><br><span class="line">1015ss15</span><br><span class="line">1016ss16</span><br></pre></td></tr></table></figure><p>（2）创建分桶表</p><blockquote><p>create table stu_buck(id int, name string)</p><p>clustered by(id) </p><p>into 4 buckets</p><p>row format delimited fields terminated by ‘\t’;</p></blockquote><p>（3）查看表结构</p><blockquote><p>  hive (default)&gt; desc formatted stu_buck;</p></blockquote><blockquote><p>  Num Buckets: 4</p></blockquote><p>（4）导入数据到分桶表中</p><blockquote><p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/student.txt’ into table stu_buck;</p></blockquote><p>（5）查看创建的分桶表中是否分成4个桶,发现并没有分成4个桶。是什么原因呢？</p><p>2．创建分桶表时，数据通过子查询的方式导入</p><p>（1）先建一个普通的stu表</p><blockquote><p>create table stu(id int, name string) </p><p>row format delimited fields terminated by ‘\t’; </p></blockquote><p>（2）向普通的stu表中导入数据</p><blockquote><p>load data local inpath ‘/opt/module/datas/student.txt’ into table stu; </p></blockquote><p>（3）清空stu_buck表中数据</p><blockquote><p>truncate table stu_buck; select * from stu_buck; </p></blockquote><p>（4）导入数据到分桶表，通过子查询的方式</p><blockquote><p>insert into table stu_buck select id, name from stu; </p></blockquote><p>（5）发现还是只有一个分桶</p><p>（6）需要设置一个属性</p><blockquote><p>hive (default)&gt; set hive.enforce.bucketing=true;</p><p> hive (default)&gt; set mapreduce.job.reduces=-1; </p><p>hive (default)&gt; insert into table stu_buck select id, name from stu; </p></blockquote><p>（7）查询分桶的数据,在hdfs上查看已经分桶</p><blockquote><p>hive (default)&gt; select * from stu_buck; </p><p>OK</p><p>stu_buck.id   stu_buck.name</p><p>1004  ss4</p><p>1008  ss8</p><p>1012  ss12</p><p>1016  ss16</p><p>1001  ss1</p><p>1005  ss5</p><p>1009  ss9</p><p>1013  ss13</p><p>1002  ss2</p><p>1006  ss6</p><p>1010  ss10</p><p>1014  ss14</p><p>1003  ss3</p><p>1007  ss7</p><p>1011  ss11</p><p>1015  ss15</p></blockquote><h3 id="6-6-2-分桶抽样查询"><a href="#6-6-2-分桶抽样查询" class="headerlink" title="6.6.2 分桶抽样查询"></a>6.6.2 分桶抽样查询</h3><p>对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。</p><p>查询表stu_buck中的数据。</p><blockquote><p>hive (default)&gt; select * from stu_buck tablesample(bucket 1 out of 4 on id); </p></blockquote><p>注：tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y) 。</p><p>y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。</p><p>**==x表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。==**例如，table总bucket数为4，tablesample(bucket1 out of 2)，表示总共抽取（4/2=）2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据。</p><p><strong>==注意：x的值必须小于等于y的值，否则==</strong></p><p>FAILED: SemanticException [Error 10061]: Numerator should not be bigger than<br>denominator in sample clause for table stu_buck</p><h2 id="6-7-其他常用查询函数"><a href="#6-7-其他常用查询函数" class="headerlink" title="6.7 其他常用查询函数"></a>6.7 其他常用查询函数</h2><h3 id="6-7-1-空字段赋值"><a href="#6-7-1-空字段赋值" class="headerlink" title="6.7.1 空字段赋值"></a>6.7.1 空字段赋值</h3><ol><li>函数说明</li></ol><p>NVL：给值为NULL的数据赋值，它的格式是NVL( string1, replace_with)。它的功能是如果string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都为NULL ，则返回NULL。</p><ol start="2"><li>数据准备：采用员工表</li></ol><p>3.查询：如果员工的comm为NULL，则用-1代替</p><blockquote><p>hive (default)&gt; select nvl(comm,-1) from emp;</p><p>OK</p><p>_c0</p><p>20.0</p><p>300.0</p><p>500.0</p><p>-1.0</p><p>1400.0</p><p>-1.0</p><p>-1.0</p><p>-1.0</p><p>-1.0</p><p>0.0</p><p>-1.0</p><p>-1.0</p><p>-1.0</p><p>-1.0</p></blockquote><ol start="4"><li>查询：如果员工的comm为NULL，则用领导id代替</li></ol><blockquote><p>hive (default)&gt; select nvl(comm,mgr) from emp;</p><p>OK</p><p>_c0</p><p>20.0</p><p>300.0</p><p>500.0</p><p>7839.0</p><p>1400.0</p><p>7839.0</p><p>7839.0</p><p>7566.0</p><p>NULL</p><p>0.0</p><p>7788.0</p><p>7698.0</p><p>7566.0</p></blockquote><h3 id="6-7-2-CASE-WHEN"><a href="#6-7-2-CASE-WHEN" class="headerlink" title="6.7.2 CASE WHEN"></a>6.7.2 CASE WHEN</h3><ol><li>数据准备</li></ol><table><thead><tr><th>name</th><th>dept_id</th><th>sex</th></tr></thead><tbody><tr><td>悟空</td><td>A</td><td>男</td></tr><tr><td>大海</td><td>A</td><td>男</td></tr><tr><td>宋宋</td><td>B</td><td>男</td></tr><tr><td>凤姐</td><td>A</td><td>女</td></tr><tr><td>婷姐</td><td>B</td><td>女</td></tr><tr><td>婷婷</td><td>B</td><td>女</td></tr></tbody></table><p>2．需求</p><blockquote><p>  求出不同部门男女各多少人。结果如下：</p><p>  A 2 1</p></blockquote><blockquote><p>  B 1 2</p></blockquote><p>3．创建本地emp_sex.txt，导入数据</p><blockquote><p>  [xing@hadoop102 datas]$ vi emp_sex.txt</p><p>  悟空 A  男</p><p>  大海 A  男</p><p>  宋宋 B  男</p><p>  凤姐 A  女</p><p>  婷姐 B  女</p><p>  婷婷 B  女</p></blockquote><p>4．创建hive表并导入数据</p><blockquote><p>  create table emp_sex(</p><p>  name string, </p><p>  dept_id string, </p><p>  sex string) </p><p>  row format delimited fields terminated by “\t”;</p><p>  load data local inpath ‘/opt/module/datas/emp_sex.txt’ into table emp_sex;</p></blockquote><p>5．按需求查询数据</p><blockquote><p>select </p><p>dept_id,</p><p>sum(case sex when ‘男’ then 1 else 0 end) male_count,</p><p>sum(case sex when ‘女’ then 1 else 0 end) female_count</p><p>from </p><p>emp_sex</p><p>group by</p><p>dept_id;</p></blockquote><h3 id="6-7-2-行转列"><a href="#6-7-2-行转列" class="headerlink" title="6.7.2 行转列"></a>6.7.2 行转列</h3><p>1．相关函数说明</p><blockquote><p>  CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;</p></blockquote><blockquote><p>  CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL和空字符串。分隔符将被加到被连接的字符串之间;</p></blockquote><blockquote><p>  COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。</p></blockquote><p>2．数据准备</p><table><thead><tr><th>name</th><th>constellation</th><th>blood_type</th></tr></thead><tbody><tr><td>孙悟空</td><td>白羊座</td><td>A</td></tr><tr><td>大海</td><td>射手座</td><td>A</td></tr><tr><td>宋宋</td><td>白羊座</td><td>B</td></tr><tr><td>猪八戒</td><td>白羊座</td><td>A</td></tr><tr><td>凤姐</td><td>射手座</td><td>A</td></tr></tbody></table><p>3．需求</p><blockquote><p>  把星座和血型一样的人归类到一起。结果如下：</p></blockquote><blockquote><p>  射手座,A      大海|凤姐</p><p>  白羊座,A      孙悟空|猪八戒</p><p>  白羊座,B       宋宋</p></blockquote><p>4．创建本地constellation.txt，导入数据</p><blockquote><p>  [xing@hadoop102 datas]$ vi constellation.txt</p><p>  孙悟空  白羊座  A</p><p>  大海    射手座  A</p><p>  宋宋    白羊座  B</p><p>  猪八戒  白羊座  A</p><p>  凤姐    射手座  A</p></blockquote><p>5．创建hive表并导入数据</p><blockquote><p>  create table person_info(</p><p>  name string, </p><p>  constellation string, </p><p>  blood_type string) </p><p>  row format delimited fields terminated by “\t”;</p><p>  load data local inpath “/opt/module/datas/person_info.txt” into table person_info;</p></blockquote><p>6．按需求查询数据</p><blockquote><p>select</p><p> t1.base,</p><p> concat_ws(‘|’, collect_set(t1.name)) name</p><p>from</p><p> (select</p><p>​    name,</p><p>​    concat(constellation, “,”, blood_type) base</p><p> from</p><p>​    person_info) t1</p><p>group by</p><p> t1.base;</p></blockquote><h3 id="6-7-3-列转行"><a href="#6-7-3-列转行" class="headerlink" title="6.7.3 列转行"></a>6.7.3 列转行</h3><p>1．函数说明</p><p>EXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行。</p><p>LATERAL VIEW</p><p>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</p><p>解释：用于和split,<br>explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p><p>2．数据准备</p><table><thead><tr><th>movie</th><th>category</th></tr></thead><tbody><tr><td>《疑犯追踪》</td><td>悬疑,动作,科幻,剧情</td></tr><tr><td>《Lie to me》</td><td>悬疑,警匪,动作,心理,剧情</td></tr><tr><td>《战狼2》</td><td>战争,动作,灾难</td></tr></tbody></table><p>3．需求</p><blockquote><p>  将电影分类中的数组数据展开。结果如下：</p></blockquote><blockquote><p>  《疑犯追踪》   悬疑</p><p>  《疑犯追踪》   动作</p><p>  《疑犯追踪》   科幻</p><p>  《疑犯追踪》   剧情</p><p>  《Lie to me》  悬疑</p><p>  《Lie to me》  警匪</p><p>  《Lie to me》  动作</p><p>  《Lie to me》  心理</p><p>  《Lie to me》  剧情</p><p>  《战狼2》     战争</p><p>  《战狼2》     动作</p><p>  《战狼2》     灾难</p></blockquote><p>4．创建本地movie.txt，导入数据</p><blockquote><p>  [xing@hadoop102 datas]$ vi movie.txt</p><p>  《疑犯追踪》  悬疑,动作,科幻,剧情</p><p>  《Lie to me》 悬疑,警匪,动作,心理,剧情</p><p>  《战狼2》 战争,动作,灾难</p></blockquote><p>5．创建hive表并导入数据</p><blockquote><p>create table movie_info(</p><p> movie string, </p><p> category array<string>) </p><p>row format delimited fields terminated by “\t”</p><p>collection items terminated by “,”;</p><p>load data local inpath “/opt/module/datas/movie.txt” into table movie_info;</p></blockquote><p>6．按需求查询数据</p><blockquote><p>select</p><p> movie,</p><p> category_name</p><p>from </p><p> movie_info lateral view explode(category) table_tmp as category_name;</p></blockquote><h3 id="6-7-4-窗口函数"><a href="#6-7-4-窗口函数" class="headerlink" title="6.7.4 窗口函数"></a>6.7.4 窗口函数</h3><p>1．相关函数说明</p><blockquote><p>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化</p></blockquote><blockquote><p>  CURRENT ROW：当前行</p></blockquote><blockquote><p>  n PRECEDING：往前n行数据</p></blockquote><blockquote><p>  n FOLLOWING：往后n行数据</p></blockquote><blockquote><p>UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点</p></blockquote><blockquote><p>  LAG(col,n)：往前第n行数据</p></blockquote><blockquote><p>  LEAD(col,n)：往后第n行数据</p></blockquote><blockquote><p>  NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。<strong>注意：n必须为int类型。</strong></p></blockquote><p>2．数据准备：name，orderdate，cost</p><blockquote><p>  jack,2017-01-01,10</p><p>  tony,2017-01-02,15</p><p>  jack,2017-02-03,23</p><p>  tony,2017-01-04,29</p><p>  jack,2017-01-05,46</p><p>  jack,2017-04-06,42</p><p>  tony,2017-01-07,50</p><p>  jack,2017-01-08,55</p><p>  mart,2017-04-08,62</p><p>  mart,2017-04-09,68</p><p>  neil,2017-05-10,12</p><p>  mart,2017-04-11,75</p><p>  neil,2017-06-12,80</p><p>  mart,2017-04-13,94</p></blockquote><p>3．需求</p><ol><li><p>查询在2017年4月份购买过的顾客及总人数</p></li><li><p>查询顾客的购买明细及月购买总额</p></li><li><p>上述的场景,要将cost按照日期进行累加</p></li><li><p>查询顾客上次的购买时间</p></li><li><p>查询前20%时间的订单信息</p></li></ol><p>4．创建本地business.txt，导入数据</p><blockquote><p>  [xing@hadoop102 datas]$ vi business.txt</p></blockquote><p>5．创建hive表并导入数据</p><blockquote><p>create table business(</p><p>name string, </p><p>orderdate string,</p><p>cost int</p><p>) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’;</p><p>load data local inpath “/opt/module/datas/business.txt” into table business;</p></blockquote><p>6．按需求查询数据</p><p>（1）查询在2017年4月份购买过的顾客及总人数</p><blockquote><p>select name,count(*) over ()   from business   where substring(orderdate,1,7) = ‘2017-04’   group by name;  </p></blockquote><p>（2）查询顾客的购买明细及月购买总额</p><blockquote><p>select name,orderdate,cost,sum(cost) over(partition  by month(orderdate)) from   business;  </p></blockquote><p>（3）上述的场景,要将cost按照日期进行累加</p><blockquote><p>select name,orderdate,cost,  </p><p> sum(cost) over() as sample1,–所有行相加 </p><p> sum(cost) over(partition by name) as sample2,–按name分组，组内数据相加   </p><p>sum(cost) over(partition by name order by  orderdate) as sample3,–按name分组，组内数据累加   sum(cost) over(partition by name order by orderdate  rows between UNBOUNDED PRECEDING and current row ) as sample4 ,–和sample3一样,由起点到当前行的聚合   </p><p>sum(cost) over(partition by name order by orderdate  rows between 1 PRECEDING and current row) as sample5, –当前行和前面一行做聚合   </p><p>sum(cost) over(partition by name order by orderdate  rows between 1 PRECEDING AND 1 FOLLOWING ) as sample6,–当前行和前边一行及后面一行   </p><p>sum(cost) over(partition by name order by orderdate  rows between current row and UNBOUNDED FOLLOWING ) as sample7 –当前行及后面所有行   from business;  </p></blockquote><p>（4）查看顾客上次的购买时间</p><blockquote><p>select name,orderdate,cost,   lag(orderdate,1,’1900-01-01’) over(partition by  name order by orderdate ) as time1, lag(orderdate,2) over (partition by name  order by orderdate) as time2   from business;  </p></blockquote><p>（5）查询前20%时间的订单信息</p><blockquote><p>select * from ( select name,orderdate,cost, ntile(5) over(order by orderdate) sorted</p></blockquote><h3 id="6-7-5-Rank"><a href="#6-7-5-Rank" class="headerlink" title="6.7.5 Rank"></a>6.7.5 Rank</h3><p>1．函数说明</p><blockquote><p>  RANK() 排序相同时会重复，总数不会变</p></blockquote><blockquote><p>  DENSE_RANK() 排序相同时会重复，总数会减少</p></blockquote><blockquote><p>  ROW_NUMBER() 会根据顺序计算</p></blockquote><p>2．数据准备</p><table><thead><tr><th>name</th><th>subject</th><th>score</th></tr></thead><tbody><tr><td>孙悟空</td><td>语文</td><td>87</td></tr><tr><td>孙悟空</td><td>数学</td><td>95</td></tr><tr><td>孙悟空</td><td>英语</td><td>68</td></tr><tr><td>大海</td><td>语文</td><td>94</td></tr><tr><td>大海</td><td>数学</td><td>56</td></tr><tr><td>大海</td><td>英语</td><td>84</td></tr><tr><td>宋宋</td><td>语文</td><td>64</td></tr><tr><td>宋宋</td><td>数学</td><td>86</td></tr><tr><td>宋宋</td><td>英语</td><td>84</td></tr><tr><td>婷婷</td><td>语文</td><td>65</td></tr><tr><td>婷婷</td><td>数学</td><td>85</td></tr><tr><td>婷婷</td><td>英语</td><td>78</td></tr></tbody></table><p>3．需求</p><blockquote><p>  计算每门学科成绩排名。</p></blockquote><p>4．创建本地movie.txt，导入数据</p><blockquote><p>  [xing@hadoop102 datas]$ vi score.txt</p></blockquote><p>5．创建hive表并导入数据</p><blockquote><p>create table score(</p><p>name string,</p><p>subject string, </p><p>score int) </p><p>row format delimited fields terminated by “\t”;</p><p>load data local inpath ‘/opt/module/datas/score.txt’ into table score;</p></blockquote><p>6．按需求查询数据</p><blockquote><p>select name,</p><p>subject,</p><p>score,</p><p>rank() over(partition by subject order by score desc) rp,</p><p>dense_rank() over(partition by subject order by score desc) drp,</p><p>row_number() over(partition by subject order by score desc) rmp</p><p>from score;</p></blockquote><blockquote><p>name  subject score  rp   drp   rmp</p><p>孙悟空 数学  95   1    1    1</p><p>宋宋  数学  86   2    2    2</p><p>婷婷  数学  85   3    3    3</p><p>大海  数学  56   4    4    4</p><p><strong>宋宋  英语  84   1    1    1</strong></p><p><strong>大海  英语  84   1    1    2</strong></p><p><strong>婷婷  英语  78   3    2    3</strong></p><p><strong>孙悟空 英语  68   4    3    4</strong></p><p>大海  语文  94   1    1    1</p><p>孙悟空 语文  87   2    2    2</p><p>婷婷  语文  65   3    3    3</p><p>宋宋  语文  64   4    4    4</p></blockquote><h1 id="第7章-函数"><a href="#第7章-函数" class="headerlink" title="第7章 函数"></a>第7章 函数</h1><h2 id="7-1-系统内置函数"><a href="#7-1-系统内置函数" class="headerlink" title="7.1 系统内置函数"></a>7.1 系统内置函数</h2><p>1．查看系统自带的函数</p><blockquote><p>  hive&gt; show functions;</p></blockquote><p>2．显示自带的函数的用法</p><blockquote><p>  hive&gt; desc function upper;</p></blockquote><p>3．详细显示自带的函数的用法</p><blockquote><p>  hive&gt; desc function extended upper;</p></blockquote><h2 id="7-2-自定义函数"><a href="#7-2-自定义函数" class="headerlink" title="7.2 自定义函数"></a>7.2 自定义函数</h2><blockquote><p>  1）Hive自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。</p></blockquote><blockquote><p>  2）当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-definedfunction）。</p></blockquote><blockquote><p>  3）根据用户自定义函数类别分为以下三种：</p></blockquote><blockquote><p>  （1）<strong>UDF</strong>（User-Defined-Function）一进一出</p></blockquote><blockquote><p>  （2）UDAF（User-Defined Aggregation Function）聚集函数，多进一出 类似于：count/max/min</p></blockquote><blockquote><p>  （3）UDTF（User-Defined Table-Generating Functions）一进多出 如lateral view explore()</p></blockquote><blockquote><p>  4）官方文档地址</p></blockquote><blockquote><p>  <a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">https://cwiki.apache.org/confluence/display/Hive/HivePlugins</a></p></blockquote><blockquote><p>  5）<strong>编程步骤：</strong></p></blockquote><blockquote><p>  （1）继承org.apache.hadoop.hive.ql.UDF</p></blockquote><blockquote><p>  （2）需要实现evaluate函数；evaluate函数支持重载；</p></blockquote><blockquote><p>  （3）在hive的命令行窗口创建函数</p></blockquote><blockquote><p>  ​        <strong>a）添加jar</strong></p></blockquote><blockquote><p>  ​            add jar linux_jar_path</p></blockquote><blockquote><p>  ​        <strong>b）创建function，</strong></p></blockquote><blockquote><p>  ​            create [temporary] function [dbname.]function_name AS class_name;</p></blockquote><blockquote><p>  （4）在hive的命令行窗口删除函数</p></blockquote><blockquote><p>  ​        Drop [temporary] function [if exists] [dbname.]function_name;</p></blockquote><blockquote><p>  6）注意事项</p></blockquote><blockquote><p>  （1）UDF必须要有返回类型，可以返回null，但是返回类型不能为void；</p></blockquote><h2 id="7-3-自定义UDF函数"><a href="#7-3-自定义UDF函数" class="headerlink" title="7.3 自定义UDF函数"></a>7.3 自定义UDF函数</h2><p>1．创建一个Maven工程Hive</p><p>2．导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><p>3．创建一个类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing.hive;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Lower</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span> <span class="params">(<span class="keyword">final</span> String s)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (s == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> s.toLowerCase();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>4．打成jar包上传到服务器/opt/module/jars/udf.jar</p><p>5．将jar包添加到hive的classpath</p><blockquote><p>  hive (default)&gt; add jar /opt/module/datas/udf.jar;</p></blockquote><p>6．创建临时函数与开发好的java class关联</p><blockquote><p>  hive (default)&gt; create temporary function mylower as “com.xing.hive.Lower”;</p></blockquote><p>7．即可在hql中使用自定义的函数strip</p><blockquote><p>  hive (default)&gt; select ename, mylower(ename) lowername from emp;</p></blockquote><h1 id="第8章-压缩和存储"><a href="#第8章-压缩和存储" class="headerlink" title="第8章 压缩和存储"></a>第8章 <a name="8">压缩和存储</a></h1><h2 id="8-1-Hadoop源码编译支持Snappy压缩"><a href="#8-1-Hadoop源码编译支持Snappy压缩" class="headerlink" title="8.1 Hadoop源码编译支持Snappy压缩"></a>8.1 Hadoop源码编译支持Snappy压缩</h2><h3 id="8-1-1-资源准备"><a href="#8-1-1-资源准备" class="headerlink" title="8.1.1 资源准备"></a>8.1.1 资源准备</h3><p>1．CentOS联网</p><p>配置CentOS能连接外网。Linux虚拟机ping <a href="http://www.baidu.com/">www.baidu.com</a><br>是畅通的</p><p><strong>注意：采用root角色编译，减少文件夹权限出现问题</strong></p><p>2．jar包准备(hadoop源码、JDK8 、maven、protobuf)</p><p>（1）hadoop-2.7.2-src.tar.gz</p><p>（2）jdk-8u144-linux-x64.tar.gz</p><p>（3）snappy-1.1.3.tar.gz</p><p>（4）apache-maven-3.0.5-bin.tar.gz</p><p>（5）protobuf-2.5.0.tar.gz</p><h3 id="8-1-2-jar包安装"><a href="#8-1-2-jar包安装" class="headerlink" title="8.1.2 jar包安装"></a>8.1.2 jar包安装</h3><p><strong>注意：所有操作必须在root用户下完成</strong></p><p>1．JDK解压、配置环境变量JAVA_HOME和PATH，验证<a href="http://lib.csdn.net/base/javase">java</a>-version(如下都需要验证是否配置成功)</p><blockquote><p>[root@hadoop101 software] # tar -zxf jdk-8u144-linux-x64.tar.gz -C /opt/module/</p><p>[root@hadoop101 software]# vi /etc/profile</p><p> #JAVA_HOME  export  JAVA_HOME=/opt/module/jdk1.8.0_144  export  PATH=$PATH:$JAVA_HOME/bin  </p><p>[root@hadoop101 software]#source /etc/profile</p></blockquote><p><strong>验证命令：java -version</strong></p><p>2．Maven解压、配置 MAVEN_HOME和PATH</p><blockquote><p>[root@hadoop101 software]# tar -zxvf apache-maven-3.0.5-bin.tar.gz -C /opt/module/</p><p>[root@hadoop101 apache-maven-3.0.5]# vi /etc/profile</p><p> #MAVEN_HOME  export  MAVEN_HOME=/opt/module/apache-maven-3.0.5  export  PATH=$PATH:$MAVEN_HOME/bin  </p><p>[root@hadoop101 software]#source /etc/profile</p></blockquote><p><strong>验证命令：mvn -version</strong></p><h3 id="8-1-3-编译源码"><a href="#8-1-3-编译源码" class="headerlink" title="8.1.3 编译源码"></a>8.1.3 编译源码</h3><p>1．准备编译环境</p><blockquote><p>  [root@hadoop101 software]# yum install svn</p></blockquote><blockquote><p>  [root@hadoop101 software]# yum install autoconf automake libtool cmake</p></blockquote><blockquote><p>  [root@hadoop101 software]# yum install ncurses-devel</p></blockquote><blockquote><p>  [root@hadoop101 software]# yum install openssl-devel</p></blockquote><blockquote><p>  [root@hadoop101 software]# yum install gcc*</p></blockquote><p>2．编译安装snappy</p><blockquote><p>  [root@hadoop101 software]# tar -zxvf snappy-1.1.3.tar.gz -C /opt/module/</p></blockquote><blockquote><p>  [root@hadoop101 module]# cd snappy-1.1.3/</p></blockquote><blockquote><p>  [root@hadoop101 snappy-1.1.3]# ./configure</p></blockquote><blockquote><p>  [root@hadoop101 snappy-1.1.3]# make</p></blockquote><blockquote><p>  [root@hadoop101 snappy-1.1.3]# make install</p></blockquote><blockquote><p>  # 查看snappy库文件</p></blockquote><blockquote><p>  [root@hadoop101 snappy-1.1.3]# ls -lh /usr/local/lib |grep snappy</p></blockquote><p>3．编译安装protobuf</p><blockquote><p>  [root@hadoop101 software]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/</p></blockquote><blockquote><p>  [root@hadoop101 module]# cd protobuf-2.5.0/</p></blockquote><blockquote><p>  [root@hadoop101 protobuf-2.5.0]# ./configure</p></blockquote><blockquote><p>  [root@hadoop101 protobuf-2.5.0]# make</p></blockquote><blockquote><p>  [root@hadoop101 protobuf-2.5.0]# make install</p></blockquote><blockquote><p>  # 查看protobuf版本以测试是否安装成功<br>  [root@hadoop101 protobuf-2.5.0]# protoc –version</p></blockquote><p>4．编译hadoop native</p><blockquote><p>  [root@hadoop101 software]# tar -zxvf hadoop-2.7.2-src.tar.gz</p></blockquote><blockquote><p>  [root@hadoop101 software]# cd hadoop-2.7.2-src/</p></blockquote><blockquote><p>  [root@hadoop101 software]# mvn clean package -DskipTests -Pdist,native<br>  -Dtar -Dsnappy.lib=/usr/local/lib -Dbundle.snappy</p></blockquote><blockquote><p>  执行成功后，/opt/software/hadoop-2.7.2-src/hadoop-dist/target/<a href="http://lib.csdn.net/base/hadoop">hadoop</a>-2.7.2.tar.gz即为新生成的支持snappy压缩的二进制安装包。</p></blockquote><h2 id="8-2-Hadoop压缩配置"><a href="#8-2-Hadoop压缩配置" class="headerlink" title="8.2 Hadoop压缩配置"></a>8.2 Hadoop压缩配置</h2><h3 id="8-2-1-MR支持的压缩编码"><a href="#8-2-1-MR支持的压缩编码" class="headerlink" title="8.2.1 MR支持的压缩编码"></a>8.2.1 MR支持的压缩编码</h3><table><thead><tr><th>压缩格式</th><th>工具</th><th>算法</th><th>文件扩展名</th><th>是否可切分</th></tr></thead><tbody><tr><td>DEFAULT</td><td>无</td><td>DEFAULT</td><td>.deflate</td><td>否</td></tr><tr><td>Gzip</td><td>gzip</td><td>DEFAULT</td><td>.gz</td><td>否</td></tr><tr><td>bzip2</td><td>bzip2</td><td>bzip2</td><td>.bz2</td><td>是</td></tr><tr><td>LZO</td><td>lzop</td><td>LZO</td><td>.lzo</td><td>是</td></tr><tr><td>Snappy</td><td>无</td><td>Snappy</td><td>.snappy</td><td>否</td></tr></tbody></table><p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示：</p><table><thead><tr><th>压缩格式</th><th>对应的编码/解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><p>压缩性能的比较：</p><table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody><tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB/s</td><td>58MB/s</td></tr><tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB/s</td><td>9.5MB/s</td></tr><tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB/s</td><td>74.6MB/s</td></tr></tbody></table><p><a href="http://google.github.io/snappy/">http://google.github.io/snappy/</a></p><p>On a single core of a Core i7 processor in 64-bit mode, Snappy <strong>compresses</strong> at<br>about <strong>250 MB/sec</strong> or more and <strong>decompresses</strong> at about <strong>500 MB/sec</strong> or more.</p><h3 id="8-2-2-压缩参数配置"><a href="#8-2-2-压缩参数配置" class="headerlink" title="8.2.2 压缩参数配置"></a>8.2.2 压缩参数配置</h3><p>要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：</p><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs  （在core-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.Lz4Codec</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compress</td><td>false</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.map.output.compress.codec</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>使用LZO、LZ4或snappy编解码器在此阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutputformat.compress</td><td>false</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.codec</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.type</td><td>RECORD</td><td>reducer输出</td><td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td></tr></tbody></table><h2 id="8-3-开启Map输出阶段压缩"><a href="#8-3-开启Map输出阶段压缩" class="headerlink" title="8.3 开启Map输出阶段压缩"></a>8.3 开启Map输出阶段压缩</h2><p>开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下：</p><p><strong>案例实操：</strong></p><p>1．开启hive中间传输数据压缩功能</p><blockquote><p>  hive (default)&gt;set hive.exec.compress.intermediate=true;</p></blockquote><p>2．开启mapreduce中map输出压缩功能</p><blockquote><p>  hive (default)&gt;set mapreduce.map.output.compress=true;</p></blockquote><p>3．设置mapreduce中map输出数据的压缩方式</p><blockquote><p>  hive (default)&gt;set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;</p></blockquote><p>4．执行查询语句</p><blockquote><p>  hive (default)&gt; select count(ename) name from emp;</p></blockquote><h2 id="8-4-开启Reduce输出阶段压缩"><a href="#8-4-开启Reduce输出阶段压缩" class="headerlink" title="8.4 开启Reduce输出阶段压缩"></a>8.4 开启Reduce输出阶段压缩</h2><p>当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。</p><p><strong>案例实操：</strong></p><p>1．开启hive最终输出数据压缩功能</p><blockquote><p>  hive (default)&gt;set hive.exec.compress.output=true;</p></blockquote><p>2．开启mapreduce最终输出数据压缩</p><blockquote><p>  hive (default)&gt;set mapreduce.output.fileoutputformat.compress=true;</p></blockquote><p>3．设置mapreduce最终数据输出压缩方式</p><blockquote><p>  hive (default)&gt; set mapreduce.output.fileoutputformat.compress.codec =org.apache.hadoop.io.compress.SnappyCodec;</p></blockquote><p>4．设置mapreduce最终数据输出压缩为块压缩</p><blockquote><p>  hive (default)&gt; set mapreduce.output.fileoutputformat.compress.type=BLOCK;</p></blockquote><p>5．测试一下输出结果是否是压缩文件</p><blockquote><p>  hive (default)&gt; insert overwrite local directory ‘/opt/module/datas/distribute-result’ select * from emp distribute by deptno sort by empno desc;</p></blockquote><h2 id="8-5-文件存储格式"><a href="#8-5-文件存储格式" class="headerlink" title="8.5 文件存储格式"></a>8.5 文件存储格式</h2><p>Hive支持的存储数的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。</p><h3 id="8-5-1-列式存储和行式存储"><a href="#8-5-1-列式存储和行式存储" class="headerlink" title="8.5.1 列式存储和行式存储"></a>8.5.1 列式存储和行式存储</h3><p>列式存储和行式存储</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113931.png" alt="image-20200918011108143"></p><p>所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p><p>1．行存储的特点</p><p>查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p><p>2．列存储的特点</p><p>因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</p><p><strong>==TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；==</strong></p><p><strong>==ORC和PARQUET是基于列式存储的。==</strong></p><h3 id="8-5-2-TextFile格式"><a href="#8-5-2-TextFile格式" class="headerlink" title="8.5.2 TextFile格式"></a>8.5.2 TextFile格式</h3><p>默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。</p><h3 id="8-5-3-Orc格式"><a href="#8-5-3-Orc格式" class="headerlink" title="8.5.3 Orc格式"></a>8.5.3 Orc格式</h3><p>Orc (Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式。</p><p>如图所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当RowGroup概念，不过大小由4MB-&gt;250MB，这样应该能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index<br>Data，Row Data，Stripe Footer：</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922163426.png" alt="image-20200922163424885"></p><p> Orc格式</p><p>1）Index Data：一个轻量级的index，默认是**==每隔1W行做一个索引==**。这里做的索引应该只是记录某行的各字段在RowData中的offset。</p><p>2）RowData：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。</p><p>3）Stripe Footer：存的是各个Stream的类型，长度等信息。</p><p>每个文件有一个FileFooter，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到FileFooter长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。</p><h3 id="8-5-4-Parquet格式"><a href="#8-5-4-Parquet格式" class="headerlink" title="8.5.4 Parquet格式"></a>8.5.4 Parquet格式</h3><p>Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。</p><p>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，**==因此Parquet格式文件是自解析的。==**</p><p>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把**==每一个行组由一个Mapper任务处理，增大任务执行并行度。==**Parquet文件的格式如图所示。</p><p><img src="d:\桌面\冉辰星总结\03-hive\Hive(2)详细.assets\20200918113932.png" alt="image-20200918011216597"></p><p> Parquet格式</p><p>上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic<br>Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：**==数据页、字典页和索引页==**。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。</p><h3 id="8-5-5-主流文件存储格式对比实验"><a href="#8-5-5-主流文件存储格式对比实验" class="headerlink" title="8.5.5 主流文件存储格式对比实验"></a>8.5.5 主流文件存储格式对比实验</h3><p>从存储文件的压缩比和查询速度两个角度对比。</p><p><strong>存储文件的压缩比测试：</strong></p><ol><li><p>测试数据<a href="./log.data">log.data</a></p><p>2．TextFile</p></li></ol><p>（1）创建表，存储数据格式为TEXTFILE</p><blockquote><p>create  table log_text ( </p><p> track_time  string,  url  string,  session_id  string,  referer  string,  ip  string,  end_user_id  string,  city_id  string  )  </p><p>row  format delimited fields terminated by ‘\t’  stored  as textfile ;  </p></blockquote><p>（2）向表中加载数据</p><blockquote><p>hive  (default)&gt; load data local inpath ‘/opt/module/datas/log.data’ into table  log_text ;  </p></blockquote><p>（3）查看表中数据大小</p><blockquote><p>hive  (default)&gt; dfs -du -h /user/hive/warehouse/log_text;  </p></blockquote><blockquote><p><strong>==18.1 M==</strong> /user/hive/warehouse/log_text/log.data</p></blockquote><p>3．ORC</p><p>​    （1）创建表，存储数据格式为ORC</p><blockquote><p>create  table log_orc(  track_time  string,  url  string,  session_id  string,  referer  string,  ip  string,  end_user_id  string,  city_id  string  ) </p><p> row  format delimited fields terminated by ‘\t’  stored  as orc ;  </p></blockquote><p>（2）向表中加载数据</p><blockquote><p>hive  (default)&gt; insert into table log_orc select * from log_text ;  </p></blockquote><p>（3）查看表中数据大小</p><blockquote><p>hive  (default)&gt; dfs -du -h /user/hive/warehouse/log_orc/ ;  </p></blockquote><blockquote><p>**==2.8 M== **/user/hive/warehouse/log_orc/000000_0</p></blockquote><p>4．Parquet</p><p>（1）创建表，存储数据格式为parquet</p><blockquote><p>create  table log_parquet(  track_time  string,  url  string,  session_id  string,  referer  string,  ip  string,  end_user_id  string,  city_id  string  )  row  format delimited fields terminated by ‘\t’  stored  as parquet ;    </p></blockquote><p>（2）向表中加载数据</p><blockquote><p>hive  (default)&gt; insert into table log_parquet select * from log_text ;  </p></blockquote><p>（3）查看表中数据大小</p><blockquote><p>hive  (default)&gt; dfs -du -h /user/hive/warehouse/log_parquet/ ;  </p></blockquote><blockquote><p>**==13.1 M== ** /user/hive/warehouse/log_parquet/000000_0</p></blockquote><p>存储文件的压缩比总结：</p><blockquote><p>ORC &gt; Parquet &gt; textFile</p></blockquote><p><strong>存储文件的查询速度测试：</strong></p><p>1．TextFile</p><blockquote><p>  hive (default)&gt; select count(*) from log_text;</p></blockquote><blockquote><p>  _c0</p></blockquote><blockquote><p>  100000</p></blockquote><blockquote><p>  Time taken: 21.54 seconds, Fetched: 1 row(s)</p></blockquote><blockquote><p>  Time taken: 21.08 seconds, Fetched: 1 row(s)</p></blockquote><blockquote><p>  Time taken: 19.298 seconds, Fetched: 1 row(s)</p></blockquote><p>2．ORC</p><blockquote><p>  hive (default)&gt; select count(*) from log_orc;</p></blockquote><blockquote><p>  _c0</p></blockquote><blockquote><p>  100000</p></blockquote><blockquote><p>  Time taken: 20.867 seconds, Fetched: 1 row(s)</p></blockquote><blockquote><p>  Time taken: 22.667 seconds, Fetched: 1 row(s)</p></blockquote><blockquote><p>  Time taken: 18.36 seconds, Fetched: 1 row(s)</p></blockquote><p>3．Parquet</p><blockquote><p>  hive (default)&gt; select count(*) from log_parquet;</p></blockquote><blockquote><p>  _c0</p></blockquote><blockquote><p>  100000</p></blockquote><blockquote><p>  Time taken: 22.922 seconds, Fetched: 1 row(s)</p></blockquote><blockquote><p>  Time taken: 21.074 seconds, Fetched: 1 row(s)</p></blockquote><blockquote><p>  Time taken: 18.384 seconds, Fetched: 1 row(s)</p></blockquote><blockquote><p>  存储文件的查询速度总结：**==查询速度相近==。**</p></blockquote><h2 id="8-6-存储和压缩结合"><a href="#8-6-存储和压缩结合" class="headerlink" title="8.6 存储和压缩结合"></a>8.6 存储和压缩结合</h2><h3 id="8-6-1-修改Hadoop集群具有Snappy压缩方式"><a href="#8-6-1-修改Hadoop集群具有Snappy压缩方式" class="headerlink" title="8.6.1 修改Hadoop集群具有Snappy压缩方式"></a>8.6.1 修改Hadoop集群具有Snappy压缩方式</h3><p>1．查看hadoop checknative命令使用</p><blockquote><p>  [xing@hadoop104 hadoop-2.7.2]$ hadoop checknative [-a|-h] check native hadoop and compression libraries availability</p></blockquote><p>2．查看hadoop支持的压缩方式</p><blockquote><p>  [xing@hadoop104 hadoop-2.7.2]$ hadoop checknative</p><p>  17/12/24 20:32:52 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version</p><p>  17/12/24 20:32:52 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</p><p>  Native library checking:</p><p>  hadoop: true /opt/module/hadoop-2.7.2/lib/native/libhadoop.so</p><p>  zlib:  true /lib64/libz.so.1</p><p>  snappy: false </p><p>  lz4:   true revision:99</p><p>  bzip2:  false</p></blockquote><p>3．将编译好的支持Snappy压缩的hadoop-2.7.2.tar.gz包导入到hadoop102的/opt/software中</p><p>4．解压hadoop-2.7.2.tar.gz到当前路径</p><blockquote><p>  [xing@hadoop102 software]$ tar -zxvf hadoop-2.7.2.tar.gz</p></blockquote><p>5．进入到/opt/software/hadoop-2.7.2/lib/native路径可以看到支持Snappy压缩的动态链接库</p><blockquote><p>  [xing@hadoop102 native]$ pwd</p></blockquote><blockquote><p>  /opt/software/hadoop-2.7.2/lib/native</p></blockquote><blockquote><p>  [xing@hadoop102 native]$ ll</p></blockquote><blockquote><p>  -rw-r–r–. 1 xing xing 472950 9月 1 10:19 libsnappy.a</p></blockquote><blockquote><p>  -rwxr-xr-x. 1 xing xing 955 9月 1 10:19 libsnappy.la</p></blockquote><blockquote><p>  lrwxrwxrwx. 1 xing xing 18 12月 24 20:39 libsnappy.so -&gt;<br>  libsnappy.so.1.3.0</p></blockquote><blockquote><p>  lrwxrwxrwx. 1 xing xing 18 12月 24 20:39 libsnappy.so.1 -&gt;<br>  libsnappy.so.1.3.0</p></blockquote><blockquote><p>  -rwxr-xr-x. 1 xing xing 228177 9月 1 10:19 libsnappy.so.1.3.0</p></blockquote><p>6．拷贝/opt/software/hadoop-2.7.2/lib/native里面的所有内容到开发集群的/opt/module/hadoop-2.7.2/lib/native路径上</p><blockquote><p>  [xing@hadoop102 native]$ cp ../native/*<br>  /opt/module/hadoop-2.7.2/lib/native/</p></blockquote><p>7．分发集群</p><blockquote><p>  [xing@hadoop102 lib]$ xsync native/</p></blockquote><p>8．再次查看hadoop支持的压缩类型</p><blockquote><p>  [xing@hadoop102 hadoop-2.7.2]$ hadoop checknative</p><p>  17/12/24 20:45:02 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version</p><p>  17/12/24 20:45:02 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</p><p>  Native library checking:</p><p>  hadoop: true /opt/module/hadoop-2.7.2/lib/native/libhadoop.so</p><p>  zlib:  true /lib64/libz.so.1</p><p>  snappy: true /opt/module/hadoop-2.7.2/lib/native/libsnappy.so.1</p><p>  lz4:   true revision:99</p><p>  bzip2:  false</p></blockquote><p>9．重新启动hadoop集群和hive</p><h3 id="8-6-2-测试存储和压缩"><a href="#8-6-2-测试存储和压缩" class="headerlink" title="8.6.2 测试存储和压缩"></a>8.6.2 测试存储和压缩</h3><p>官网：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC</a></p><p>ORC存储方式的压缩：</p><table><thead><tr><th>Key</th><th>Default</th><th>Notes</th></tr></thead><tbody><tr><td>orc.compress</td><td>ZLIB</td><td>high level compression (one of NONE, ZLIB, SNAPPY)</td></tr><tr><td>orc.compress.size</td><td>262,144</td><td>number of bytes in each compression chunk</td></tr><tr><td>orc.stripe.size</td><td>67,108,864</td><td>number of bytes in each stripe</td></tr><tr><td>orc.row.index.stride</td><td>10,000</td><td>number of rows between index entries (must be &gt;= 1000)</td></tr><tr><td>orc.create.index</td><td>true</td><td>whether to create row indexes</td></tr><tr><td>orc.bloom.filter.columns</td><td>“”</td><td>comma separated list of column names for which bloom filter should be created</td></tr><tr><td>orc.bloom.filter.fpp</td><td>0.05</td><td>false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)</td></tr></tbody></table><p>1．创建一个非压缩的的ORC存储方式</p><p>（1）建表语句</p><blockquote><p>create table log_orc_none(</p><p>track_time string,</p><p>url string,</p><p>session_id string,</p><p>referer string,</p><p>ip string,</p><p>end_user_id string,</p><p>city_id string</p><p>)</p><p>row format delimited fields terminated by ‘\t’</p><p>stored as orc tblproperties (“orc.compress”=”NONE”);</p></blockquote><p>（2）插入数据</p><blockquote><p>hive (default)&gt; insert into table log_orc_none select * from log_text ; </p></blockquote><p>（3）查看插入后数据</p><blockquote><p>hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_none/ ; </p></blockquote><blockquote><p><strong>==7.7 M /user/hive/warehouse/log_orc_none/000000_0==</strong></p></blockquote><p>2．创建一个SNAPPY压缩的ORC存储方式</p><p>（1）建表语句</p><blockquote><p>create table log_orc_snappy( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string ) row format delimited fields terminated by ‘\t’ stored as orc tblproperties (“orc.compress”=”SNAPPY”); </p></blockquote><p>（2）插入数据</p><blockquote><p>hive (default)&gt; insert into table log_orc_snappy select * from log_text ; </p></blockquote><p>（3）查看插入后数据</p><blockquote><p>hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_snappy/ ; </p></blockquote><blockquote><p><strong>==3.8 M==</strong> /user/hive/warehouse/log_orc_snappy/000000_0</p></blockquote><p>3．上一节中默认创建的ORC存储方式，导入数据后的大小为</p><blockquote><p>**==2.8 M== **/user/hive/warehouse/log_orc/000000_0</p></blockquote><blockquote><p>比Snappy压缩的还小。**==原因是orc存储文件默认采用ZLIB压缩。比snappy压缩的小。==**</p></blockquote><p>4．存储方式和压缩总结</p><p><strong>==在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。==</strong></p><h1 id="第9章-企业级调优"><a href="#第9章-企业级调优" class="headerlink" title="第9章 企业级调优"></a>第9章 企业级调优</h1><h2 id="9-1-Fetch抓取"><a href="#9-1-Fetch抓取" class="headerlink" title="9.1 Fetch抓取"></a>9.1 Fetch抓取</h2><p>Fetch抓取是指，**==Hive中对某些情况的查询可以不必使用MapReduce计算==**。例如：SELECT * FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。</p><p>在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，**==老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。==**</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.fetch.task.conversion<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>more<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      Expects one of [none, minimal, more].</span><br><span class="line">      Some select queries can be converted to single FETCH task minimizing latency.</span><br><span class="line">      Currently the query should be single sourced not having any subquery and should not have</span><br><span class="line">      any aggregations or distincts (which incurs RS), lateral views and joins.</span><br><span class="line">      0. none : disable hive.fetch.task.conversion</span><br><span class="line">      1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only</span><br><span class="line">      2. more  : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>案例</p><p>1）把hive.fetch.task.conversion设置成none，然后执行查询语句，都会执行mapreduce程序。</p><blockquote><p>  hive (default)&gt; set hive.fetch.task.conversion=none;</p></blockquote><blockquote><p>  hive (default)&gt; select * from emp;</p></blockquote><blockquote><p>  hive (default)&gt; select ename from emp;</p></blockquote><blockquote><p>  hive (default)&gt; select ename from emp limit 3;</p></blockquote><p>2）把hive.fetch.task.conversion设置成more，然后执行查询语句，如下查询方式都不会执行mapreduce程序。</p><blockquote><p>  hive (default)&gt; set hive.fetch.task.conversion=more;</p></blockquote><blockquote><p>  hive (default)&gt; select * from emp;</p></blockquote><blockquote><p>  hive (default)&gt; select ename from emp;</p></blockquote><blockquote><p>  hive (default)&gt; select ename from emp limit 3;</p></blockquote><h2 id="9-2-本地模式"><a href="#9-2-本地模式" class="headerlink" title="9.2 本地模式"></a>9.2 本地模式</h2><p>大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于大多数这种情况，**==Hive可以通过本地模式在单台机器上处理所有的任务==。==对于小数据集，执行时间可以明显被缩短。==**</p><p>用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。</p><blockquote><p>set hive.exec.mode.local.auto=true; //开启本地mr</p><p>设置local mr的最大输入数据量，当输入数据量小于这个值时采用local mr的方式，默认为134217728，即128M</p><p>set hive.exec.mode.local.auto.inputbytes.max=50000000;</p><p>设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4</p><p>set hive.exec.mode.local.auto.input.files.max=10;</p></blockquote><p>案例实操：</p><p>1）开启本地模式，并执行查询语句</p><blockquote><p>  hive (default)&gt; set hive.exec.mode.local.auto=true;</p></blockquote><blockquote><p>  hive (default)&gt; select * from emp cluster by deptno;</p></blockquote><blockquote><p>  Time taken: 1.328 seconds, Fetched: 14 row(s)</p></blockquote><p>2）关闭本地模式，并执行查询语句</p><blockquote><p>  hive (default)&gt; set hive.exec.mode.local.auto=false;</p></blockquote><blockquote><p>  hive (default)&gt; select * from emp cluster by deptno;</p></blockquote><blockquote><p>  Time taken: 20.09 seconds, Fetched: 14 row(s)</p></blockquote><h2 id="9-3-表的优化"><a href="#9-3-表的优化" class="headerlink" title="9.3 表的优化"></a>9.3 表的优化</h2><h3 id="9-3-1-小表、大表Join"><a href="#9-3-1-小表、大表Join" class="headerlink" title="9.3.1 小表、大表Join"></a>9.3.1 小表、大表Join</h3><p>将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。</p><p><strong>==实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。==</strong></p><p><strong>案例实操</strong></p><ol><li>需求</li></ol><p>测试大表JOIN小表和小表JOIN大表的效率</p><p>2．建大表、小表和JOIN后表的语句</p><blockquote><p>// 创建大表</p><p>create table bigtable(</p><p>id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) </p><p>row format delimited fields terminated by ‘\t’;</p><p>// 创建小表</p><p>create table smalltable(</p><p>id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) </p><p>row format delimited fields terminated by ‘\t’;</p><p>// 创建join后表的语句</p><p>create table jointable(</p><p>id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) </p><p>row format delimited fields terminated by ‘\t’;</p></blockquote><p>3．分别向大表和小表中导入数据</p><blockquote><p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/bigtable’ intotable bigtable;</p></blockquote><blockquote><p>  hive (default)&gt;load data local inpath ‘/opt/module/datas/smalltable’ into table smalltable;</p></blockquote><p>4．关闭mapjoin功能（默认是打开的）</p><blockquote><p>  set hive.auto.convert.join = false;</p></blockquote><p>5．执行小表JOIN大表语句</p><blockquote><p>  insert overwrite table jointable</p><p>  select b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</p><p>  from smalltable s</p><p>  left join bigtable b</p><p>  on b.id = s.id;</p></blockquote><p><strong>==Time taken: 35.921 seconds==</strong></p><p><strong>==No rows affected (44.456 seconds)==</strong></p><p>6．执行大表JOIN小表语句</p><blockquote><p>  insert overwrite table jointable</p><p>  select b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</p><p>  from bigtable b</p><p>  left join smalltable s</p><p>  on s.id = b.id;</p></blockquote><p>Time taken: 34.196 seconds</p><p><strong>==No rows affected (26.287 seconds)==</strong></p><h3 id="9-3-2-大表Join大表"><a href="#9-3-2-大表Join大表" class="headerlink" title="9.3.2 大表Join大表"></a>9.3.2 大表Join大表</h3><p>1．空KEY过滤</p><p>有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空，操作如下：</p><p>案例实操</p><p>（1）配置历史服务器,配置mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>启动历史服务器</p><blockquote><p>  sbin/mr-jobhistory-daemon.sh start historyserver</p></blockquote><p>查看jobhistory</p><p><a href="http://hadoop102:19888/jobhistory">http://hadoop102:19888/jobhistory</a></p><p>（2）创建原始数据表、空id表、合并后数据表</p><blockquote><p>// 创建原始表</p><p>create table ori(</p><p>id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) </p><p>row format delimited fields terminated by ‘\t’;</p><p>// 创建空id表</p><p>create table nullidtable(</p><p>id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) </p><p>row format delimited fields terminated by ‘\t’;</p><p>// 创建join后表的语句</p><p>create table jointable(</p><p>id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) </p><p>row format delimited fields terminated by ‘\t’;</p></blockquote><p>（3）分别加载原始数据和空id数据到对应表中</p><blockquote><p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/ori’ into table ori;</p></blockquote><blockquote><p>  hive (default)&gt; load data local inpath ‘/opt/module/datas/nullid’ into table nullidtable;</p></blockquote><p>（4）测试不过滤空id</p><blockquote><p>  hive (default)&gt; insert overwrite table jointable</p></blockquote><blockquote><p>  select n.* from nullidtable n left join ori o on n.id = o.id;</p></blockquote><p><strong>==Time taken: 42.038 seconds==</strong></p><p><strong>==Time taken: 37.284 seconds==</strong></p><p>（5）测试过滤空id</p><blockquote><p>  hive (default)&gt; insert overwrite table jointable</p></blockquote><blockquote><p>  select n.* from (select * from nullidtable where id is not null ) n left join ori o on n.id = o.id;</p></blockquote><p><strong>==Time taken: 31.725 seconds==</strong></p><p><strong>==Time taken: 28.876 seconds==</strong></p><p>2．空key转换</p><p>有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。例如：</p><p><strong>案例实操：</strong></p><p><strong>不随机分布空null值：</strong></p><p>（1）设置5个reduce个数</p><blockquote><p>  <strong>==set mapreduce.job.reduces = 5;==</strong></p></blockquote><p>（2）JOIN两张表</p><blockquote><p>  insert overwrite table jointable select n.* from nullidtable n left join ori b on n.id = b.id;</p></blockquote><p><strong>结果：如图所示，可以看出来，出现了数据倾斜，某些reducer的资源消耗远大于其他reducer。</strong></p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113934.png" alt="image-20200918103254514"></p><p><strong>随机分布空null值</strong></p><p>（1）设置5个reduce个数</p><blockquote><p><strong>==set mapreduce.job.reduces = 5;==</strong></p></blockquote><p>（2）JOIN两张表</p><blockquote><p>  insert overwrite table jointable</p><p>  select n.* from nullidtable n full join ori o on</p><p>  case when n.id is null then concat(‘hive’, rand()) else n.id end = o.id;</p></blockquote><p><strong>结果：如图所示，可以看出来，消除了数据倾斜，负载均衡reducer的资源消耗</strong></p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113935.png" alt="image-20200918103502930"></p><h3 id="9-3-3-MapJoin"><a href="#9-3-3-MapJoin" class="headerlink" title="9.3.3 MapJoin"></a>9.3.3 MapJoin</h3><p>如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。</p><p>1．开启MapJoin参数设置</p><p>（1）设置自动选择Mapjoin</p><blockquote><p>  set hive.auto.convert.join = true; 默认为true</p></blockquote><p>（2）大表小表的阈值设置（默认25M以下认为是小表）：</p><blockquote><p>  set hive.mapjoin.smalltable.filesize=25000000;</p></blockquote><p>2．MapJoin工作机制</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200918113936.png" alt="image-20200918104519263"></p><p><strong>案例实操：</strong></p><p>（1）开启Mapjoin功能</p><blockquote><p>  set hive.auto.convert.join = true; 默认为true</p></blockquote><p>（2）执行小表JOIN大表语句</p><blockquote><p>insert overwrite table jointable</p><p>select b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</p><p>from smalltable s</p><p>join bigtable b</p><p>on s.id = b.id;</p></blockquote><p>Time taken: 24.594 seconds</p><p>（3）执行大表JOIN小表语句</p><blockquote><p>insert overwrite table jointable</p><p>select b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</p><p>from bigtable b</p><p>join smalltable s</p><p>on s.id = b.id;</p></blockquote><p>Time taken: 24.315 seconds</p><h3 id="9-3-4-Group-By"><a href="#9-3-4-Group-By" class="headerlink" title="9.3.4 Group By"></a>9.3.4 Group By</h3><p>默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。</p><p>并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。</p><p>1．开启Map端聚合参数设置</p><p>（1）是否在Map端进行聚合，默认为True</p><blockquote><p>  hive.map.aggr = true</p></blockquote><p>（2）在Map端进行聚合操作的条目数目</p><blockquote><p>  hive.groupby.mapaggr.checkinterval = 100000</p></blockquote><p>（3）有数据倾斜的时候进行负载均衡（默认是false）</p><blockquote><p>  hive.groupby.skewindata = true</p></blockquote><p><strong>==当选项设定为 true，生成的查询计划会有两个MR Job==**。第一个MRJob中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的</strong>==Group By Key有可能被分发到不同的Reduce中==**，从而达到负载均衡的目的；第二个MRJob再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</p><h3 id="9-3-5-Count-Distinct-去重统计"><a href="#9-3-5-Count-Distinct-去重统计" class="headerlink" title="9.3.5 Count(Distinct) 去重统计"></a>9.3.5 Count(Distinct) 去重统计</h3><p>数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个ReduceTask来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换：</p><p><strong>案例实操</strong></p><p>1． 创建一张大表</p><blockquote><p>hive (default)&gt; create table bigtable(</p><p>id bigint, time bigint, uid string, keyword</p><p>string, url_rank int, click_num int, click_url string)</p><p>row format delimitedfields terminated by ‘\t’;</p></blockquote><p>2．加载数据</p><blockquote><p>hive (default)&gt; load data local inpath ‘/opt/module/datas/bigtable’ into table bigtable;</p></blockquote><p>3．设置5个reduce个数</p><blockquote><p>set mapreduce.job.reduces = 5;</p></blockquote><p>4．执行去重id查询</p><blockquote><p>hive (default)&gt; select count(distinct id) from bigtable;</p><p>Stage-Stage-1: Map: 1 Reduce: 1  Cumulative CPU: 7.12 sec  HDFS Read: 120741990 HDFS Write: 7 SUCCESS</p><p>Total MapReduce CPU Time Spent: 7 seconds 120 msec</p><p>OK</p><p>c0</p><p>100001</p><p>Time taken: 23.607 seconds, Fetched: 1 row(s)</p></blockquote><p>5．采用GROUP by去重id</p><blockquote><p>hive (default)&gt; select count(id) from (select id from bigtable group by id) a;</p><p>Stage-Stage-1: Map: 1 Reduce: 5  Cumulative CPU: 17.53 sec  HDFS Read: 120752703 HDFS Write: 580 SUCCESS</p><p>Stage-Stage-2: Map: 1 Reduce: 1  Cumulative CPU: 4.29 sec  HDFS Read: 9409 HDFS Write: 7 SUCCESS</p><p>Total MapReduce CPU Time Spent: 21 seconds 820 msec</p><p>OK</p><p>_c0</p><p>100001</p><p>Time taken: 50.795 seconds, Fetched: 1 row(s)</p></blockquote><p><strong>==虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。==</strong></p><h3 id="9-3-6-笛卡尔积"><a href="#9-3-6-笛卡尔积" class="headerlink" title="9.3.6 笛卡尔积"></a>9.3.6 笛卡尔积</h3><p>尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。</p><h3 id="9-3-7-行列过滤"><a href="#9-3-7-行列过滤" class="headerlink" title="9.3.7 行列过滤"></a>9.3.7 行列过滤</h3><p>列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。</p><p>行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤，比如：</p><p><strong>案例实操：</strong></p><p>1．测试先关联两张表，再用where条件过滤</p><blockquote><p>  hive (default)&gt; select o.id from bigtable b</p><p>  join ori o on o.id = b.id</p><p>  where o.id &lt;= 10;</p></blockquote><p>Time taken: **==34.406==**seconds, Fetched: 100 row(s)</p><p>2．通过子查询后，再关联表</p><blockquote><p>  hive (default)&gt; select b.id from bigtable b</p><p>  join (select id from ori where id &lt;= 10 ) o </p><p>  on b.id = o.id;</p></blockquote><p>Time taken: <strong>==30.058==</strong> seconds, Fetched: 100 row(s)</p><h3 id="9-3-8-动态分区调整"><a href="#9-3-8-动态分区调整" class="headerlink" title="9.3.8 动态分区调整"></a>9.3.8 动态分区调整</h3><p>关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置。</p><p>1．开启动态分区参数设置</p><p>（1）开启动态分区功能（默认true，开启）</p><blockquote><p>  <strong>==hive.exec.dynamic.partition=true==</strong></p></blockquote><p>（2）设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。）</p><blockquote><p>  hive.exec.dynamic.partition.mode=nonstrict</p></blockquote><p>（3）在所有执行MR的节点上，最大一共可以创建多少个动态分区。</p><blockquote><p>  hive.exec.max.dynamic.partitions=1000</p></blockquote><p>（4）**==在每个执行MR的节点上，最大可以创建多少个动态分区==**。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。</p><blockquote><p>  hive.exec.max.dynamic.partitions.pernode=100</p></blockquote><p>（5）整个MR Job中，最大可以创建多少个HDFS文件。</p><blockquote><p>  hive.exec.max.created.files=100000</p></blockquote><p>（6）当有空分区生成时，是否抛出异常。一般不需要设置。</p><blockquote><p>  hive.error.on.empty.partition=false</p></blockquote><p>2．案例实操</p><p>需求：将ori中的数据按照时间(如：20111230000008)，插入到目标表ori_partitioned_target的相应分区中。</p><p>（1）创建分区表</p><blockquote><p>create table ori_partitioned(</p><p>id bigint, time bigint, uid string, keyword string,  url_rank int, click_num int, click_url string)  </p><p>partitioned by (p_time bigint)  </p><p>row format delimited fields terminated by ‘\t’; |</p></blockquote><p>（2）加载数据到分区表中</p><blockquote><p>hive (default)&gt; load data local inpath ‘/home/xing/ds1’ into table  ori_partitioned </p><p>partition(p_time=’20111230000010’) ; </p><p>hive (default)&gt; load data local inpath ‘/home/xing/ds2’ into table ori_partitioned partition(p_time=’20111230000011’) ; </p></blockquote><p>（3）创建目标分区表</p><blockquote><p>create table ori_partitioned_target(</p><p>id bigint, time bigint, uid string,  keyword string, url_rank int, click_num int, click_url string) PARTITIONED BY (p_time STRING)</p><p>row format delimited fields terminated by ‘\t’; |</p></blockquote><p>（4）设置动态分区</p><blockquote><p> set hive.exec.dynamic.partition = true; </p><p>set hive.exec.dynamic.partition.mode = nonstrict; </p><p>set hive.exec.max.dynamic.partitions = 1000; </p><p>set hive.exec.max.dynamic.partitions.pernode = 100; </p><p>set hive.exec.max.created.files = 100000; </p><p>set hive.error.on.empty.partition = false; </p><p>hive (default)&gt; insert overwrite table ori_partitioned_target partition (p_time)  </p><p>select id, time, uid, keyword, url_rank, click_num, click_url, p_time from ori_partitioned;</p></blockquote><p>（5）查看目标分区表的分区情况</p><blockquote><p>  hive (default)&gt; show partitions ori_partitioned_target;</p></blockquote><h3 id="9-3-9-分桶"><a href="#9-3-9-分桶" class="headerlink" title="9.3.9 分桶"></a>9.3.9 分桶</h3><p><a href="#6.6">详见6.6章</a>。</p><h3 id="9-3-10-分区"><a href="#9-3-10-分区" class="headerlink" title="9.3.10 分区"></a>9.3.10 分区</h3><p><a href="#4.6">详见4.6章</a>。</p><h2 id="9-4-数据倾斜"><a href="#9-4-数据倾斜" class="headerlink" title="9.4 数据倾斜"></a>9.4 数据倾斜</h2><h3 id="9-4-1-合理设置Map数"><a href="#9-4-1-合理设置Map数" class="headerlink" title="9.4.1 合理设置Map数"></a>9.4.1 合理设置Map数</h3><p><strong>1）通常情况下，作业会通过input的目录产生一个或者多个map任务。</strong></p><p>主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。</p><p><strong>2）是不是map数越多越好？</strong></p><p>答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。</p><p><strong>3）是不是保证每个map处理接近128m的文件块，就高枕无忧了？</strong></p><p>答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。</p><p>针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数；</p><h3 id="9-4-2-小文件进行合并"><a href="#9-4-2-小文件进行合并" class="headerlink" title="9.4.2 小文件进行合并"></a>9.4.2 小文件进行合并</h3><p>在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。</p><blockquote><p>set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</p></blockquote><h3 id="9-4-3-复杂文件增加Map数"><a href="#9-4-3-复杂文件增加Map数" class="headerlink" title="9.4.3 复杂文件增加Map数"></a>9.4.3 复杂文件增加Map数</h3><p>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。</p><p>增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。</p><p><strong>案例实操：</strong></p><p>1．执行查询</p><blockquote><p>  hive (default)&gt; select count(*) from emp;</p></blockquote><blockquote><p>  Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</p></blockquote><p>2．设置最大切片值为100个字节</p><blockquote><p>  hive (default)&gt; set mapreduce.input.fileinputformat.split.maxsize=100;</p></blockquote><blockquote><p>  hive (default)&gt; select count(*) from emp;</p></blockquote><blockquote><p>  Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1</p></blockquote><h3 id="9-4-4-合理设置Reduce数"><a href="#9-4-4-合理设置Reduce数" class="headerlink" title="9.4.4 合理设置Reduce数"></a>9.4.4 合理设置Reduce数</h3><p>1．调整reduce个数方法一</p><p>（1）每个Reduce处理的数据量默认是256MB</p><blockquote><p>  hive.exec.reducers.bytes.per.reducer=256000000</p></blockquote><p>（2）每个任务最大的reduce数，默认为1009</p><blockquote><p>  hive.exec.reducers.max=1009</p></blockquote><p>（3）计算reducer数的公式</p><blockquote><p>  N=min(参数2，总输入数据量/参数1)</p></blockquote><p>2．调整reduce个数方法二</p><p>在hadoop的mapred-default.xml文件中修改</p><p>设置每个job的Reduce个数</p><blockquote><p>  set mapreduce.job.reduces = 15;</p></blockquote><p>3．reduce个数并不是越多越好</p><p>1）过多的启动和初始化reduce也会消耗时间和资源；</p><p>2）另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</p><p>在设置reduce个数的时候也需要考虑这两个原则：**==处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适；==**</p><h2 id="9-5-并行执行"><a href="#9-5-并行执行" class="headerlink" title="9.5 并行执行"></a>9.5 并行执行</h2><p>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。</p><p>通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。</p><blockquote><p>set hive.exec.parallel=true; //打开任务并行执行</p></blockquote><blockquote><p>set hive.exec.parallel.thread.number=16; //同一个sql允许最大并行度，默认为8。</p></blockquote><p><strong>==当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。==</strong></p><h2 id="9-6-严格模式"><a href="#9-6-严格模式" class="headerlink" title="9.6 严格模式"></a>9.6 严格模式</h2><p>Hive提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询。</p><p>通过设置属性hive.mapred.mode值为默认是非严格模式**==nonstrict==**。开启严格模式需要修改hive.mapred.mode值为strict，开启严格模式可以禁止3种类型的查询。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.mapred.mode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>strict<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      The mode in which the Hive operations are being performed. </span><br><span class="line">      In strict mode, some risky queries are not allowed to run. They include:</span><br><span class="line">        Cartesian Product.</span><br><span class="line">        No partition being picked up for a query.</span><br><span class="line">        Comparing bigints and strings.</span><br><span class="line">        Comparing bigints and doubles.</span><br><span class="line">        Orderby without limit.</span><br><span class="line"><span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><ol><li><p>对于分区表，**==除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行==**。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。</p></li><li><p>对于**==使用了order by语句的查询，要求必须使用limit语句==**。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。</p></li><li><p>**==限制笛卡尔积的查询==**。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。</p></li></ol><h2 id="9-7-JVM重用"><a href="#9-7-JVM重用" class="headerlink" title="9.7 JVM重用"></a>9.7 JVM重用</h2><p>JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是**==对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。==**</p><p>Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。**==JVM重用可以使得JVM实例在同一个job中重新使用N次==**。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.jvm.numtasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      How many tasks to run per jvm. If set to -1, there is no limit. </span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。</p><h2 id="9-8-推测执行"><a href="#9-8-推测执行" class="headerlink" title="9.8 推测执行"></a>9.8 推测执行</h2><p>在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。</p><p>设置开启推测执行参数：Hadoop的mapred-site.xml文件中进行配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      If true, then multiple instances of some map tasks  may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      If true, then multiple instances of some reduce tasks  may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>不过hive本身也提供了配置项来控制reduce-side的推测执行：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.mapred.reduce.tasks.speculative.execution<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">        Whether speculative execution for reducers should be turned on. <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>关于调优这些推测执行变量，还很难给一个具体的建议。**==如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉==**。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大大。</p><h2 id="9-9-压缩"><a href="#9-9-压缩" class="headerlink" title="9.9 压缩"></a>9.9 压缩</h2><p><a href="#8">详见第8章。</a></p><h2 id="9-10-执行计划（Explain）"><a href="#9-10-执行计划（Explain）" class="headerlink" title="9.10 执行计划（Explain）"></a>9.10 执行计划（Explain）</h2><p>1．基本语法</p><p>EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query</p><p>2．案例实操</p><p>（1）查看下面这条语句的执行计划</p><blockquote><p>  hive (default)&gt; explain select * from emp;</p></blockquote><blockquote><p>  hive (default)&gt; explain select deptno, avg(sal) avg_sal from emp group by<br>  deptno;</p></blockquote><p>（2）查看详细执行计划</p><blockquote><p>  hive (default)&gt; explain extended select * from emp;</p></blockquote><blockquote><p>  hive (default)&gt; explain extended select deptno, avg(sal) avg_sal from emp group by deptno;</p></blockquote><h1 id="第10章-Hive实战之谷粒影音"><a href="#第10章-Hive实战之谷粒影音" class="headerlink" title="第10章 Hive实战之谷粒影音"></a>第10章 Hive实战之谷粒影音</h1><h2 id="10-1-需求描述"><a href="#10-1-需求描述" class="headerlink" title="10.1 需求描述"></a>10.1 需求描述</h2><p>统计硅谷影音视频网站的常规指标，各种TopN指标：</p><p>--统计视频观看数Top10</p><p>--统计视频类别热度Top10</p><p>--统计视频观看数Top20所属类别</p><p>--统计视频观看数Top50所关联视频的所属类别Rank</p><p>--统计每个类别中的视频热度Top10</p><p>--统计每个类别中视频流量Top10</p><p>--统计上传视频最多的用户Top10以及他们上传的视频</p><p>--统计每个类别视频观看数Top10</p><h2 id="10-2-项目"><a href="#10-2-项目" class="headerlink" title="10.2 项目"></a>10.2 项目</h2><h3 id="10-2-1-数据结构"><a href="#10-2-1-数据结构" class="headerlink" title="10.2.1 数据结构"></a>10.2.1 数据结构</h3><p>1．视频表</p><table><thead><tr><th>字段</th><th>备注</th><th>详细描述</th></tr></thead><tbody><tr><td>video id</td><td>视频唯一id</td><td>11位字符串</td></tr><tr><td>uploader</td><td>视频上传者</td><td>上传视频的用户名String</td></tr><tr><td>age</td><td>视频年龄</td><td>视频在平台上的整数天</td></tr><tr><td>category</td><td>视频类别</td><td>上传视频指定的视频分类</td></tr><tr><td>length</td><td>视频长度</td><td>整形数字标识的视频长度</td></tr><tr><td>views</td><td>观看次数</td><td>视频被浏览的次数</td></tr><tr><td>rate</td><td>视频评分</td><td>满分5分</td></tr><tr><td>ratings</td><td>流量</td><td>视频的流量，整型数字</td></tr><tr><td>conments</td><td>评论数</td><td>一个视频的整数评论数</td></tr><tr><td>related ids</td><td>相关视频id</td><td>相关视频的id，最多20个</td></tr></tbody></table><p>2．用户表</p><table><thead><tr><th>字段</th><th>备注</th><th>字段类型</th></tr></thead><tbody><tr><td>uploader</td><td>上传者用户名</td><td>string</td></tr><tr><td>videos</td><td>上传视频数</td><td>int</td></tr><tr><td>friends</td><td>朋友数量</td><td>int</td></tr></tbody></table><h3 id="10-2-2-ETL原始数据"><a href="#10-2-2-ETL原始数据" class="headerlink" title="10.2.2 ETL原始数据"></a>10.2.2 ETL原始数据</h3><p>通过观察原始数据形式，可以发现，视频可以有多个所属分类，每个所属分类用&amp;符号分割，且分割的两边有空格字符，同时相关视频也是可以有多个元素，多个相关视频又用“\t”进行分割。为了分析数据时方便对存在多个子元素的数据进行操作，我们首先进行数据重组清洗操作。即：将所有的类别用“&amp;”分割，同时去掉两边空格，多个相关视频id也使用“&amp;”进行分割。</p><p>1．ETL之ETLUtil</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ETLUtil</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">oriString2ETLString</span><span class="params">(String ori)</span></span>&#123;</span><br><span class="line">StringBuilder etlString = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">String[] splits = ori.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"><span class="keyword">if</span>(splits.length &lt; <span class="number">9</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">splits[<span class="number">3</span>] = splits[<span class="number">3</span>].replace(<span class="string">&quot; &quot;</span>, <span class="string">&quot;&quot;</span>);</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; splits.length; i++)&#123;</span><br><span class="line"><span class="keyword">if</span>(i &lt; <span class="number">9</span>)&#123;</span><br><span class="line"><span class="keyword">if</span>(i == splits.length - <span class="number">1</span>)&#123;</span><br><span class="line">etlString.append(splits[i]);</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">etlString.append(splits[i] + <span class="string">&quot;\t&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line"><span class="keyword">if</span>(i == splits.length - <span class="number">1</span>)&#123;</span><br><span class="line">etlString.append(splits[i]);</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">etlString.append(splits[i] + <span class="string">&quot;&amp;&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> etlString.toString();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>2．ETL之Mapper</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.xing.util.ETLUtil;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">VideoETLMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">Text text = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">String etlString = ETLUtil.oriString2ETLString(value.toString());</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(StringUtils.isBlank(etlString)) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">text.set(etlString);</span><br><span class="line">context.write(NullWritable.get(), text);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>3．ETL之Runner</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">VideoETLRunner</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> Configuration conf = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setConf</span><span class="params">(Configuration conf)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.conf = conf;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Configuration <span class="title">getConf</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>.conf;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">conf = <span class="keyword">this</span>.getConf();</span><br><span class="line">conf.set(<span class="string">&quot;inpath&quot;</span>, args[<span class="number">0</span>]);</span><br><span class="line">conf.set(<span class="string">&quot;outpath&quot;</span>, args[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">job.setJarByClass(VideoETLRunner.class);</span><br><span class="line"></span><br><span class="line">job.setMapperClass(VideoETLMapper.class);</span><br><span class="line">job.setMapOutputKeyClass(NullWritable.class);</span><br><span class="line">job.setMapOutputValueClass(Text.class);</span><br><span class="line">job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">this</span>.initJobInputPath(job);</span><br><span class="line"><span class="keyword">this</span>.initJobOutputPath(job);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initJobOutputPath</span><span class="params">(Job job)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">Configuration conf = job.getConfiguration();</span><br><span class="line">String outPathString = conf.get(<span class="string">&quot;outpath&quot;</span>);</span><br><span class="line"></span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">Path outPath = <span class="keyword">new</span> Path(outPathString);</span><br><span class="line"><span class="keyword">if</span>(fs.exists(outPath))&#123;</span><br><span class="line">fs.delete(outPath, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">FileOutputFormat.setOutputPath(job, outPath);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initJobInputPath</span><span class="params">(Job job)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">Configuration conf = job.getConfiguration();</span><br><span class="line">String inPathString = conf.get(<span class="string">&quot;inpath&quot;</span>);</span><br><span class="line"></span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">Path inPath = <span class="keyword">new</span> Path(inPathString);</span><br><span class="line"><span class="keyword">if</span>(fs.exists(inPath))&#123;</span><br><span class="line">FileInputFormat.addInputPath(job, inPath);</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;HDFS中该文件目录不存在：&quot;</span> + inPathString);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line"><span class="keyword">int</span> resultCode = ToolRunner.run(<span class="keyword">new</span> VideoETLRunner(), args);</span><br><span class="line"><span class="keyword">if</span>(resultCode == <span class="number">0</span>)&#123;</span><br><span class="line">System.out.println(<span class="string">&quot;Success!&quot;</span>);</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">System.out.println(<span class="string">&quot;Fail!&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">System.exit(resultCode);</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">System.exit(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>4．执行ETL</p><blockquote><p>$ bin/yarn jar ~/softwares/jars/gulivideo-0.0.1-SNAPSHOT.jar \ </p><p>com.xing.etl.ETLVideosRunner  \  </p><p>/gulivideo/video/2008/0222  \  </p><p>/gulivideo/output/video/2008/0222  </p></blockquote><h2 id="10-3-准备工作"><a href="#10-3-准备工作" class="headerlink" title="10.3 准备工作"></a>10.3 准备工作</h2><h3 id="10-3-1-创建表"><a href="#10-3-1-创建表" class="headerlink" title="10.3.1 创建表"></a>10.3.1 创建表</h3><p>创建表：gulivideo_ori，gulivideo_user_ori，</p><p>创建表：gulivideo_orc，gulivideo_user_orc</p><p>gulivideo_ori：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_ori(</span><br><span class="line">    videoId <span class="keyword">string</span>, </span><br><span class="line">    uploader <span class="keyword">string</span>, </span><br><span class="line">    age <span class="built_in">int</span>, </span><br><span class="line">    <span class="keyword">category</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;, </span><br><span class="line">    <span class="keyword">length</span> <span class="built_in">int</span>, </span><br><span class="line">    views <span class="built_in">int</span>, </span><br><span class="line">    rate <span class="built_in">float</span>, </span><br><span class="line">    ratings <span class="built_in">int</span>, </span><br><span class="line">    comments <span class="built_in">int</span>,</span><br><span class="line">    relatedId <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;&amp;&quot;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure><p>gulivideo_user_ori：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_user_ori(</span><br><span class="line">    uploader <span class="keyword">string</span>,</span><br><span class="line">    videos <span class="built_in">int</span>,</span><br><span class="line">    friends <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span> </span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure><p>然后把原始数据插入到orc表中</p><p>gulivideo_orc：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_orc(</span><br><span class="line">    videoId <span class="keyword">string</span>, </span><br><span class="line">    uploader <span class="keyword">string</span>, </span><br><span class="line">    age <span class="built_in">int</span>, </span><br><span class="line">    <span class="keyword">category</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;, </span><br><span class="line">    <span class="keyword">length</span> <span class="built_in">int</span>, </span><br><span class="line">    views <span class="built_in">int</span>, </span><br><span class="line">    rate <span class="built_in">float</span>, </span><br><span class="line">    ratings <span class="built_in">int</span>, </span><br><span class="line">    comments <span class="built_in">int</span>,</span><br><span class="line">    relatedId <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;)</span><br><span class="line">clustered <span class="keyword">by</span> (uploader) <span class="keyword">into</span> <span class="number">8</span> buckets </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span> </span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;&amp;&quot;</span> </span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc;</span><br></pre></td></tr></table></figure><p>gulivideo_user_orc：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_user_orc(</span><br><span class="line">    uploader <span class="keyword">string</span>,</span><br><span class="line">    videos <span class="built_in">int</span>,</span><br><span class="line">    friends <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span> </span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc;</span><br></pre></td></tr></table></figure><h3 id="10-3-2-导入ETL后的数据"><a href="#10-3-2-导入ETL后的数据" class="headerlink" title="10.3.2 导入ETL后的数据"></a>10.3.2 导入ETL后的数据</h3><p>gulivideo_ori：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">&quot;/gulivideo/output/video/2008/0222&quot;</span> <span class="keyword">into</span> <span class="keyword">table</span> gulivideo_ori; </span><br></pre></td></tr></table></figure><p>gulivideo_user_ori：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">&quot;/gulivideo/user/2008/0903&quot;</span> <span class="keyword">into</span> <span class="keyword">table</span> gulivideo_user_ori; </span><br></pre></td></tr></table></figure><h3 id="10-3-3-向ORC表插入数据"><a href="#10-3-3-向ORC表插入数据" class="headerlink" title="10.3.3 向ORC表插入数据"></a>10.3.3 向ORC表插入数据</h3><p>gulivideo_orc：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> gulivideo_orc <span class="keyword">select</span> * <span class="keyword">from</span> gulivideo_ori; </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>gulivideo_user_orc：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> gulivideo_user_orc <span class="keyword">select</span> * <span class="keyword">from</span> gulivideo_user_ori; </span><br></pre></td></tr></table></figure><h2 id="10-4-业务分析"><a href="#10-4-业务分析" class="headerlink" title="10.4 业务分析"></a>10.4 业务分析</h2><h3 id="10-4-1-统计视频观看数Top10"><a href="#10-4-1-统计视频观看数Top10" class="headerlink" title="10.4.1 统计视频观看数Top10"></a>10.4.1 统计视频观看数Top10</h3><p>思路：使用order by按照views字段做一个全局排序即可，同时我们设置只显示前10条。</p><p>最终代码：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>   </span><br><span class="line">videoId,  uploader,   age,   <span class="keyword">category</span>,   <span class="keyword">length</span>,   views,   rate,   ratings,   comments <span class="keyword">from</span>   gulivideo_orc  </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span>   views  <span class="keyword">desc</span> <span class="keyword">limit</span>   <span class="number">10</span>; </span><br></pre></td></tr></table></figure><h3 id="10-4-2-统计视频类别热度Top10"><a href="#10-4-2-统计视频类别热度Top10" class="headerlink" title="10.4.2 统计视频类别热度Top10"></a>10.4.2 统计视频类别热度Top10</h3><p>思路：</p><ol><li><p>即统计每个类别有多少个视频，显示出包含视频最多的前10个类别。</p></li><li><p>我们需要按照类别group by聚合，然后count组内的videoId个数即可。</p></li><li><p>因为当前表结构为：一个视频对应一个或多个类别。所以如果要group<br>by类别，需要先将类别进行列转行(展开)，然后再进行count即可。</p></li><li><p>最后按照热度排序，显示前10条。</p></li></ol><p>最终代码：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>  </span><br><span class="line">category_name <span class="keyword">as</span> <span class="keyword">category</span>, <span class="keyword">count</span>(t1.videoId) <span class="keyword">as</span> hot  </span><br><span class="line"><span class="keyword">from</span> (<span class="keyword">select</span> videoId, category_name </span><br><span class="line">      <span class="keyword">from</span> gulivideo_orc <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) t_catetory <span class="keyword">as</span> category_name) t1 <span class="keyword">group</span> <span class="keyword">by</span>   t1.category_name  </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span>   hot  <span class="keyword">desc</span> <span class="keyword">limit</span>   <span class="number">10</span>; </span><br></pre></td></tr></table></figure><h3 id="10-4-3-统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数"><a href="#10-4-3-统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数" class="headerlink" title="10.4.3 统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数"></a>10.4.3 统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数</h3><p>思路：</p><ol><li><p>先找到观看数最高的20个视频所属条目的所有信息，降序排列</p></li><li><p>把这20条信息中的category分裂出来(列转行)</p></li><li><p>最后查询视频分类名称和该分类下有多少个Top20的视频</p></li></ol><p>最终代码：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>   </span><br><span class="line">category_name <span class="keyword">as</span> <span class="keyword">category</span>, <span class="keyword">count</span>(t2.videoId) <span class="keyword">as</span> hot_with_views  </span><br><span class="line"><span class="keyword">from</span> (<span class="keyword">select</span> </span><br><span class="line">      videoId, category_name   </span><br><span class="line">      <span class="keyword">from</span> (<span class="keyword">select</span> *   <span class="keyword">from</span>   gulivideo_orc   <span class="keyword">order</span> <span class="keyword">by</span>   views   <span class="keyword">desc</span> <span class="keyword">limit</span>   <span class="number">20</span>) t1   <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) t_catetory <span class="keyword">as</span> category_name</span><br><span class="line">     ) t2  </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span>  category_name  </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span>  hot_with_views  <span class="keyword">desc</span>; </span><br></pre></td></tr></table></figure><h3 id="10-4-4-统计视频观看数Top50所关联视频的所属类别Rank"><a href="#10-4-4-统计视频观看数Top50所关联视频的所属类别Rank" class="headerlink" title="10.4.4 统计视频观看数Top50所关联视频的所属类别Rank"></a>10.4.4 统计视频观看数Top50所关联视频的所属类别Rank</h3><p>思路：</p><ol><li>查询出观看数最多的前50个视频的所有信息(当然包含了每个视频对应的关联视频)，记为临时表t1</li></ol><p>t1：观看数前50的视频</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>  *  <span class="keyword">from</span>   gulivideo_orc  <span class="keyword">order</span> <span class="keyword">by</span>   views  <span class="keyword">desc</span> <span class="keyword">limit</span>   <span class="number">50</span>; </span><br></pre></td></tr></table></figure><ol start="2"><li>将找到的50条视频信息的相关视频relatedId列转行，记为临时表t2</li></ol><p>t2：将相关视频的id进行列转行操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>   <span class="keyword">explode</span>(relatedId) <span class="keyword">as</span> videoId  <span class="keyword">from</span>   t1; </span><br></pre></td></tr></table></figure><ol start="3"><li>将相关视频的id和gulivideo_orc表进行inner join操作</li></ol><p>t5：得到两列数据，一列是category，一列是之前查询出来的相关视频id</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(</span><br><span class="line">    <span class="keyword">select</span>  </span><br><span class="line">    <span class="keyword">distinct</span>(t2.videoId), t3.category  </span><br><span class="line">    <span class="keyword">from</span>  t2 </span><br><span class="line">    <span class="keyword">inner</span> <span class="keyword">join</span>  gulivideo_orc t3 </span><br><span class="line">    <span class="keyword">on</span> t2.videoId = t3.videoId</span><br><span class="line">) t4 <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) t_catetory <span class="keyword">as</span> category_name;  </span><br></pre></td></tr></table></figure><ol start="4"><li>按照视频类别进行分组，统计每组视频个数，然后排行</li></ol><p>最终代码：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>   </span><br><span class="line">category_name <span class="keyword">as</span> <span class="keyword">category</span>, <span class="keyword">count</span>(t5.videoId) <span class="keyword">as</span> hot  </span><br><span class="line"><span class="keyword">from</span> ( <span class="keyword">select</span> videoId, category_name <span class="keyword">from</span> (  </span><br><span class="line">    <span class="keyword">select</span>  <span class="keyword">distinct</span>(t2.videoId), t3.category  <span class="keyword">from</span> (  </span><br><span class="line">            <span class="keyword">select</span>   <span class="keyword">explode</span>(relatedId) <span class="keyword">as</span> videoId  <span class="keyword">from</span> (  </span><br><span class="line">                <span class="keyword">select</span>  *  <span class="keyword">from</span>  gulivideo_orc  <span class="keyword">order</span> <span class="keyword">by</span>  views  <span class="keyword">desc</span> <span class="keyword">limit</span>   <span class="number">50</span></span><br><span class="line">            ) t1</span><br><span class="line">        ) t2  <span class="keyword">inner</span> <span class="keyword">join</span>  gulivideo_orc t3 <span class="keyword">on</span> t2.videoId = t3.videoId</span><br><span class="line"> ) t4 <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) t_catetory <span class="keyword">as</span> category_name) t5 </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span>   category_name  <span class="keyword">order</span> <span class="keyword">by</span>   hot  <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><h3 id="10-4-5-统计每个类别中的视频热度Top10，以Music为例"><a href="#10-4-5-统计每个类别中的视频热度Top10，以Music为例" class="headerlink" title="10.4.5 统计每个类别中的视频热度Top10，以Music为例"></a>10.4.5 统计每个类别中的视频热度Top10，以Music为例</h3><p>思路：</p><p>1)要想统计Music类别中的视频热度Top10，需要先找到Music类别，那么就需要将category展开，所以可以创建一张表用于存放categoryId展开的数据。</p><ol start="2"><li><p>向category展开的表中插入数据。</p></li><li><p>统计对应类别（Music）中的视频热度。</p></li></ol><p>最终代码：</p><p>创建表类别表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_category(  </span><br><span class="line">    videoId <span class="keyword">string</span>,  uploader <span class="keyword">string</span>,  age <span class="built_in">int</span>,  categoryId <span class="keyword">string</span>,  <span class="keyword">length</span> <span class="built_in">int</span>,   </span><br><span class="line">    views <span class="built_in">int</span>,   rate <span class="built_in">float</span>,   ratings <span class="built_in">int</span>,   comments <span class="built_in">int</span>,  relatedId <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span>  </span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;&amp;&quot;</span>  </span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc; </span><br></pre></td></tr></table></figure><p>向类别表中插入数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> gulivideo_category  </span><br><span class="line"><span class="keyword">select</span>   </span><br><span class="line">videoId, uploader, age, categoryId, <span class="keyword">length</span>, views, rate, ratings, comments, relatedId <span class="keyword">from</span>  gulivideo_orc </span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) catetory <span class="keyword">as</span> categoryId;</span><br></pre></td></tr></table></figure><p>统计Music类别的Top10（也可以统计其他）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>   </span><br><span class="line">videoId,  views </span><br><span class="line"><span class="keyword">from</span>  gulivideo_category  </span><br><span class="line"><span class="keyword">where</span>  categoryId = <span class="string">&quot;Music&quot;</span>  </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span>  views  <span class="keyword">desc</span> <span class="keyword">limit</span>  <span class="number">10</span>;</span><br></pre></td></tr></table></figure><h3 id="10-4-6-统计每个类别中视频流量Top10，以Music为例"><a href="#10-4-6-统计每个类别中视频流量Top10，以Music为例" class="headerlink" title="10.4.6 统计每个类别中视频流量Top10，以Music为例"></a>10.4.6 统计每个类别中视频流量Top10，以Music为例</h3><p>思路：</p><ol><li><p>创建视频类别展开表（categoryId列转行后的表）</p></li><li><p>按照ratings排序即可</p></li></ol><p>最终代码：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>   </span><br><span class="line">videoId,  views,  ratings  </span><br><span class="line"><span class="keyword">from</span>   gulivideo_category  </span><br><span class="line"><span class="keyword">where</span>   categoryId = <span class="string">&quot;Music&quot;</span>  </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span>   ratings  <span class="keyword">desc</span> <span class="keyword">limit</span>   <span class="number">10</span>; </span><br></pre></td></tr></table></figure><h3 id="10-4-7-统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频"><a href="#10-4-7-统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频" class="headerlink" title="10.4.7 统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频"></a>10.4.7 统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频</h3><p>思路：</p><ol><li>先找到上传视频最多的10个用户的用户信息</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>  *  </span><br><span class="line"><span class="keyword">from</span>   gulivideo_user_orc  </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span>   videos  <span class="keyword">desc</span> <span class="keyword">limit</span>   <span class="number">10</span>; </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>2)通过uploader字段与gulivideo_orc表进行join，得到的信息按照views观看次数进行排序即可。</p><p>最终代码：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>   </span><br><span class="line"></span><br><span class="line">t2.videoId,   t2.views,  t2.ratings,  t1.videos,  t1.friends  </span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> ( <span class="keyword">select</span>  *  <span class="keyword">from</span>  gulivideo_user_orc  <span class="keyword">order</span> <span class="keyword">by</span>  videos <span class="keyword">desc</span>   <span class="keyword">limit</span>   <span class="number">10</span>) t1 </span><br><span class="line"></span><br><span class="line"><span class="keyword">join</span>  gulivideo_orc t2 <span class="keyword">on</span>  t1.uploader = t2.uploader  </span><br><span class="line"></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span>  views <span class="keyword">desc</span>  <span class="keyword">limit</span>   <span class="number">20</span>; </span><br></pre></td></tr></table></figure><h3 id="10-4-8-统计每个类别视频观看数Top10"><a href="#10-4-8-统计每个类别视频观看数Top10" class="headerlink" title="10.4.8 统计每个类别视频观看数Top10"></a>10.4.8 统计每个类别视频观看数Top10</h3><p>思路：</p><ol><li>先得到categoryId展开的表数据</li></ol><p>2)子查询按照categoryId进行分区，然后分区内排序，并生成递增数字，该递增数字这一列起名为rank列</p><ol start="3"><li>通过子查询产生的临时表，查询rank值小于等于10的数据行即可。</li></ol><p>最终代码：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span>   t1.*  <span class="keyword">from</span> (  </span><br><span class="line">    <span class="keyword">select</span> </span><br><span class="line">    videoId, categoryId,  views, </span><br><span class="line">    row_number() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> categoryId <span class="keyword">order</span> <span class="keyword">by</span> views <span class="keyword">desc</span>) <span class="keyword">rank</span> </span><br><span class="line">    <span class="keyword">from</span> gulivideo_category</span><br><span class="line">) t1 <span class="keyword">where</span>  <span class="keyword">rank</span> &lt;= <span class="number">10</span>; </span><br></pre></td></tr></table></figure><h1 id="第11章-常见错误及解决方案"><a href="#第11章-常见错误及解决方案" class="headerlink" title="第11章 常见错误及解决方案"></a>第11章 常见错误及解决方案</h1><p>1）SecureCRT 7.3出现乱码或者删除不掉数据，免安装版的SecureCRT<br>卸载或者用虚拟机直接操作或者换安装版的SecureCRT</p><p>2）连接不上mysql数据库</p><p>（1）导错驱动包，应该把mysql-connector-java-5.1.27-bin.jar导入/opt/module/hive/lib的不是这个包。错把mysql-connector-java-5.1.27.tar.gz导入hive/lib包下。</p><p>（2）修改user表中的主机名称没有都修改为%，而是修改为localhost</p><p>3）hive默认的输入格式处理是CombineHiveInputFormat，会对小文件进行合并。</p><p>hive (default)&gt; set hive.input.format;</p><p>hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</p><blockquote><p>  可以采用HiveInputFormat就会根据分区数输出相应的文件。</p></blockquote><p>hive (default)&gt; set<br>hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</p><p>4）不能执行mapreduce程序</p><p>可能是hadoop的yarn没开启。</p><p>5）启动mysql服务时，报MySQL server PID file could not be found! 异常。</p><p>在/var/lock/subsys/mysql路径下创建hadoop102.pid，并在文件中添加内容：4396</p><p>6）报service mysql status MySQL is not running, but lock file<br>(/var/lock/subsys/mysql[失败])异常。</p><p>解决方案：在/var/lib/mysql 目录下创建： -rw-rw—-. 1 mysql mysql 5 12月 22<br>16:41 hadoop102.pid 文件，并修改权限为 777。</p><p>7）JVM堆内存溢出</p><p>描述：java.lang.OutOfMemoryError: Java heap space</p><p>解决：在yarn-site.xml中加入如下代码</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2.1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.child.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xmx1024m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;第1章-Hive入门&quot;&gt;&lt;a href=&quot;#第1章-Hive入门&quot; class=&quot;headerlink&quot; title=&quot;第1章 Hive入门&quot;&gt;&lt;/a&gt;第1章 Hive入门&lt;/h1&gt;&lt;h2 id=&quot;1-1-什么是Hive&quot;&gt;&lt;a href=&quot;#1-1-什么是Hive&quot; class=&quot;headerlink&quot; title=&quot;1.1 什么是Hive&quot;&gt;&lt;/a&gt;1.1 什么是Hive&lt;/h2&gt;&lt;p&gt;Hive：由Facebook开源用于解决海量结构化日志的数据统计。&lt;/p&gt;
&lt;p&gt;Hive是基于Hadoop的一个==&lt;strong&gt;数据仓库工具&lt;/strong&gt;==，可以将==&lt;strong&gt;结构化的数据文件映射为一张表&lt;/strong&gt;==，并提供==&lt;strong&gt;类SQL&lt;/strong&gt;==查询功能。&lt;/p&gt;
&lt;p&gt;==&lt;strong&gt;本质是：将HQL转化成MapReduce程序&lt;/strong&gt;==&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://gitee.com/curryfor369/picgo/raw/master/img/20200918113926.png&quot; alt=&quot;image-20200917232726291&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Hive" scheme="http://iscurry.com/categories/Hive/"/>
    
    
    <category term="Detail" scheme="http://iscurry.com/tags/Detail/"/>
    
    <category term="Hive" scheme="http://iscurry.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive开窗函数</title>
    <link href="http://iscurry.com/2020/01/01/Hive(3)%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0/"/>
    <id>http://iscurry.com/2020/01/01/Hive(3)%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0/</id>
    <published>2020-01-01T05:16:22.000Z</published>
    <updated>2020-09-25T02:17:27.500Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hive-开窗函数"><a href="#Hive-开窗函数" class="headerlink" title="Hive 开窗函数"></a>Hive 开窗函数</h1><pre><code>工作中用到了几个hive开窗函数,便想把hive开窗函数系统梳理一遍。</code></pre><h1 id="1、开窗函数定义"><a href="#1、开窗函数定义" class="headerlink" title="1、开窗函数定义"></a>1、开窗函数定义</h1><pre><code>普通的聚合函数聚合的行集是组,开窗函数聚合的行集是窗口。因此,普通的聚合函数每组(Group by)只返回一个值，而开窗函数则可为窗口中的每行都返回一个值。简单理解，就是对查询的结果多出一列，这一列可以是聚合值，也可以是排序值。开窗函数一般分为两类,聚合开窗函数和排序开窗函数。</code></pre><a id="more"></a><h1 id="2、测试数据"><a href="#2、测试数据" class="headerlink" title="2、测试数据"></a>2、测试数据</h1><pre><code>-- 建表create table student_scores(id int,studentId int,language int,math int,english int,classId string,departmentId string);-- 写入数据insert into table student_scores values   (1,111,68,69,90,&#39;class1&#39;,&#39;department1&#39;),  (2,112,73,80,96,&#39;class1&#39;,&#39;department1&#39;),  (3,113,90,74,75,&#39;class1&#39;,&#39;department1&#39;),  (4,114,89,94,93,&#39;class1&#39;,&#39;department1&#39;),  (5,115,99,93,89,&#39;class1&#39;,&#39;department1&#39;),  (6,121,96,74,79,&#39;class2&#39;,&#39;department1&#39;),  (7,122,89,86,85,&#39;class2&#39;,&#39;department1&#39;),  (8,123,70,78,61,&#39;class2&#39;,&#39;department1&#39;),  (9,124,76,70,76,&#39;class2&#39;,&#39;department1&#39;),  (10,211,89,93,60,&#39;class1&#39;,&#39;department2&#39;),  (11,212,76,83,75,&#39;class1&#39;,&#39;department2&#39;),  (12,213,71,94,90,&#39;class1&#39;,&#39;department2&#39;),  (13,214,94,94,66,&#39;class1&#39;,&#39;department2&#39;),  (14,215,84,82,73,&#39;class1&#39;,&#39;department2&#39;),  (15,216,85,74,93,&#39;class1&#39;,&#39;department2&#39;),  (16,221,77,99,61,&#39;class2&#39;,&#39;department2&#39;),  (17,222,80,78,96,&#39;class2&#39;,&#39;department2&#39;),  (18,223,79,74,96,&#39;class2&#39;,&#39;department2&#39;),  (19,224,75,80,78,&#39;class2&#39;,&#39;department2&#39;),  (20,225,82,85,63,&#39;class2&#39;,&#39;department2&#39;);</code></pre><h1 id="3、聚合开窗函数"><a href="#3、聚合开窗函数" class="headerlink" title="3、聚合开窗函数"></a>3、聚合开窗函数</h1><h3 id="count开窗函数"><a href="#count开窗函数" class="headerlink" title="count开窗函数"></a>count开窗函数</h3><pre><code>-- count 开窗函数select studentId,math,departmentId,classId,-- 以符合条件的所有行作为窗口count(math) over() as count1, -- 以按classId分组的所有行作为窗口count(math) over(partition by classId) as count2, -- 以按classId分组、按math排序的所有行作为窗口count(math) over(partition by classId order by math) as count3, -- 以按classId分组、按math排序、按 当前行+往前1行+往后2行的行作为窗口count(math) over(partition by classId order by math rows between 1 preceding and 2 following) as count4from student_scores where departmentId=&#39;department1&#39;;结果studentid   math    departmentid    classid count1  count2  count3  count4111         69      department1     class1  9       5       1       3113         74      department1     class1  9       5       2       4112         80      department1     class1  9       5       3       4115         93      department1     class1  9       5       4       3114         94      department1     class1  9       5       5       2124         70      department1     class2  9       4       1       3121         74      department1     class2  9       4       2       4123         78      department1     class2  9       4       3       3122         86      department1     class2  9       4       4       2结果解释:studentid=115,count1为所有的行数9,count2为分区class1中的行数5,count3为分区class1中math值&lt;=93的行数4,count4为分区class1中math值向前+1行向后+2行(实际只有1行)的总行数3。</code></pre><h3 id="sum开窗函数"><a href="#sum开窗函数" class="headerlink" title="sum开窗函数"></a>sum开窗函数</h3><pre><code>-- sum开窗函数select studentId,math,departmentId,classId,-- 以符合条件的所有行作为窗口sum(math) over() as sum1,-- 以按classId分组的所有行作为窗口sum(math) over(partition by classId) as sum2, -- 以按classId分组、按math排序后、按到当前行(含当前行)的所有行作为窗口sum(math) over(partition by classId order by math) as sum3, -- 以按classId分组、按math排序后、按当前行+往前1行+往后2行的行作为窗口sum(math) over(partition by classId order by math rows between 1 preceding and 2 following) as sum4from student_scores where departmentId=&#39;department1&#39;;结果studentid   math    departmentid    classid sum1    sum2    sum3    sum4111         69      department1     class1  718     410     69      223113         74      department1     class1  718     410     143     316112         80      department1     class1  718     410     223     341115         93      department1     class1  718     410     316     267114         94      department1     class1  718     410     410     187124         70      department1     class2  718     308     70      222121         74      department1     class2  718     308     144     308123         78      department1     class2  718     308     222     238122         86      department1     class2  718     308     308     164结果解释:    同count开窗函数</code></pre><h3 id="min开窗函数"><a href="#min开窗函数" class="headerlink" title="min开窗函数"></a>min开窗函数</h3><pre><code>-- min 开窗函数select studentId,math,departmentId,classId,-- 以符合条件的所有行作为窗口min(math) over() as min1,-- 以按classId分组的所有行作为窗口min(math) over(partition by classId) as min2, -- 以按classId分组、按math排序后、按到当前行(含当前行)的所有行作为窗口min(math) over(partition by classId order by math) as min3, -- 以按classId分组、按math排序后、按当前行+往前1行+往后2行的行作为窗口min(math) over(partition by classId order by math rows between 1 preceding and 2 following) as min4from student_scores where departmentId=&#39;department1&#39;;结果studentid   math    departmentid    classid min1    min2    min3    min4111         69      department1     class1  69      69      69      69113         74      department1     class1  69      69      69      69112         80      department1     class1  69      69      69      74115         93      department1     class1  69      69      69      80114         94      department1     class1  69      69      69      93124         70      department1     class2  69      70      70      70121         74      department1     class2  69      70      70      70123         78      department1     class2  69      70      70      74122         86      department1     class2  69      70      70      78结果解释:    同count开窗函数</code></pre><h3 id="max开窗函数"><a href="#max开窗函数" class="headerlink" title="max开窗函数"></a>max开窗函数</h3><pre><code>-- max 开窗函数select studentId,math,departmentId,classId,-- 以符合条件的所有行作为窗口max(math) over() as max1,-- 以按classId分组的所有行作为窗口max(math) over(partition by classId) as max2, -- 以按classId分组、按math排序后、按到当前行(含当前行)的所有行作为窗口max(math) over(partition by classId order by math) as max3, -- 以按classId分组、按math排序后、按当前行+往前1行+往后2行的行作为窗口max(math) over(partition by classId order by math rows between 1 preceding and 2 following) as max4from student_scores where departmentId=&#39;department1&#39;;结果studentid   math    departmentid    classid max1    max2    max3    max4111         69      department1     class1  94      94      69      80113         74      department1     class1  94      94      74      93112         80      department1     class1  94      94      80      94115         93      department1     class1  94      94      93      94114         94      department1     class1  94      94      94      94124         70      department1     class2  94      86      70      78121         74      department1     class2  94      86      74      86123         78      department1     class2  94      86      78      86122         86      department1     class2  94      86      86      86结果解释:    同count开窗函数</code></pre><h3 id="avg开窗函数"><a href="#avg开窗函数" class="headerlink" title="avg开窗函数"></a>avg开窗函数</h3><pre><code>-- avg 开窗函数select studentId,math,departmentId,classId,-- 以符合条件的所有行作为窗口avg(math) over() as avg1,-- 以按classId分组的所有行作为窗口avg(math) over(partition by classId) as avg2, -- 以按classId分组、按math排序后、按到当前行(含当前行)的所有行作为窗口avg(math) over(partition by classId order by math) as avg3, -- 以按classId分组、按math排序后、按当前行+往前1行+往后2行的行作为窗口avg(math) over(partition by classId order by math rows between 1 preceding and 2 following) as avg4from student_scores where departmentId=&#39;department1&#39;;结果studentid   math    departmentid    classid avg1                avg2    avg3                avg4111         69      department1     class1  79.77777777777777   82.0    69.0                74.33333333333333113         74      department1     class1  79.77777777777777   82.0    71.5                79.0112         80      department1     class1  79.77777777777777   82.0    74.33333333333333   85.25115         93      department1     class1  79.77777777777777   82.0    79.0                89.0114         94      department1     class1  79.77777777777777   82.0    82.0                93.5124         70      department1     class2  79.77777777777777   77.0    70.0                74.0121         74      department1     class2  79.77777777777777   77.0    72.0                77.0123         78      department1     class2  79.77777777777777   77.0    74.0                79.33333333333333122         86      department1     class2  79.77777777777777   77.0    77.0                82.0结果解释:    同count开窗函数</code></pre><h3 id="first-value开窗函数"><a href="#first-value开窗函数" class="headerlink" title="first_value开窗函数"></a>first_value开窗函数</h3><pre><code>返回分区中的第一个值。-- first_value 开窗函数select studentId,math,departmentId,classId,-- 以符合条件的所有行作为窗口first_value(math) over() as first_value1,-- 以按classId分组的所有行作为窗口first_value(math) over(partition by classId) as first_value2, -- 以按classId分组、按math排序后、按到当前行(含当前行)的所有行作为窗口first_value(math) over(partition by classId order by math) as first_value3, -- 以按classId分组、按math排序后、按当前行+往前1行+往后2行的行作为窗口first_value(math) over(partition by classId order by math rows between 1 preceding and 2 following) as first_value4from student_scores where departmentId=&#39;department1&#39;;结果studentid   math    departmentid    classid first_value1    first_value2    first_value3    first_value4111         69      department1     class1  69              69              69              69113         74      department1     class1  69              69              69              69112         80      department1     class1  69              69              69              74115         93      department1     class1  69              69              69              80114         94      department1     class1  69              69              69              93124         70      department1     class2  69              74              70              70121         74      department1     class2  69              74              70              70123         78      department1     class2  69              74              70              74122         86      department1     class2  69              74              70              78结果解释:    studentid=124 first_value1:第一个值是69,first_value2:classId=class1分区 math的第一个值是69。</code></pre><h3 id="last-value开窗函数"><a href="#last-value开窗函数" class="headerlink" title="last_value开窗函数"></a>last_value开窗函数</h3><pre><code>返回分区中的第一个值。-- last_value 开窗函数select studentId,math,departmentId,classId,-- 以符合条件的所有行作为窗口last_value(math) over() as last_value1,-- 以按classId分组的所有行作为窗口last_value(math) over(partition by classId) as last_value2, -- 以按classId分组、按math排序后、按到当前行(含当前行)的所有行作为窗口last_value(math) over(partition by classId order by math) as last_value3, -- 以按classId分组、按math排序后、按当前行+往前1行+往后2行的行作为窗口last_value(math) over(partition by classId order by math rows between 1 preceding and 2 following) as last_value4from student_scores where departmentId=&#39;department1&#39;;结果studentid   math    departmentid    classid last_value1 last_value2 last_value3 last_value4111         69      department1     class1  70          93          69          80113         74      department1     class1  70          93          74          93112         80      department1     class1  70          93          80          94115         93      department1     class1  70          93          93          94114         94      department1     class1  70          93          94          94124         70      department1     class2  70          70          70          78121         74      department1     class2  70          70          74          86123         78      department1     class2  70          70          78          86122         86      department1     class2  70          70          86          86</code></pre><h1 id="4、lag开窗函数"><a href="#4、lag开窗函数" class="headerlink" title="4、lag开窗函数"></a>4、lag开窗函数</h1><pre><code>lag(col,n,default) 用于统计窗口内往上第n个值。    col:列名    n:往上第n行    default:往上第n行为NULL时候，取默认值,不指定则取NULL-- lag 开窗函数select studentId,math,departmentId,classId, --窗口内 往上取第二个 取不到时赋默认值60lag(math,2,60) over(partition by classId order by math) as lag1, --窗口内 往上取第二个 取不到时赋默认值NULLlag(math,2) over(partition by classId order by math) as lag2from student_scores where departmentId=&#39;department1&#39;;结果studentid   math    departmentid    classid lag1    lag2111         69      department1     class1  60      NULL113         74      department1     class1  60      NULL112         80      department1     class1  69      69115         93      department1     class1  74      74114         94      department1     class1  80      80124         70      department1     class2  60      NULL121         74      department1     class2  60      NULL123         78      department1     class2  70      70122         86      department1     class2  74      74结果解释:    第3行 lag1:窗口内(69 74 80) 当前行80 向上取第二个值为69    倒数第3行 lag2:窗口内(70 74) 当前行74 向上取第二个值为NULL</code></pre><h3 id="lead开窗函数"><a href="#lead开窗函数" class="headerlink" title="lead开窗函数"></a>lead开窗函数</h3><pre><code>lead(col,n,default) 用于统计窗口内往下第n个值。    col:列名    n:往下第n行    default:往下第n行为NULL时候，取默认值,不指定则取NULL-- lead开窗函数select studentId,math,departmentId,classId, --窗口内 往下取第二个 取不到时赋默认值60lead(math,2,60) over(partition by classId order by math) as lead1, --窗口内 往下取第二个 取不到时赋默认值NULLlead(math,2) over(partition by classId order by math) as lead2from student_scores where departmentId=&#39;department1&#39;;结果studentid   math    departmentid    classid lead1   lead2111         69      department1     class1  80      80113         74      department1     class1  93      93112         80      department1     class1  94      94115         93      department1     class1  60      NULL114         94      department1     class1  60      NULL124         70      department1     class2  78      78121         74      department1     class2  86      86123         78      department1     class2  60      NULL122         86      department1     class2  60      NULL结果解释:    第4行lead1 窗口内向下第二个值为空，赋值60</code></pre><h3 id="cume-dist开窗函数"><a href="#cume-dist开窗函数" class="headerlink" title="cume_dist开窗函数"></a>cume_dist开窗函数</h3><pre><code>计算某个窗口或分区中某个值的累积分布。假定升序排序，则使用以下公式确定累积分布：小于等于当前值x的行数 / 窗口或partition分区内的总行数。其中，x 等于 order by 子句中指定的列的当前行中的值。-- cume_dist 开窗函数select studentId,math,departmentId,classId,-- 统计小于等于当前分数的人数占总人数的比例cume_dist() over(order by math) as cume_dist1,-- 统计大于等于当前分数的人数占总人数的比例cume_dist() over(order by math desc) as cume_dist2,-- 统计分区内小于等于当前分数的人数占总人数的比例cume_dist() over(partition by classId order by math) as cume_dist3from student_scores where departmentId=&#39;department1&#39;;结果studentid   math    departmentid    classid cume_dist1              cume_dist2          cume_dist3111         69      department1     class1  0.1111111111111111      1.0                 0.2113         74      department1     class1  0.4444444444444444      0.7777777777777778  0.4112         80      department1     class1  0.6666666666666666      0.4444444444444444  0.6115         93      department1     class1  0.8888888888888888      0.2222222222222222  0.8114         94      department1     class1  1.0                     0.1111111111111111  1.0124         70      department1     class2  0.2222222222222222      0.8888888888888888  0.25121         74      department1     class2  0.4444444444444444      0.7777777777777778  0.5123         78      department1     class2  0.5555555555555556      0.5555555555555556  0.75122         86      department1     class2  0.7777777777777778      0.3333333333333333  1.0结果解释:    第三行:        cume_dist1=小于等于80的人数为6/总人数9=0.6666666666666666        cume_dist2=大于等于80的人数为4/总人数9=0.4444444444444444        cume_dist3=分区内小于等于80的人数为3/分区内总人数5=0.6</code></pre><h1 id="5、排序开窗函数"><a href="#5、排序开窗函数" class="headerlink" title="5、排序开窗函数"></a>5、排序开窗函数</h1><h3 id="rank开窗函数"><a href="#rank开窗函数" class="headerlink" title="rank开窗函数"></a>rank开窗函数</h3><pre><code>rank 开窗函数基于 over 子句中的 order by 确定一组值中一个值的排名。如果存在partition by ,则为每个分区组中的每个值排名。排名可能不是连续的。例如，如果两个行的排名为 1，则下一个排名为 3。-- rank 开窗函数select *,-- 对全部学生按数学分数排序 rank() over(order by math) as rank1,-- 对院系 按数学分数排序rank() over(partition by departmentId order by math) as rank2,-- 对每个院系每个班级 按数学分数排序rank() over(partition by departmentId,classId order by math) as rank3from student_scores;结果id  studentid   language    math    english     classid departmentid    rank1   rank2   rank31   111         68          69      90          class1  department1     1       1       13   113         90          74      75          class1  department1     3       3       22   112         73          80      96          class1  department1     9       6       35   115         99          93      89          class1  department1     15      8       44   114         89          94      93          class1  department1     17      9       59   124         76          70      76          class2  department1     2       2       16   121         96          74      79          class2  department1     3       3       28   123         70          78      61          class2  department1     7       5       37   122         89          86      85          class2  department1     14      7       415  216         85          74      93          class1  department2     3       1       114  215         84          82      73          class1  department2     11      5       211  212         76          83      75          class1  department2     12      6       310  211         89          93      60          class1  department2     15      8       412  213         71          94      90          class1  department2     17      9       513  214         94          94      66          class1  department2     17      9       518  223         79          74      96          class2  department2     3       1       117  222         80          78      96          class2  department2     7       3       219  224         75          80      78          class2  department2     9       4       320  225         82          85      63          class2  department2     13      7       416  221         77          99      61          class2  department2     20      11      5</code></pre><h3 id="dense-rank开窗函数"><a href="#dense-rank开窗函数" class="headerlink" title="dense_rank开窗函数"></a>dense_rank开窗函数</h3><pre><code>dense_rank与rank有一点不同,当排名一样的时候,接下来的行是连续的。如两个行的排名为 1，则下一个排名为 2。-- dense_rank 开窗函数select *,-- 对全部学生按数学分数排序dense_rank() over(order by math) as dense_rank1,-- 对院系 按数学分数排序dense_rank() over(partition by departmentId order by math) as dense_rank2,-- 对每个院系每个班级 按数学分数排序dense_rank() over(partition by departmentId,classId order by math) as dense_rank3from student_scores;结果:id  studentid   language    math    english classid departmentid    dense_rank1 dense_rank2 dense_rank31   111         68          69      90      class1  department1     1           1           13   113         90          74      75      class1  department1     3           3           22   112         73          80      96      class1  department1     5           5           35   115         99          93      89      class1  department1     10          7           44   114         89          94      93      class1  department1     11          8           59   124         76          70      76      class2  department1     2           2           16   121         96          74      79      class2  department1     3           3           28   123         70          78      61      class2  department1     4           4           37   122         89          86      85      class2  department1     9           6           415  216         85          74      93      class1  department2     3           1           114  215         84          82      73      class1  department2     6           4           211  212         76          83      75      class1  department2     7           5           310  211         89          93      60      class1  department2     10          7           412  213         71          94      90      class1  department2     11          8           513  214         94          94      66      class1  department2     11          8           518  223         79          74      96      class2  department2     3           1           117  222         80          78      96      class2  department2     4           2           219  224         75          80      78      class2  department2     5           3           320  225         82          85      63      class2  department2     8           6           416  221         77          99      61      class2  department2     12          9           5</code></pre><h3 id="ntile开窗函数"><a href="#ntile开窗函数" class="headerlink" title="ntile开窗函数"></a>ntile开窗函数</h3><pre><code>将分区中已排序的行划分为大小尽可能相等的指定数量的排名的组，并返回给定行所在的组的排名。-- ntile 开窗函数select *,-- 对分区内的数据分成两组ntile(2) over(partition by departmentid order by math) as ntile1,-- 对分区内的数据分成三组ntile(3) over(partition by departmentid order by math) as ntile2from student_scores;结果id  studentid   language    math    english classid departmentid    ntile1  ntile21   111         68          69      90      class1  department1     1       19   124         76          70      76      class2  department1     1       16   121         96          74      79      class2  department1     1       13   113         90          74      75      class1  department1     1       28   123         70          78      61      class2  department1     1       22   112         73          80      96      class1  department1     2       27   122         89          86      85      class2  department1     2       35   115         99          93      89      class1  department1     2       34   114         89          94      93      class1  department1     2       318  223         79          74      96      class2  department2     1       115  216         85          74      93      class1  department2     1       117  222         80          78      96      class2  department2     1       119  224         75          80      78      class2  department2     1       114  215         84          82      73      class1  department2     1       211  212         76          83      75      class1  department2     1       220  225         82          85      63      class2  department2     2       210  211         89          93      60      class1  department2     2       212  213         71          94      90      class1  department2     2       313  214         94          94      66      class1  department2     2       316  221         77          99      61      class2  department2     2       3结果解释:    第8行        ntile1:对分区的数据均匀分成2组后，当前行的组排名为2        ntile2:对分区的数据均匀分成3组后，当前行的组排名为3</code></pre><h3 id="row-number开窗函数"><a href="#row-number开窗函数" class="headerlink" title="row_number开窗函数"></a>row_number开窗函数</h3><pre><code>从1开始对分区内的数据排序。-- row_number 开窗函数select studentid,departmentid,classid,math,-- 对分区departmentid,classid内的数据按math排序row_number() over(partition by departmentid,classid order by math) as row_numberfrom student_scores;结果studentid   departmentid    classid math    row_number111         department1     class1  69      1113         department1     class1  74      2112         department1     class1  80      3115         department1     class1  93      4114         department1     class1  94      5124         department1     class2  70      1121         department1     class2  74      2123         department1     class2  78      3122         department1     class2  86      4216         department2     class1  74      1215         department2     class1  82      2212         department2     class1  83      3211         department2     class1  93      4213         department2     class1  94      5214         department2     class1  94      6223         department2     class2  74      1222         department2     class2  78      2224         department2     class2  80      3225         department2     class2  85      4221         department2     class2  99      5结果解释:    同一分区,相同值，不同序。如studentid=213 studentid=214 值都为94 排序为5,6。</code></pre><h3 id="percent-rank开窗函数"><a href="#percent-rank开窗函数" class="headerlink" title="percent_rank开窗函数"></a>percent_rank开窗函数</h3><pre><code>计算给定行的百分比排名。可以用来计算超过了百分之多少的人。如360小助手开机速度超过了百分之多少的人。(当前行的rank值-1)/(分组内的总行数-1)-- percent_rank 开窗函数select studentid,departmentid,classid,math,row_number() over(partition by departmentid,classid order by math) as row_number,percent_rank() over(partition by departmentid,classid order by math) as percent_rankfrom student_scores;结果studentid   departmentid    classid math    row_number  percent_rank111         department1     class1  69      1           0.0113         department1     class1  74      2           0.25112         department1     class1  80      3           0.5115         department1     class1  93      4           0.75114         department1     class1  94      5           1.0124         department1     class2  70      1           0.0121         department1     class2  74      2           0.3333333333333333123         department1     class2  78      3           0.6666666666666666122         department1     class2  86      4           1.0216         department2     class1  74      1           0.0215         department2     class1  82      2           0.2212         department2     class1  83      3           0.4211         department2     class1  93      4           0.6213         department2     class1  94      5           0.8214         department2     class1  94      6           0.8223         department2     class2  74      1           0.0222         department2     class2  78      2           0.25224         department2     class2  80      3           0.5225         department2     class2  85      4           0.75221         department2     class2  99      5           1.0结果解释:    studentid=115,percent_rank=(4-1)/(5-1)=0.75    studentid=123,percent_rank=(3-1)/(4-1)=0.6666666666666666</code></pre>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Hive-开窗函数&quot;&gt;&lt;a href=&quot;#Hive-开窗函数&quot; class=&quot;headerlink&quot; title=&quot;Hive 开窗函数&quot;&gt;&lt;/a&gt;Hive 开窗函数&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;工作中用到了几个hive开窗函数,便想把hive开窗函数系统梳理一遍。&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&quot;1、开窗函数定义&quot;&gt;&lt;a href=&quot;#1、开窗函数定义&quot; class=&quot;headerlink&quot; title=&quot;1、开窗函数定义&quot;&gt;&lt;/a&gt;1、开窗函数定义&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;普通的聚合函数聚合的行集是组,开窗函数聚合的行集是窗口。因此,普通的聚合函数每组(Group by)只返回一个值，而开窗函数则可为窗口中的每行都返回一个值。
简单理解，就是对查询的结果多出一列，这一列可以是聚合值，也可以是排序值。
开窗函数一般分为两类,聚合开窗函数和排序开窗函数。&lt;/code&gt;&lt;/pre&gt;</summary>
    
    
    
    <category term="Hive" scheme="http://iscurry.com/categories/Hive/"/>
    
    
    <category term="Hive" scheme="http://iscurry.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Kafka(2)详细</title>
    <link href="http://iscurry.com/2019/12/22/Kafka(2)%E8%AF%A6%E7%BB%86/"/>
    <id>http://iscurry.com/2019/12/22/Kafka(2)%E8%AF%A6%E7%BB%86/</id>
    <published>2019-12-22T01:45:02.000Z</published>
    <updated>2020-09-25T02:17:07.141Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第1章-Kafka概述"><a href="#第1章-Kafka概述" class="headerlink" title="第1章 Kafka概述"></a>第1章 Kafka概述</h1><h2 id="1-1定义"><a href="#1-1定义" class="headerlink" title="1.1定义"></a>1.1定义</h2><p>Kafka是一个分布式的基于发布/订阅模式的<strong>消息队列，</strong>主要应用于大数据实时处理领域。</p><h2 id="1-2-消息队列（Message-Queue）"><a href="#1-2-消息队列（Message-Queue）" class="headerlink" title="1.2 消息队列（Message Queue）"></a>1.2 消息队列（Message Queue）</h2><h3 id="1-2-1-传统消息队列应用场景"><a href="#1-2-1-传统消息队列应用场景" class="headerlink" title="1.2.1 传统消息队列应用场景"></a><strong>1.2.1</strong> <strong>传统消息队列应用场景</strong></h3><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142517.png"></p><a id="more"></a><h3 id="1-2-2-消息队列的两种模式"><a href="#1-2-2-消息队列的两种模式" class="headerlink" title="1.2.2 消息队列的两种模式"></a><strong>1.2.2</strong> <strong>消息队列的两种模式</strong></h3><p><strong>（1 ）点对点模式</strong>（<strong>一对一</strong>，消费者主动拉取数据，消息收到后消息清除）</p><p>消息生产者生产消息发送到Queue中，然后消息消费者从Queue中取出并且消费消息。</p><p>消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。  </p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142518.png"></p><p><strong>（2 ）发布/订阅模式</strong>（<strong>一对多</strong>，消费者消费数据之后不会清除消息）</p><p>消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142519.png"></p><h2 id="1-3-Kafka基础架构"><a href="#1-3-Kafka基础架构" class="headerlink" title="1.3 Kafka基础架构"></a>1.3 Kafka基础架构</h2><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922134043.png" alt="image-20200922100703077"></p><p><strong>1）Producer</strong>：消息生产者，就是向kafka broker发消息的客户端；<br><strong>2）Consumer</strong>：消息消费者，向kafka broker取消息的客户端；<br><strong>3）Consumer Group （CG）</strong>：消费者组，由多个consumer组成。<strong>消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个消费者消费；消费者组之间互不影响。</strong>所有的消费者都属于某个消费者组，<strong>即消费者组是逻辑上的一个订阅者。</strong><br><strong>4）Broker</strong>：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。<br><strong>5）Topic</strong>：可以理解为一个队列(Que)，<strong>生产者和消费者面向的都是一个topic；</strong><br><strong>6）Partition</strong>：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，<strong>一个topic可以分为多个partition</strong>，每个partition是一个有序的队列(Que)；<br><strong>7）Replica</strong>：副本，为保证集群中的某个节点发生故障时， <strong>该节点上的partition数据不丢失，且kafka仍然能够继续工作</strong>，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。<br><strong>8）leader</strong>：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。<br><strong>9）follower</strong>：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader。</p><h1 id="第2章-Kafka快速入门"><a href="#第2章-Kafka快速入门" class="headerlink" title="第2章 Kafka快速入门"></a>第2章 Kafka快速入门</h1><h2 id="2-1-安装部署"><a href="#2-1-安装部署" class="headerlink" title="2.1 安装部署"></a>2.1 安装部署</h2><h3 id="2-1-1-集群规划"><a href="#2-1-1-集群规划" class="headerlink" title="2.1.1 集群规划"></a><strong>2.1.1</strong> <strong>集群规划</strong></h3><table><thead><tr><th>hadoop102</th><th>hadoop103</th><th>hadoop104</th></tr></thead><tbody><tr><td>zk</td><td>zk</td><td>zk</td></tr><tr><td>kafka</td><td>kafka</td><td>kafka</td></tr></tbody></table><h3 id="2-1-2-jar包下载"><a href="#2-1-2-jar包下载" class="headerlink" title="2.1.2 jar包下载"></a><strong>2.1.2 jar包下载</strong></h3><p><a href="http://kafka.apache.org/2downloads.html">http://kafka.apache.org/2downloads.html</a></p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142520.png"></p><h3 id="2-1-3-集群部署"><a href="#2-1-3-集群部署" class="headerlink" title="2.1.3 集群部署"></a><strong>2.1.3</strong> <strong>集群部署</strong></h3><p>1）解压安装包</p><blockquote><p>[xing@hadoop102 software]$ tar -zxvf kafka_2.11-0.11.0.0.tgz -C /opt/module/</p></blockquote><p>2）修改解压后的文件名称</p><blockquote><p>[xing@hadoop102 module]$ mv kafka_2.11-0.11.0.0/ kafka</p></blockquote><p>3）在/opt/module/kafka目录下创建logs文件夹</p><blockquote><p>[xing@hadoop102 kafka]$ mkdir logs</p></blockquote><p>4）修改配置文件</p><blockquote><p>[xing@hadoop102 kafka]$ cd config/<br>[xing@hadoop102 config]$ vi server.properties</p></blockquote><p>输入以下内容：</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142521.png"></p><p>​                                                        </p><p>5）配置环境变量</p><blockquote><p>[xing@hadoop102 module]$ sudo vi /etc/profile</p></blockquote> <figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#KAFKA_HOME</span></span><br><span class="line"><span class="attr">export</span> <span class="string">KAFKA_HOME=/opt/module/kafka</span></span><br><span class="line"><span class="attr">export</span> <span class="string">PATH=$PATH:$KAFKA_HOME/bin</span></span><br></pre></td></tr></table></figure><blockquote><p>[xing@hadoop102 module]$ source /etc/profile</p></blockquote><p>6）分发安装包</p><blockquote><p>[xing@hadoop102 module]$ xsync kafka/</p></blockquote><blockquote><p>注意：分发之后记得配置其他机器的环境变量</p></blockquote><p>7）分别在hadoop103和hadoop104上修改配置文件/opt/module/kafka/config/server.properties中的broker.id=1、broker.id=2</p><blockquote><p>注：broker.id不得重复</p></blockquote><p>8）启动集群</p><p>依次在hadoop102、hadoop103、hadoop104节点上启动kafka</p><blockquote><p>[xing@hadoop102 kafka]$ bin/kafka-server-start.sh -daemon config/server.properties</p></blockquote><blockquote><p>[xing@hadoop103 kafka]$ bin/kafka-server-start.sh -daemon  config/server.properties</p></blockquote><blockquote><p>[xing@hadoop104 kafka]$ bin/kafka-server-start.sh -daemon  config/server.properties</p></blockquote><p>9）关闭集群</p><blockquote><p>[xing@hadoop102 kafka]$ bin/kafka-server-stop.sh</p></blockquote><blockquote><p>[xing@hadoop103 kafka]$ bin/kafka-server-stop.sh</p></blockquote><blockquote><p>[xing@hadoop104 kafka]$ bin/kafka-server-stop.sh</p></blockquote><p>10）kafka群起脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> `cat /opt/module/hadoop-2.7.2/etc/hadoop/slaves`</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;****** <span class="variable">$i</span> ******&quot;</span> </span><br><span class="line">ssh <span class="variable">$i</span> <span class="string">&#x27;source /etc/profile&amp;&amp;/opt/module/kafka_2.11-0.11.0.2/bin/kafka-server-start.sh -daemon /opt/module/kafka_2.11-0.11.0.2/config/server.properties&#x27;</span></span><br><span class="line"><span class="built_in">echo</span> $?</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h2 id="2-2-Kafka命令行操作"><a href="#2-2-Kafka命令行操作" class="headerlink" title="2.2 Kafka命令行操作"></a>2.2 Kafka命令行操作</h2><p>1）查看当前服务器中的所有topic</p><blockquote><p>[xing@hadoop102 kafka]$ bin/kafka-topics.sh –zookeeper hadoop102:2181 –list</p></blockquote><p>2）创建topic</p><blockquote><p>[xing@hadoop102 kafka]$ bin/kafka-topics.sh –zookeeper hadoop102:2181 \ </p><p>–create –replication-factor 3 –partitions 1 –topic first</p></blockquote><p>选项说明：</p><blockquote><p>–topic 定义topic名</p><p>–replication-factor 定义副本数</p><p>–partitions 定义分区数</p></blockquote><p>3）删除topic</p><blockquote><p>[xing@hadoop102 kafka]$ bin/kafka-topics.sh –zookeeper hadoop102:2181 \</p><p>–delete –topic first</p></blockquote><blockquote><p>需要server.properties中设置delete.topic.enable=true<strong>否则只是标记删除。</strong></p></blockquote><p>4）发送消息</p><blockquote><p>[xing@hadoop102 kafka]$ bin/kafka-console-producer.sh \</p><p>–broker-list hadoop102:9092 –topic first </p><p>&gt;hello world</p><p>&gt;xing xing</p></blockquote><p>5）消费消息</p><blockquote><p>[xing@hadoop103 kafka]$ bin/kafka-console-consumer.sh \</p><p>–bootstrap-server hadoop102:9092 –from-beginning –topic first</p></blockquote><blockquote><p>[xing@hadoop103 kafka]$ bin/kafka-console-consumer.sh \</p><p>–bootstrap-server hadoop102:9092 –from-beginning –topic first</p></blockquote><p>–from-beginning：会把主题中以往所有的数据都读取出来。</p><p>6）查看某个Topic的详情</p><blockquote><p>[xing@hadoop102 kafka]$ bin/kafka-topics.sh –zookeeper hadoop102:2181 \</p><p>–describe –topic first</p></blockquote><p>7）修改分区数</p><blockquote><p>[xing@hadoop102 kafka]$bin/kafka-topics.sh –zookeeper hadoop102:2181 –alter –topic first –partitions 6</p></blockquote><h1 id="第3章-Kafka架构深入"><a href="#第3章-Kafka架构深入" class="headerlink" title="第3章 Kafka架构深入"></a>第3章 Kafka架构深入</h1><h2 id="3-1-Kafka工作流程及文件存储机制"><a href="#3-1-Kafka工作流程及文件存储机制" class="headerlink" title="3.1 Kafka工作流程及文件存储机制"></a>3.1 Kafka工作流程及文件存储机制</h2><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142523.png"></p><p><strong>Kafka</strong>中消息是以<strong>topic</strong> 进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。</p><p>topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142522.png"></p><p>由于生产者生产的消息会不断追加到log文件末尾，<strong>为防止log文件过大</strong>导致数据定位效率低下，Kafka采取了<strong>分片</strong>和<strong>索引</strong>机制，将每个partition分为多个segment。每个segment对应两个文件——“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号。例如，first这个topic有三个分区，则其对应的文件夹为first-0,first-1,first-2。</p><p>00000000000000000000.index</p><p>00000000000000000000.log</p><p>00000000000000170410.index</p><p>00000000000000170410.log</p><p>00000000000000239430.index</p><p>00000000000000239430.log</p><p>index和log文件以当前segment的第一条消息的offset命名。下图为index文件和log文件的结构示意图。</p><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142524.png"></p><p><strong>“.index”文件存储大量的索引信息，“.log”文件存储大量的数据，索引文件中的元数据指向对应数据文件中message的物理偏移地址。</strong></p><h2 id="3-2-Kafka生产者"><a href="#3-2-Kafka生产者" class="headerlink" title="3.2 Kafka生产者"></a>3.2 Kafka生产者</h2><h3 id="3-2-2-分区策略"><a href="#3-2-2-分区策略" class="headerlink" title="3.2.2 分区策略"></a>3.2.2 分区策略</h3><h4 id="1）分区的原因"><a href="#1）分区的原因" class="headerlink" title="1）分区的原因"></a><strong>1）分区的原因</strong></h4><p>（1）<strong>方便在集群中扩展</strong>，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；</p><p>（2）<strong>可以提高并发</strong>，因为可以以Partition为单位读写了。</p><h4 id="2）分区的原则"><a href="#2）分区的原则" class="headerlink" title="2）分区的原则"></a><strong>2）分区的原则</strong></h4><p>我们需要将producer发送的数据封装成一个<strong>ProducerRecord</strong>对象。</p><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142525.png"></p><p>（1）指明 partition 的情况下，直接将指明的值作为 partiton 值；</p><p>（2）没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值；</p><p>（3）既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。</p><h3 id="3-2-3-数据可靠性保证"><a href="#3-2-3-数据可靠性保证" class="headerlink" title="3.2.3 数据可靠性保证"></a>3.2.3 数据可靠性保证</h3><p><strong>为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。</strong></p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142526.png"></p><h4 id="1）副本数据同步策略"><a href="#1）副本数据同步策略" class="headerlink" title="1）副本数据同步策略"></a><strong>1）副本数据同步策略</strong></h4><table><thead><tr><th><strong>方案</strong></th><th><strong>优点</strong></th><th><strong>缺点</strong></th></tr></thead><tbody><tr><td><strong>半数以上完成同步，就发送ack</strong></td><td>延迟低</td><td>选举新的leader时，容忍n台节点的故障，需要2n+1个副本</td></tr><tr><td><strong>全部完成同步，才发送ack</strong></td><td>选举新的leader时，容忍n台节点的故障，需要n+1个副本</td><td>延迟高</td></tr></tbody></table><p>Kafka选择了第二种方案，原因如下：</p><p>1.同样为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。</p><p>2.虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小。</p><h4 id="2）ISR2"><a href="#2）ISR2" class="headerlink" title="2）ISR2"></a><strong>2）ISR2</strong></h4><p>​    采用第二种方案之后，设想以下情景：leader收到数据，所有follower都开始同步数据，但有一个follower，因为某种故障，迟迟不能与leader进行同步，那leader就要一直等下去，直到它完成同步，才能发送ack。这个问题怎么解决呢？</p><p> <strong>Leader维护了一个动态的in-sync replica set (ISR)，意为和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给producer发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由</strong>replica.lag.time.max.ms<strong>参数设定。Leader发生故障之后，就会从ISR中选举新的leader。</strong></p><h4 id="3）ack应答机制"><a href="#3）ack应答机制" class="headerlink" title="3）ack应答机制"></a><strong>3）ack应答机制</strong></h4><p>对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等ISR中的follower全部接收成功。</p><p>所以Kafka为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。</p><p><strong>acks参数配置：</strong></p><p><strong>acks</strong>：</p><p>0：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能<strong>丢失数据</strong>；</p><p>1：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会<strong>丢失数据</strong>；</p><p>-1（all）：producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成<strong>数据重复</strong>。</p><h4 id="4）故障处理细节"><a href="#4）故障处理细节" class="headerlink" title="4）故障处理细节"></a><strong>4）故障处理细节</strong></h4><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142528.png"></p><p><strong>（1）follower故障</strong></p><p>follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该<strong>follower的LEO大于等于该Partition的HW</strong>，即follower追上leader之后，就可以重新加入ISR了。</p><p><strong>（2）leader故障</strong></p><p>leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于<strong>HW</strong>的部分截掉，然后从新的leader同步数据。</p><p><strong>注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复</strong>。</p><h3 id="3-2-4-Exactly-Once语义"><a href="#3-2-4-Exactly-Once语义" class="headerlink" title="3.2.4 Exactly Once语义"></a>3.2.4 Exactly Once语义</h3><p>将服务器的ACK级别设置为-1，可以保证Producer到Server之间不会丢失数据，即At Least Once语义。相对的，将服务器ACK级别设置为0，可以保证生产者每条消息只会被发送一次，即At Most Once语义。</p><p><strong>At Least Once可以保证数据不丢失</strong>，但是不能保证数据不重复；相对的，<strong>At most Once可以保证数据不重复</strong>，但是不能保证数据不丢失。但是，<strong>对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即Exactly Once语义</strong>。在0.11版本以前的Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。</p><p>0.11版本的Kafka，引入了一项重大特性：<strong>幂等性</strong>。所谓的幂等性就是指Producer不论向Server发送多少次重复数据，Server端都只会持久化一条。 幂等性结合At Least Once语义，就构成了Kafka的Exactly Once语义。即：</p><p><strong>At Least Once + 幂等性 = Exactly Once</strong></p><p>要启用幂等性，只需要将Producer的参数中enable.idempotence设置为true即可。Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带Sequence Number。而Broker端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时，Broker只会持久化一条。</p><p>但是PID重启就会变化，同时不同的Partition也具有不同主键，所以幂等性无法保证跨分区跨会话的Exactly Once。</p><h2 id="3-3-Kafka消费者"><a href="#3-3-Kafka消费者" class="headerlink" title="3.3 Kafka消费者"></a>3.3 Kafka消费者</h2><h3 id="3-3-1-消费方式"><a href="#3-3-1-消费方式" class="headerlink" title="3.3.1 消费方式"></a>3.3.1 消费方式</h3><p><strong>consumer采用pull（拉）模式从broker中读取数据。</strong></p><p><strong>push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。</strong>它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。</p><p><strong>pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。</strong>针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout。</p><h3 id="3-3-3-分区分配策略"><a href="#3-3-3-分区分配策略" class="headerlink" title="3.3.3 分区分配策略"></a>3.3.3 分区分配策略</h3><p>一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。</p><p>Kafka有两种分配策略，一是round-robin，一是range。</p><h4 id="1）round-robin"><a href="#1）round-robin" class="headerlink" title="1）round-robin"></a><strong>1）round-robin</strong></h4><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142527.png"></p><h4 id="2）range"><a href="#2）range" class="headerlink" title="2）range"></a><strong>2）range</strong></h4><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142529.png"></p><h3 id="3-3-4-offset的维护"><a href="#3-3-4-offset的维护" class="headerlink" title="3.3.4 offset的维护"></a>3.3.4 offset的维护</h3><p>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。</p><p><strong>Kafka 0.9版本之前，consumer默认将offset保存在Zookeeper中，从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为__consumer_offsets。</strong></p><h2 id="3-4-Kafka-高效读写数据"><a href="#3-4-Kafka-高效读写数据" class="headerlink" title="3.4 Kafka 高效读写数据"></a>3.4 Kafka 高效读写数据</h2><h3 id="1）顺序写磁盘"><a href="#1）顺序写磁盘" class="headerlink" title="1）顺序写磁盘"></a><strong>1）顺序写磁盘</strong></h3><p>Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到到600M/s，而随机写只有100k/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其<strong>省去了大量磁头寻址的时间</strong>。</p><h3 id="2）应用Pagecache"><a href="#2）应用Pagecache" class="headerlink" title="2）应用Pagecache"></a><strong>2）应用Pagecache</strong></h3><p>Kafka数据持久化是直接持久化到Pagecache中，这样会产生以下几个好处：</p><ul><li><p>I/O Scheduler 会将连续的小块写组装成大块的物理写从而提高性能</p></li><li><p>I/O Scheduler 会尝试将一些写操作重新按顺序排好，从而减少磁盘头的移动时间</p></li><li><p>充分利用所有空闲内存（非 JVM 内存）。如果使用应用层 Cache（即 JVM 堆内存），会增加 GC 负担</p></li><li><p>读操作可直接在 Page Cache 内进行。如果消费和生产速度相当，甚至不需要通过物理磁盘（直接通过 Page Cache）交换数据</p></li><li><p>如果进程重启，JVM 内的 Cache 会失效，但 Page Cache 仍然可用</p></li></ul><p>尽管持久化到Pagecache上可能会造成宕机丢失数据的情况，但这可以被Kafka的Replication机制解决。如果为了保证这种情况下数据不丢失而强制将 Page Cache 中的数据 Flush 到磁盘，反而会降低性能。</p><h3 id="3）零复制技术"><a href="#3）零复制技术" class="headerlink" title="3）零复制技术"></a><strong>3）零复制技术</strong></h3><p>  <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142530.png"></p><h2 id="3-5-Zk在Kafka中的作用"><a href="#3-5-Zk在Kafka中的作用" class="headerlink" title="3.5 Zk在Kafka中的作用"></a>3.5 Zk在Kafka中的作用</h2><p>Kafka集群中有一个broker会被选举为Controller，负责<strong>管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作</strong>。</p><p>Controller的管理工作都是依赖于Zookeeper的。</p><p>​    以下为partition的leader选举过程：</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142531.png"></p><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922134053.png" alt="image-20200922120657055"></p><h2 id="3-6-Kafka事务"><a href="#3-6-Kafka事务" class="headerlink" title="3.6 Kafka事务"></a>3.6 Kafka事务</h2><p>​    Kafka从0.11版本开始引入了事务支持。事务可以保证Kafka在Exactly Once语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。</p><h3 id="3-6-1-Producer事务"><a href="#3-6-1-Producer事务" class="headerlink" title="3.6.1 Producer事务"></a>3.6.1 Producer事务</h3><p>​    为了实现跨分区跨会话的事务，需要引入一个全局唯一的Transaction ID，并将Producer获得的PID和Transaction ID绑定。这样当Producer重启后就可以通过正在进行的Transaction ID获得原来的PID。</p><p>​    为了管理Transaction，Kafka引入了一个新的组件Transaction Coordinator。Producer就是通过和Transaction Coordinator交互获得Transaction ID对应的任务状态。Transaction Coordinator还负责将事务所有写入Kafka的一个内部Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。</p><h3 id="3-6-2-Consumer事务"><a href="#3-6-2-Consumer事务" class="headerlink" title="3.6.2 Consumer事务"></a>3.6.2 Consumer事务</h3><p>​    上述事务机制主要是从Producer方面考虑，对于Consumer而言，事务的保证就会相对较弱，尤其时无法保证Commit的信息被精确消费。这是由于Consumer可以通过offset访问任意信息，而且不同的Segment File生命周期不同，同一事务的消息可能会出现重启后被删除的情况。</p><h1 id="第4章-Kafka-API"><a href="#第4章-Kafka-API" class="headerlink" title="第4章 Kafka API"></a>第4章 Kafka API</h1><h2 id="4-1-Producer-API"><a href="#4-1-Producer-API" class="headerlink" title="4.1 Producer API"></a>4.1 Producer API</h2><h3 id="4-1-1-消息发送流程"><a href="#4-1-1-消息发送流程" class="headerlink" title="4.1.1 消息发送流程"></a>4.1.1 消息发送流程</h3><p>Kafka的Producer发送消息采用的是<strong>异步发送</strong>的方式。在消息发送的过程中，涉及到了<strong>两个线程——main线程和Sender线程</strong>，以及<strong>一个线程共享变量——RecordAccumulator</strong>。main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker。</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142532.png"></p><p><strong>相关参数：</strong></p><p><strong>batch.size</strong>：只有数据积累到batch.size之后，sender才会发送数据。</p><p><strong>linger.ms</strong>：如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。</p><h3 id="4-1-1-异步发送API"><a href="#4-1-1-异步发送API" class="headerlink" title="4.1.1 异步发送API"></a>4.1.1 异步发送API</h3><p><strong>1）导入依赖</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.11.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>2）编写代码</strong></p><p>需要用到的类：</p><p><strong>KafkaProducer</strong>：需要创建一个生产者对象，用来发送数据</p><p><strong>ProducerConfig</strong>：获取所需的一系列配置参数</p><p><strong>ProducerRecord</strong>：每条数据都要封装成一个ProducerRecord对象</p><p><strong>1.不带回调函数的API</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);<span class="comment">//kafka集群，broker-list</span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">1</span>);<span class="comment">//重试次数</span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);<span class="comment">//批次大小</span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);<span class="comment">//等待时间</span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);<span class="comment">//RecordAccumulator缓冲区大小</span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;first&quot;</span>, Integer.toString(i), Integer.toString(i)));</span><br><span class="line">        &#125;</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>2.带回调函数的API</strong></p><p>回调函数会在producer收到ack时调用，为异步调用，该方法有两个参数，分别是RecordMetadata和Exception，如果Exception为null，说明消息发送成功，如果Exception不为null，说明消息发送失败。</p><p><strong>注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);<span class="comment">//kafka集群，broker-list</span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">1</span>);<span class="comment">//重试次数</span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);<span class="comment">//批次大小</span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);<span class="comment">//等待时间</span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);<span class="comment">//RecordAccumulator缓冲区大小</span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;first&quot;</span>, Integer.toString(i), Integer.toString(i)), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line"></span><br><span class="line">                <span class="comment">//回调函数，该方法会在Producer收到ack时调用，为异步调用</span></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">if</span> (exception ** <span class="keyword">null</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;success-&gt;&quot;</span> + metadata.offset());</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        exception.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="4-1-2-同步发送API"><a href="#4-1-2-同步发送API" class="headerlink" title="4.1.2 同步发送API"></a>4.1.2 同步发送API</h3><p>​    同步发送的意思就是，一条消息发送之后，会阻塞当前线程，直至返回ack。</p><p>由于send方法返回的是一个Future对象，根据Futrue对象的特点，我们也可以实现同步发送的效果，只需在调用Future对象的get方发即可。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Producer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);<span class="comment">//kafka集群，broker-list</span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">1</span>);<span class="comment">//重试次数</span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);<span class="comment">//批次大小</span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);<span class="comment">//等待时间</span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);<span class="comment">//RecordAccumulator缓冲区大小</span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;first&quot;</span>, Integer.toString(i), Integer.toString(i))).get();</span><br><span class="line">        &#125;</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="4-2-Consumer-API"><a href="#4-2-Consumer-API" class="headerlink" title="4.2 Consumer API"></a>4.2 Consumer API</h2><p>Consumer消费数据时的可靠性是很容易保证的，因为数据在Kafka中是持久化的，故不用担心数据丢失问题。</p><p>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。</p><p>所以offset的维护是Consumer消费数据是必须考虑的问题。</p><h3 id="4-2-1-自动提交offset"><a href="#4-2-1-自动提交offset" class="headerlink" title="4.2.1 自动提交offset"></a>4.2.1 自动提交offset</h3><p><strong>1）导入依赖</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.11.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>2）编写代码</strong></p><p>需要用到的类：</p><p><strong>KafkaConsumer</strong>：需要创建一个消费者对象，用来消费数据</p><p><strong>ConsumerConfig</strong>：获取所需的一系列配置参数</p><p><strong>ConsuemrRecord</strong>：每条数据都要封装成一个ConsumerRecord对象</p><p>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。 </p><p>自动提交offset的相关参数：</p><p><strong>enable.auto.commit</strong>：是否开启自动提交offset功能</p><p><strong>auto.commit.interval.ms</strong>：自动提交offset的时间间隔</p><p>以下为自动提交offset的代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>); <span class="comment">// 是否开启自动提交offset功能</span></span><br><span class="line">        props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>); <span class="comment">// 自动提交offset的时间间隔</span></span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records)</span><br><span class="line">                System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="4-2-2-手动提交offset"><a href="#4-2-2-手动提交offset" class="headerlink" title="4.2.2 手动提交offset"></a>4.2.2 手动提交offset</h3><p>虽然自动提交offset十分简介便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因此Kafka还提供了手动提交offset的API。</p><p>手动提交offset的方法有两种：分别是<strong>commitSync（同步提交）</strong>和<strong>commitAsync（异步提交）</strong>。两者的相同点是，都会将<strong>本次poll的一批数据最高的偏移量提交</strong>；不同点是，commitSync阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而commitAsync则没有失败重试机制，故有可能提交失败。</p><p><strong>1）同步提交offset</strong></p><p>由于同步提交offset有失败重试机制，故更加可靠，以下为同步提交offset的示例。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> liubo</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomComsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);<span class="comment">//Kafka集群</span></span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);<span class="comment">//消费者组，只要group.id相同，就属于同一个消费者组</span></span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);<span class="comment">//关闭自动提交offset</span></span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));<span class="comment">//消费者订阅主题</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);<span class="comment">//消费者拉取数据</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());</span><br><span class="line">            &#125;</span><br><span class="line">            consumer.commitSync();<span class="comment">//同步提交，当前线程会阻塞知道offset提交成功</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>2）异步提交offset</strong></p><p>虽然同步提交offset更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会收到很大的影响。因此更多的情况下，会选用异步提交offset的方式。</p><p>以下为异步提交offset的示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> liubo</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);<span class="comment">//Kafka集群</span></span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);<span class="comment">//消费者组，只要group.id相同，就属于同一个消费者组</span></span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);<span class="comment">//关闭自动提交offset</span></span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));<span class="comment">//消费者订阅主题</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);<span class="comment">//消费者拉取数据</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());</span><br><span class="line">            &#125;</span><br><span class="line">            consumer.commitAsync(<span class="keyword">new</span> OffsetCommitCallback() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">if</span> (exception != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        System.err.println(<span class="string">&quot;Commit failed for&quot;</span> + offsets);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);<span class="comment">//异步提交</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>3）</strong> <strong>数据漏消费和重复消费分析</strong></p><p>无论是同步提交还是异步提交offset，都有可能会造成数据的漏消费或者重复消费。<strong>先提交offset后消费，有可能造成数据的漏消费；而先消费后提交offset，有可能会造成数据的重复消费</strong>。</p><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142533.png" alt="image-20200922131135032"></p><h3 id="4-2-3-自定义存储offset"><a href="#4-2-3-自定义存储offset" class="headerlink" title="4.2.3 自定义存储offset"></a>4.2.3 自定义存储offset</h3><p>Kafka 0.9版本之前，offset存储在zookeeper，0.9版本之后，默认将offset存储在Kafka的一个内置的topic中。除此之外，Kafka还可以选择自定义存储offset。</p><p>Offset的维护是相当繁琐的，因为需要考虑到消费者的Rebalance。</p><p><strong>当有新的消费者加入消费者组、已有的消费者退出消费者组或者所订阅的主题的分区发生变化，就会触发到分区的重新分配，重新分配的过程叫做Rebalance。</strong></p><p>消费者发生Rebalance之后，每个消费者消费的分区就会发生变化。<strong>因此消费者要首先获取到自己被重新分配到的分区，并且定位到每个分区最近提交的offset位置继续消费</strong></p><p>要实现自定义存储offset，需要借助<strong>ConsumerRebalanceListener</strong>，以下为示例代码，其中提交和获取offset的方法，需要根据所选的offset存储系统自行实现。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> liubo</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Map&lt;TopicPartition, Long&gt; currentOffset = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);<span class="comment">//Kafka集群</span></span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);<span class="comment">//消费者组，只要group.id相同，就属于同一个消费者组</span></span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);<span class="comment">//关闭自动提交offset</span></span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>), <span class="keyword">new</span> ConsumerRebalanceListener() &#123;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">//该方法会在Rebalance之前调用</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">                commitOffset(currentOffset);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//该方法会在Rebalance之后调用</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">                currentOffset.clear();</span><br><span class="line">                <span class="keyword">for</span> (TopicPartition partition : partitions) &#123;</span><br><span class="line">                    consumer.seek(partition, getOffset(partition));<span class="comment">//定位到最近提交的offset位置继续消费</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);<span class="comment">//消费者拉取数据</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());</span><br><span class="line">                currentOffset.put(<span class="keyword">new</span> TopicPartition(record.topic(), record.partition()), record.offset());</span><br><span class="line">            &#125;</span><br><span class="line">            commitOffset(currentOffset); </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取某分区的最新offset</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> <span class="title">getOffset</span><span class="params">(TopicPartition partition)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//提交该消费者所有分区的offset</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">commitOffset</span><span class="params">(Map&lt;TopicPartition, Long&gt; currentOffset)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="4-3-自定义Interceptor"><a href="#4-3-自定义Interceptor" class="headerlink" title="4.3 自定义Interceptor"></a>4.3 自定义Interceptor</h2><h3 id="4-3-1-拦截器原理"><a href="#4-3-1-拦截器原理" class="headerlink" title="4.3.1 拦截器原理"></a>4.3.1 拦截器原理</h3><p>Producer拦截器(interceptor)是在Kafka 0.10版本被引入的，主要用于实现clients端的定制化控制逻辑。</p><p>对于producer而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如<strong>修改消息</strong>等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。Intercetpor的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor，其定义的方法包括：</p><p>（1）configure(configs)</p><p>获取配置信息和初始化数据时调用。</p><p>（2）onSend(ProducerRecord)：</p><p>该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。<strong>用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，</strong>否则会影响目标分区的计算。</p><p>（3）onAcknowledgement(RecordMetadata, Exception)：</p><p><strong>该方法会在消息从RecordAccumulator成功发送到Kafka Broker之后，或者在发送过程中失败时调用</strong>。并且通常都是在producer回调逻辑触发之前。onAcknowledgement运行在producer的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率。</p><p>（4）close：</p><p><strong>关闭interceptor，主要用于执行一些资源清理工作</strong></p><p>如前所述，interceptor可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外<strong>倘若指定了多个interceptor，则producer将按照指定顺序调用它们</strong>，并仅仅是捕获每个interceptor可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。</p><h3 id="4-3-2-拦截器案例"><a href="#4-3-2-拦截器案例" class="headerlink" title="4.3.2 拦截器案例"></a>4.3.2 拦截器案例</h3><p>1）需求：</p><p>实现一个简单的双interceptor组成的拦截链。第一个interceptor会在消息发送前将时间戳信息加到消息value的最前部；第二个interceptor会在消息发送后更新成功发送消息数或失败发送消息数。</p><p>  <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142534.png"></p><p>2）案例实操</p><p>（1）增加时间戳拦截器</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing.kafka.interceptor;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerInterceptor;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeInterceptor</span> <span class="keyword">implements</span> <span class="title">ProducerInterceptor</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ProducerRecord&lt;String, String&gt; <span class="title">onSend</span><span class="params">(ProducerRecord&lt;String, String&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 创建一个新的record，把时间戳写入消息体的最前部</span></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> ProducerRecord(record.topic(), record.partition(), record.timestamp(), record.key(),</span><br><span class="line">System.currentTimeMillis() + <span class="string">&quot;,&quot;</span> + record.value().toString());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>（2）统计发送消息成功和发送失败消息数，并在producer关闭时打印这两个计数器</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing.kafka.interceptor;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerInterceptor;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CounterInterceptor</span> <span class="keyword">implements</span> <span class="title">ProducerInterceptor</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt;</span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> errorCounter = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> successCounter = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ProducerRecord&lt;String, String&gt; <span class="title">onSend</span><span class="params">(ProducerRecord&lt;String, String&gt; record)</span> </span>&#123;</span><br><span class="line"> <span class="keyword">return</span> record;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 统计成功和失败的次数</span></span><br><span class="line">        <span class="keyword">if</span> (exception ** <span class="keyword">null</span>) &#123;</span><br><span class="line">            successCounter++;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            errorCounter++;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 保存结果</span></span><br><span class="line">        System.out.println(<span class="string">&quot;Successful sent: &quot;</span> + successCounter);</span><br><span class="line">        System.out.println(<span class="string">&quot;Failed sent: &quot;</span> + errorCounter);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>（3）producer主程序</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xing.kafka.interceptor;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Producer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InterceptorProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 1 设置配置信息</span></span><br><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">0</span>);</span><br><span class="line">props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);</span><br><span class="line">props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);</span><br><span class="line">props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);</span><br><span class="line">props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 构建拦截链</span></span><br><span class="line">List&lt;String&gt; interceptors = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">interceptors.add(<span class="string">&quot;com.xing.kafka.interceptor.TimeInterceptor&quot;</span>); interceptors.add(<span class="string">&quot;com.xing.kafka.interceptor.CounterInterceptor&quot;</span>); </span><br><span class="line">props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);</span><br><span class="line"> </span><br><span class="line">String topic = <span class="string">&quot;first&quot;</span>;</span><br><span class="line">Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 发送消息</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line"></span><br><span class="line">    ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(topic, <span class="string">&quot;message&quot;</span> + i);</span><br><span class="line">    producer.send(record);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 4 一定要关闭producer，这样才会调用interceptor的close方法</span></span><br><span class="line">producer.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>3）测试</p><p>（1）在kafka上启动消费者，然后运行客户端java程序。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[xing@hadoop102 kafka]$ bin/kafka-console-consumer.sh \</span><br><span class="line"></span><br><span class="line">--bootstrap-server hadoop102:9092 --from-beginning --topic first</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">1501904047034,message0</span><br><span class="line"></span><br><span class="line">1501904047225,message1</span><br><span class="line"></span><br><span class="line">1501904047230,message2</span><br><span class="line"></span><br><span class="line">1501904047234,message3</span><br><span class="line"></span><br><span class="line">1501904047236,message4</span><br><span class="line"></span><br><span class="line">1501904047240,message5</span><br><span class="line"></span><br><span class="line">1501904047243,message6</span><br><span class="line"></span><br><span class="line">1501904047246,message7</span><br><span class="line"></span><br><span class="line">1501904047249,message8</span><br><span class="line"></span><br><span class="line">1501904047252,message9</span><br></pre></td></tr></table></figure><h1 id="第5章-Flume对接Kafka"><a href="#第5章-Flume对接Kafka" class="headerlink" title="第5章 Flume对接Kafka"></a>第5章 Flume对接Kafka</h1><p><strong>1）配置flume(flume-kafka.conf)</strong></p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define</span></span><br><span class="line"><span class="meta">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="meta">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># source</span></span><br><span class="line"><span class="meta">a1.sources.r1.type</span> = <span class="string">exec</span></span><br><span class="line"><span class="meta">a1.sources.r1.command</span> = <span class="string">tail -F -c +0 /opt/module/datas/flume.log</span></span><br><span class="line"><span class="meta">a1.sources.r1.shell</span> = <span class="string">/bin/bash -c</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sink</span></span><br><span class="line"><span class="meta">a1.sinks.k1.type</span> = <span class="string">org.apache.flume.sink.kafka.KafkaSink</span></span><br><span class="line"><span class="meta">a1.sinks.k1.kafka.bootstrap.servers</span> = <span class="string">hadoop102:9092,hadoop103:9092,hadoop104:9092</span></span><br><span class="line"><span class="meta">a1.sinks.k1.kafka.topic</span> = <span class="string">first</span></span><br><span class="line"><span class="meta">a1.sinks.k1.kafka.flumeBatchSize</span> = <span class="string">20</span></span><br><span class="line"><span class="meta">a1.sinks.k1.kafka.producer.acks</span> = <span class="string">1</span></span><br><span class="line"><span class="meta">a1.sinks.k1.kafka.producer.linger.ms</span> = <span class="string">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># channel</span></span><br><span class="line"><span class="meta">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># bind</span></span><br><span class="line"><span class="meta">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>2） 启动kafkaIDEA消费者</strong></p><p><strong>3） 进入flume根目录下，启动flume</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/flume-ng agent -c conf/ -n a1 -f <span class="built_in">jobs</span>/flume-kafka.conf</span><br></pre></td></tr></table></figure><p><strong>4）向 /opt/module/datas/flume.log里追加数据，查看kafka消费者消费情况</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> hello &gt;&gt; /opt/module/datas/flume.log</span><br></pre></td></tr></table></figure><h1 id="第6章-Kafka监控"><a href="#第6章-Kafka监控" class="headerlink" title="第6章 Kafka监控"></a>第6章 Kafka监控</h1><h2 id="6-1-Kafka-Monitor"><a href="#6-1-Kafka-Monitor" class="headerlink" title="6.1 Kafka Monitor"></a>6.1 Kafka Monitor</h2><p>1.上传jar包KafkaOffsetMonitor-assembly-0.4.6.jar到集群</p><p>2.在/opt/module/下创建kafka-offset-console文件夹</p><p>3.将上传的jar包放入刚创建的目录下</p><p>4.在/opt/module/kafka-offset-console目录下创建启动脚本start.sh，内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">java -cp KafkaOffsetMonitor-assembly-0.4.6-SNAPSHOT.jar \</span><br><span class="line">com.quantifind.kafka.offsetapp.OffsetGetterWeb \</span><br><span class="line">--offsetStorage kafka \</span><br><span class="line">--kafkaBrokers hadoop102:9092,hadoop103:9092,hadoop104:9092 \</span><br><span class="line">--kafkaSecurityProtocol PLAINTEXT \</span><br><span class="line">--zk hadoop102:2181,hadoop103:2181,hadoop104:2181 \</span><br><span class="line">--port 8086 \</span><br><span class="line">--refresh 10.seconds \</span><br><span class="line">--retain 2.days \</span><br><span class="line">--dbName offsetapp_kafka &amp;</span><br></pre></td></tr></table></figure><p>5.在/opt/module/kafka-offset-console目录下创建mobile-logs文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /opt/module/kafka-offset-console/mobile-logs</span><br></pre></td></tr></table></figure><p>6.启动KakaMonitor</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start.sh</span><br></pre></td></tr></table></figure><p>7.登录页面hadoop102:8086端口查看详情</p><h2 id="6-2-Kafka-Manager"><a href="#6-2-Kafka-Manager" class="headerlink" title="6.2 Kafka Manager"></a>6.2 Kafka Manager</h2><p>1.上传压缩包kafka-manager-1.3.3.15.zip到集群</p><p>2.解压到/opt/module</p><p>3.修改配置文件conf/application.conf</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">kafka-manager.zkhosts</span>=<span class="string">&quot;kafka-manager-zookeeper:2181&quot;</span></span><br><span class="line"><span class="attr">修改为：</span></span><br><span class="line"><span class="meta">kafka-manager.zkhosts</span>=<span class="string">&quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;</span></span><br></pre></td></tr></table></figure><p>4.启动kafka-manager</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-manager</span><br></pre></td></tr></table></figure><p>5.登录hadoop102:9000页面查看详细信息</p><h1 id="第7章-Kafka面试题"><a href="#第7章-Kafka面试题" class="headerlink" title="第7章 Kafka面试题"></a>第7章 Kafka面试题</h1><h2 id="7-1-面试问题"><a href="#7-1-面试问题" class="headerlink" title="7.1 面试问题"></a>7.1 面试问题</h2><p>1.Kafka中的ISR、AR代表什么？</p><p>2.Kafka中的HW、LEO等分别代表什么？</p><p>3.Kafka中是怎么体现消息顺序性的？</p><p>4.Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？</p><p>5.Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？</p><p>6.“消费者组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？</p><p>7.消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1？</p><p>8.有哪些情形会造成重复消费？</p><p>9.那些情景会造成消息漏消费？</p><p>10.当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？</p><p>  1）会在zookeeper中的/brokers/topics节点下创建一个新的topic节点，如：/brokers/topics/first</p><p>  2）触发Controller的监听程序</p><p>  3）kafka Controller 负责topic的创建工作，并更新metadata cache</p><p>11.topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？</p><p>12.topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？</p><p>13.Kafka有内部的topic吗？如果有是什么？有什么作用？</p><p>14.Kafka分区分配的概念？</p><p>15.简述Kafka的日志目录结构？</p><p>16.如果我指定了一个offset，Kafka Controller怎么查找到对应的消息？</p><p>17.聊一聊Kafka Controller的作用？</p><p>18.Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？</p><p>19.失效副本是指什么？有那些应对措施？</p><p>20.Kafka的那些设计让它有如此高的性能？</p><h2 id="7-2-参考答案"><a href="#7-2-参考答案" class="headerlink" title="7.2 参考答案"></a>7.2 参考答案</h2><h3 id="1-Kafka中的ISR、AR又代表什么？"><a href="#1-Kafka中的ISR、AR又代表什么？" class="headerlink" title="1.Kafka中的ISR、AR又代表什么？"></a>1.Kafka中的ISR、AR又代表什么？</h3><p>  ISR：与leader保持同步的follower集合</p><p>  AR：分区的所有副本</p><h3 id="2-Kafka中的HW、LEO等分别代表什么？"><a href="#2-Kafka中的HW、LEO等分别代表什么？" class="headerlink" title="2.Kafka中的HW、LEO等分别代表什么？"></a>2.Kafka中的HW、LEO等分别代表什么？</h3><p>  LEO：没个副本的最后条消息的offset</p><p>  HW：一个分区中所有副本最小的offset</p><h3 id="3-Kafka中是怎么体现消息顺序性的？"><a href="#3-Kafka中是怎么体现消息顺序性的？" class="headerlink" title="3.Kafka中是怎么体现消息顺序性的？"></a>3.Kafka中是怎么体现消息顺序性的？</h3><p>  每个分区内，每条消息都有一个offset，故只能保证分区内有序。</p><h3 id="4-Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？"><a href="#4-Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？" class="headerlink" title="4.Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？"></a>4.Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？</h3><p>  拦截器 -&gt; 序列化器 -&gt; 分区器</p><h3 id="5-Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？"><a href="#5-Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？" class="headerlink" title="5.Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？"></a>5.Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？</h3><p>​    <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142535.png"></p><h3 id="6-“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？"><a href="#6-“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？" class="headerlink" title="6.“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？"></a>6.“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？</h3><p>  正确</p><h3 id="7-消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset-1？"><a href="#7-消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset-1？" class="headerlink" title="7.消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1？"></a>7.消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1？</h3><p>  offset+1</p><h3 id="8-有哪些情形会造成重复消费？"><a href="#8-有哪些情形会造成重复消费？" class="headerlink" title="8.有哪些情形会造成重复消费？"></a>8.有哪些情形会造成重复消费？</h3><p>  <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142536.png"></p><h3 id="9-那些情景会造成消息漏消费？"><a href="#9-那些情景会造成消息漏消费？" class="headerlink" title="9.那些情景会造成消息漏消费？"></a>9.那些情景会造成消息漏消费？</h3><p>  先提交offset，后消费，有可能造成数据的重复</p><h3 id="10-当你使用kafka-topics-sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？"><a href="#10-当你使用kafka-topics-sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？" class="headerlink" title="10.当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？"></a>10.当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？</h3><p>  1）会在zookeeper中的/brokers/topics节点下创建一个新的topic节点，如：/brokers/topics/first</p><p>  2）触发Controller的监听程序</p><p>  3）kafka Controller 负责topic的创建工作，并更新metadata cache</p><h3 id="11-topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？"><a href="#11-topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？" class="headerlink" title="11.topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？"></a>11.topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？</h3><p>可以增加</p><blockquote><p>bin/kafka-topics.sh –zookeeper localhost:2181/kafka –alter –topic topic-config –partitions 3</p></blockquote><h3 id="12-topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？"><a href="#12-topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？" class="headerlink" title="12.topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？"></a>12.topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？</h3><p>  不可以减少，被删除的分区数据难以处理。</p><h3 id="13-Kafka有内部的topic吗？如果有是什么？有什么所用？"><a href="#13-Kafka有内部的topic吗？如果有是什么？有什么所用？" class="headerlink" title="13.Kafka有内部的topic吗？如果有是什么？有什么所用？"></a>13.Kafka有内部的topic吗？如果有是什么？有什么所用？</h3><p>  __consumer_offsets,保存消费者offset</p><h3 id="14-Kafka分区分配的概念？"><a href="#14-Kafka分区分配的概念？" class="headerlink" title="14.Kafka分区分配的概念？"></a>14.Kafka分区分配的概念？</h3><p>  一个topic多个分区，一个消费者组多个消费者，故需要将分区分配个消费者(roundrobin、range)</p><h3 id="15-简述Kafka的日志目录结构？"><a href="#15-简述Kafka的日志目录结构？" class="headerlink" title="15.简述Kafka的日志目录结构？"></a>15.简述Kafka的日志目录结构？</h3><p>  每个分区对应一个文件夹，文件夹的命名为topic-0，topic-1，内部为.log和.index文件</p><h3 id="16-如果我指定了一个offset，Kafka-Controller怎么查找到对应的消息？"><a href="#16-如果我指定了一个offset，Kafka-Controller怎么查找到对应的消息？" class="headerlink" title="16.如果我指定了一个offset，Kafka Controller怎么查找到对应的消息？"></a>16.如果我指定了一个offset，Kafka Controller怎么查找到对应的消息？</h3><p>   <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922142537.png"></p><h3 id="17-聊一聊Kafka-Controller的作用？"><a href="#17-聊一聊Kafka-Controller的作用？" class="headerlink" title="17.聊一聊Kafka Controller的作用？"></a>17.聊一聊Kafka Controller的作用？</h3><p>  负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。</p><h3 id="18-Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？"><a href="#18-Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？" class="headerlink" title="18.Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？"></a>18.Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？</h3><p>  partition leader（ISR），controller（先到先得）</p><h3 id="19-失效副本是指什么？有那些应对措施？"><a href="#19-失效副本是指什么？有那些应对措施？" class="headerlink" title="19.失效副本是指什么？有那些应对措施？"></a>19.失效副本是指什么？有那些应对措施？</h3><p>  不能及时与leader同步，暂时踢出ISR，等其追上leader之后再重新加入</p><h3 id="20-Kafka的那些设计让它有如此高的性能？"><a href="#20-Kafka的那些设计让它有如此高的性能？" class="headerlink" title="20.Kafka的那些设计让它有如此高的性能？"></a>20.Kafka的那些设计让它有如此高的性能？</h3><p>  分区，顺序写磁盘，0-copy</p><h1 id="8-JIRA"><a href="#8-JIRA" class="headerlink" title="8. JIRA"></a>8. JIRA</h1><ul><li><p><input disabled="" type="checkbox">  kafka工作流程个文件存取机制</p></li><li><p><input disabled="" type="checkbox">  round-robin</p></li><li><p><input disabled="" type="checkbox">  ack认证</p></li><li><p><input disabled="" type="checkbox">  in-sync replica set (ISR)</p></li><li><p><input disabled="" type="checkbox">  零复制技术</p></li><li><p><input disabled="" type="checkbox">  Exactly Once</p></li><li><p><input disabled="" type="checkbox">  故障处理细节,leader故障 HW LEO[</p></li><li><p><input disabled="" type="checkbox">  zk在kafka作用,leader选举过程</p></li></ul><h1 id="…"><a href="#…" class="headerlink" title="…."></a>….</h1>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;第1章-Kafka概述&quot;&gt;&lt;a href=&quot;#第1章-Kafka概述&quot; class=&quot;headerlink&quot; title=&quot;第1章 Kafka概述&quot;&gt;&lt;/a&gt;第1章 Kafka概述&lt;/h1&gt;&lt;h2 id=&quot;1-1定义&quot;&gt;&lt;a href=&quot;#1-1定义&quot; class=&quot;headerlink&quot; title=&quot;1.1定义&quot;&gt;&lt;/a&gt;1.1定义&lt;/h2&gt;&lt;p&gt;Kafka是一个分布式的基于发布/订阅模式的&lt;strong&gt;消息队列，&lt;/strong&gt;主要应用于大数据实时处理领域。&lt;/p&gt;
&lt;h2 id=&quot;1-2-消息队列（Message-Queue）&quot;&gt;&lt;a href=&quot;#1-2-消息队列（Message-Queue）&quot; class=&quot;headerlink&quot; title=&quot;1.2 消息队列（Message Queue）&quot;&gt;&lt;/a&gt;1.2 消息队列（Message Queue）&lt;/h2&gt;&lt;h3 id=&quot;1-2-1-传统消息队列应用场景&quot;&gt;&lt;a href=&quot;#1-2-1-传统消息队列应用场景&quot; class=&quot;headerlink&quot; title=&quot;1.2.1 传统消息队列应用场景&quot;&gt;&lt;/a&gt;&lt;strong&gt;1.2.1&lt;/strong&gt; &lt;strong&gt;传统消息队列应用场景&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://gitee.com/curryfor369/picgo/raw/master/img/20200922142517.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Kafka" scheme="http://iscurry.com/categories/Kafka/"/>
    
    
    <category term="Detail" scheme="http://iscurry.com/tags/Detail/"/>
    
    <category term="Kafka" scheme="http://iscurry.com/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop</title>
    <link href="http://iscurry.com/2019/11/25/hadoop/"/>
    <id>http://iscurry.com/2019/11/25/hadoop/</id>
    <published>2019-11-25T02:15:42.000Z</published>
    <updated>2020-09-25T02:19:08.457Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-hadoop简介"><a href="#1-hadoop简介" class="headerlink" title="1. hadoop简介"></a>1. hadoop简介</h1><h2 id="1、作用"><a href="#1、作用" class="headerlink" title="1、作用"></a>1、作用</h2><ul><li><p>主要解决海量数据的存储和分析计算</p><a id="more"></a></li></ul><h2 id="2、历史"><a href="#2、历史" class="headerlink" title="2、历史"></a>2、历史</h2><ul><li><p>2006.3 hadoop产生</p></li><li><p>源于google的三篇论文</p></li><li><p>GFS – HDFS</p></li><li><p>MAP-REDUCE – MAPREDUCE</p></li><li><p>BIGTABLE – HBASE</p></li></ul><h2 id="3、三大版本"><a href="#3、三大版本" class="headerlink" title="3、三大版本"></a>3、三大版本</h2><ul><li>apache 原始版本</li><li>cloudera  大型企业用</li><li>hortonworks 文档较好</li></ul><h2 id="4、4大优势"><a href="#4、4大优势" class="headerlink" title="4、4大优势"></a>4、4大优势</h2><ul><li>高可靠性：多个副本</li><li>高容错性：失败任务自动重新分配</li><li>高扩展性：可拓展节点</li><li>高效性：在mapreduce思想下，hadoop是并行工作的</li></ul><h2 id="5、hadoop-1-x和2-x区别【组成架构】"><a href="#5、hadoop-1-x和2-x区别【组成架构】" class="headerlink" title="5、hadoop 1.x和2.x区别【组成架构】"></a>5、hadoop 1.x和2.x区别【组成架构】</h2><ul><li>1.x<ul><li>mapreduce 计算 + 资源调度 </li><li>hdfs 存储</li><li>common 辅助工具</li></ul></li><li>2.x 【模块化，解耦】<ul><li>hdfs 存储</li><li>yarn 资源调度</li><li>mapreduce 计算</li><li>common 辅助工具</li></ul></li></ul><h2 id="6、hdfs-组成"><a href="#6、hdfs-组成" class="headerlink" title="6、hdfs 组成:"></a>6、hdfs 组成:</h2><ul><li>namenode: 存储元数据 </li><li>datanode： 存储块数据，checksum<ul><li>secondary namenode【2NN】：每隔一段时间获取hdfs元数据的快照，辅助namenode工作，如checkpoint</li></ul></li></ul><h2 id="7、yarn-组成"><a href="#7、yarn-组成" class="headerlink" title="7、yarn 组成"></a>7、yarn 组成</h2><ul><li>resourcemanager: 资源调度总闸</li><li>nodemanager：每台机器的资源管理</li><li>applicationMaster</li><li>container</li></ul><h2 id="8、mapreduce-组成"><a href="#8、mapreduce-组成" class="headerlink" title="8、mapreduce 组成"></a>8、mapreduce 组成</h2><ul><li>map：并行处理输入数据</li><li>reduce：数据汇总</li></ul><h2 id="9、大数据生态体系"><a href="#9、大数据生态体系" class="headerlink" title="9、大数据生态体系"></a>9、大数据生态体系</h2><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200909064243.png" alt="image-20200611215224250"></p><ol><li><p>Sqoop：Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库(MySql)间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。</p></li><li><p>Flume：Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。</p></li><li><p>Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性：<br>（1）通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。<br>（2）高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。<br>（3）支持通过Kafka服务器和消费机集群来分区消息。<br>（4）支持Hadoop并行数据加载。</p></li><li><p>Storm：Storm用于“连续计算”，对数据流做连续查询，在计算时就将结果以流的形式输出给用户。</p></li><li><p>Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。</p></li><li><p>Oozie：Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。</p></li><li><p>Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。</p></li><li><p>Hive：Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</p></li><li><p>R语言：R是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。</p></li><li><p>Mahout：Apache Mahout是个可扩展的机器学习和数据挖掘库。</p></li><li><p>ZooKeeper：Zookeeper是Google的Chubby一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。</p></li></ol><h1 id="2-hadoop环境搭建"><a href="#2-hadoop环境搭建" class="headerlink" title="2. hadoop环境搭建"></a>2. hadoop环境搭建</h1><h2 id="1-虚拟机环境准备"><a href="#1-虚拟机环境准备" class="headerlink" title="1. 虚拟机环境准备"></a>1. 虚拟机环境准备</h2><ol><li>克隆虚拟机<ol><li>主机名</li><li>静态Ip</li><li>物理ip</li><li>防火墙</li><li>网络服务</li><li>主机映射</li><li>ntp时间同步</li><li>crontab 时间同步任务</li><li>ssh免密登录</li><li>普通用户root权限</li><li>普通用户ssh免密登录</li><li>rsync/xsync同步脚本</li></ol></li></ol><h2 id="2-安装JDK"><a href="#2-安装JDK" class="headerlink" title="2. 安装JDK"></a>2. 安装JDK</h2><ol><li>解压</li><li>配置环境变量</li><li>java -version测试</li></ol><h2 id="3-安装hadoop"><a href="#3-安装hadoop" class="headerlink" title="3. 安装hadoop"></a>3. 安装hadoop</h2><ol><li>解压</li><li>配置环境变量</li><li>配置6个文件</li><li>分发hadoop和环境变量</li><li>格式化namenode</li><li>启动集群 start-dfs.sh start-yarn.sh</li><li>测试、web测试50070，8088</li></ol><h2 id="3-配置历史服务器"><a href="#3-配置历史服务器" class="headerlink" title="3. 配置历史服务器"></a>3. 配置历史服务器</h2><h2 id="4-配置日志聚合"><a href="#4-配置日志聚合" class="headerlink" title="4. 配置日志聚合"></a>4. 配置日志聚合</h2><h1 id="3-hadoop-编译源码"><a href="#3-hadoop-编译源码" class="headerlink" title="3. hadoop 编译源码"></a>3. hadoop 编译源码</h1><ol><li><p>意义：hadoop默认不支持的功能，想要支持，就要编译，实际上是将需要添加的功能的代码包含到 lib/native/, 编译后的hadoop包在/opt/hadoop-2.7.2-src/hadoop-dist/target 例如 想要支持LZO压缩，就要编译,hive中可以支持Snappy压缩</p></li><li><p>centos 保证网络畅通 ping <a href="http://www.baidu.com/">www.baidu.com</a> Ok</p></li><li><p>jar包准备</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">jar包准备(hadoop源码、JDK8、maven、ant 、protobuf)</span><br><span class="line">（1）hadoop-2.7.2-src.tar.gz</span><br><span class="line">（2）jdk-8u144-linux-x64.tar.gz</span><br><span class="line">（3）apache-ant-1.9.9-bin.tar.gz（build工具，打包用的）</span><br><span class="line">（4）apache-maven-3.0.5-bin.tar.gz</span><br><span class="line">（5）protobuf-2.5.0.tar.gz（序列化的框架）</span><br></pre></td></tr></table></figure></li><li><p>解压jdk，环境变量， java -version</p></li><li><p>解压maven，配置mirros,环境变量 , mvn - version</p></li><li><p>解压ant，环境变量， ant-version</p></li><li><p>安装 glibc-headers  和 g++  yum -y install ….</p></li><li><p>解压protobuf,进入configure/ ,</p><ul><li><p>执行 </p><ul><li>make</li><li>make check</li><li>make install</li><li>ldconfig</li></ul></li><li><p>配置环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#LD_LIBRARY_PATH</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$LD_LIBRARY_PATH</span></span><br></pre></td></tr></table></figure></li></ul></li></ol><ol start="8"><li><p>安装openssl库 </p><ul><li>yum install openssl-devel</li></ul></li><li><p>安装 ncurses-devel库</p></li></ol><ul><li>yum install ncurses-devel</li></ul><ol start="10"><li>解压hadoop进入主目录</li></ol><ul><li>mvn package -Pdist,native -DskipTests -Dtar</li></ul><ol start="11"><li>成功的64位hadoop包在==<strong>/opt/hadoop-2.7.2-src/hadoop-dist/target</strong>==下</li></ol><h1 id="4-hdfs概述"><a href="#4-hdfs概述" class="headerlink" title="4. hdfs概述"></a>4. hdfs概述</h1><h2 id="1-定义"><a href="#1-定义" class="headerlink" title="1. 定义"></a>1. 定义</h2><ul><li>hdfs是 Hadoop Distributed File System ， 分布式文件管理系统</li><li>适用于一次写入，多次读出的场景</li></ul><h2 id="2-优缺点"><a href="#2-优缺点" class="headerlink" title="2. 优缺点"></a>2. 优缺点</h2><ul><li>优点<ul><li>高容错： 自动保存多个副本，丢失自动回复</li><li>适合处理大量大数据：能处理 GB，TB，PB级别的数据</li><li>可以构建在廉价的机器上，通过多副本机制提高可靠性</li></ul></li><li>缺点<ul><li>不适合处理大量小文件</li><li>不适合低延时数据访问</li><li>不支持并发写入，一个文件只能同时一个写，随机修改，只支持追加</li></ul></li></ul><h2 id="3-架构"><a href="#3-架构" class="headerlink" title="3. 架构"></a>3. 架构</h2><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200909064244.png" alt="image-20200611225021273"></p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200909064245.png" alt="image-20200611225114473"></p><h2 id="4-hdfs块大小"><a href="#4-hdfs块大小" class="headerlink" title="4. hdfs块大小"></a>4. hdfs块大小</h2><ul><li>1.x 默认64MB</li><li>2.x默认 128MB</li><li>块大小取决于磁盘传输速率，一般的磁盘读写100MB/s，磁盘性能好的可以设置256MB</li><li>块调大，有利于寻址，增加查询效率，寻址时间是读写事件的1%时，效率最高</li><li>块设太小，增加寻址时间，程序一直在寻找块的开始位置</li><li>块设太大，磁盘传输这个Block时间明显高于查找时间，程序变慢</li></ul><h1 id="5-hdfs-shell"><a href="#5-hdfs-shell" class="headerlink" title="5. hdfs shell"></a>5. hdfs shell</h1><ul><li><p>执行命令有两种</p></li><li><p>hdfs dfs -ls /</p></li><li><p>hadoop fs -lsr /</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1. hadoop fs ...  ==  hdfs dfs ...</span><br><span class="line">2. -lsr 递归查看</span><br><span class="line">3. -mkdir -p  创建多层目录</span><br><span class="line">4. -moveFromLocal == -put上传</span><br><span class="line">5. -appendToFile 追加</span><br><span class="line">6. -copyToLocal == -get  下载</span><br><span class="line">7. -getmerge 合并一个文件夹下的所有文件并复制</span><br><span class="line">8. -du -h -s  只显示大小</span><br><span class="line">9. -setrep 2 /a.txt 设置文件副本数  set replication</span><br></pre></td></tr></table></figure></li></ul><h1 id="6-hdfs-api"><a href="#6-hdfs-api" class="headerlink" title="6. hdfs api"></a>6. hdfs api</h1><ol><li>加压hadoop</li><li>配置环境边浪</li><li>创建maven，导入依赖，配置loj4j</li><li>上传下载创建删除</li><li>查看文件详细信息</li><li>定位下载</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>system<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">systemPath</span>&gt;</span>$&#123;JAVA_HOME&#125;/lib/tools.jar<span class="tag">&lt;/<span class="name">systemPath</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.rootLogger</span>=<span class="string">INFO, stdout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br><span class="line"><span class="meta">log4j.appender.logfile</span>=<span class="string">org.apache.log4j.FileAppender</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.File</span>=<span class="string">target/spring.log</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br></pre></td></tr></table></figure><h1 id="7-hdfs读写数据流程"><a href="#7-hdfs读写数据流程" class="headerlink" title="7. hdfs读写数据流程"></a>7. hdfs读写数据流程</h1><blockquote><p>写</p><ol><li>实际上也是io操作 </li><li>client 通过 distribute FileSystem向 namenode请求上传文件</li><li>nameNode 判断父目录是否存在，不存在No such dir,若存在</li><li>namenode 判断文件是否存在，若存在，file already exists，若不存在</li><li>namenode 检查可用的datanode节点，选择 距离近，负载低的节点信息返回，例如dn1,2,3</li><li>client 通过fsfileoutputstream向 dn1 请求上传数据第一个block(64/128/256)，</li><li>dn1 收到请求后继续调用 dn2, dn2 – dn3 建立通道，并逐级应答客户端</li><li>client 开始往dn1上传第一个Block,以packet为单位上传，先从磁盘读取放到一个本地内存缓存，</li><li>dn1收到1个packet就像dn2发送,dn2–dn3，dn1每传一个packet会放入一个应答队列，</li><li>当一个Block上传完成，应答队列将数据序列化到磁盘 </li><li>第一个block上传完毕后，client继续上传第二个block,直到所有block上传完成</li><li>关闭Io及fs资源</li></ol></blockquote><blockquote><p>读</p><ol><li>client 向 namenode强求读数据</li><li>namenode 将元数据信息返回给客户端</li><li>client 通过FSInputstream 向指定datanode节点读第一block</li><li>如果这个节点数据完整，一次io读完，之后关闭io</li><li>如果节点数据找不着，向同机架副本读，机架坏了，读另一个机架的副本，</li><li>每个节点读完后io流关闭，读另一个时重新打开</li><li>所有block读完，client将其合并</li></ol></blockquote><h1 id="8-网络拓扑和机架感知"><a href="#8-网络拓扑和机架感知" class="headerlink" title="8. 网络拓扑和机架感知"></a>8. 网络拓扑和机架感知</h1><p>namenode对节点距离的计算：==网络拓扑==： 找共同祖先，举例总和</p><p>机房<br>    集群1 d1<br>        机架1 r1<br>            服务器1 n1                1. d1,r1,n1 到自身的距离是0<br>            服务器2    n2                2. d1.r1.n1 到 d1.r1.n3 距离为2 共同祖先是r1<br>            服务器3 n3                3. d1.r1.n2 到 d2.r2.n5 距离4，共同祖先是机房<br>    集群2 d2<br>        机架2 r2<br>            服务器1 n4<br>            服务器2 n5<br>            服务器3 n6<br>        机架3 r3<br>            服务器1 n4<br>            服务器2 n5<br>            服务器3 n6</p><hr><p>==机架感知==–副本节点选择<br>若3个副本<br>同一机架不同节点放一个<br>不同机架放一个</p><hr><h1 id="9-namenode-工作机制"><a href="#9-namenode-工作机制" class="headerlink" title="9. namenode 工作机制"></a>9. namenode 工作机制</h1><ol><li>namenode启动时加载镜像文件和编辑日志到内存，也就是Hdfs元数据信息</li><li>client 对数据增删改，对元数据更新，记录操作日志，更新滚动日志 edits_inprogress_001 edits_001</li><li>内存数据更新</li><li>一定条件，触发2NN的checkpoint,copy namenode 的 fsimage &amp; edits to 2NN,加载到内存合并生成新的镜像文件fsimage.chkpoint</li><li>2NN copy fsimage.chkpoint to namenode &amp; rename to faimage</li></ol><p>1个block的元数据信息在内存中占150字节（byte）,一般的服务器为128G，也有256G<br>namenode元数据 存入内存、、磁盘(fsimage)、、edits、、定期合并、、2NN<br>2NN checkpoint,对namenode的元数据信息进行整理，对fsimage镜像和edits编辑日志进行合并<br>    满足 定时时间 一般1小时<br>    或满足 edits数据100w条，就执行checkpoint<br>合并细节：<br>    1. 滚动正在写的edits_inprogress_001编辑日志文件，生成新的edits_inprogress_002文件,将001改名edits_001<br>    2. 将edits1和fsimage拷贝到2NN将数据加载到内存进行合并，生成新的镜像文件fsimage.chkpoint<br>    3. 将fsimage.chkpoint拷贝到namenode重命名为fsimage</p><h1 id="10-fsiamge和edits"><a href="#10-fsiamge和edits" class="headerlink" title="10. fsiamge和edits"></a>10. fsiamge和edits</h1><ol><li>namenode格式化后，生成 <ul><li>fsimage_00000</li><li>fsimage_00000.md5</li><li>seen_txid</li></ul></li><li>增删改时<ul><li>edits_inprogress_0001</li></ul></li><li>合并后<ul><li>fsimage-00000</li><li>edit_00001</li><li>edit_inprogress_002</li><li>seen – 002</li><li>2NN上也会存放一份</li></ul></li><li>查看fsimage和edits文件<ul><li>fsimage:   hdfs oiv -p XML -i 镜像文件 -o 转换后的路径</li><li>edits : hdfs oev -p XML -i edits文件 -o 转换后路径</li></ul></li></ol><h1 id="11-checkpoint"><a href="#11-checkpoint" class="headerlink" title="11. checkpoint"></a>11. checkpoint</h1><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200909064246.png" alt="image-20200612094603509"></p><h1 id="12-常见端口"><a href="#12-常见端口" class="headerlink" title="12. 常见端口"></a>12. 常见端口</h1><ol><li>50070  web-hdfs</li><li>8088 web-mr</li><li>19888 web-history-server</li><li>9000 client-hadoop-cluster</li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-hadoop简介&quot;&gt;&lt;a href=&quot;#1-hadoop简介&quot; class=&quot;headerlink&quot; title=&quot;1. hadoop简介&quot;&gt;&lt;/a&gt;1. hadoop简介&lt;/h1&gt;&lt;h2 id=&quot;1、作用&quot;&gt;&lt;a href=&quot;#1、作用&quot; class=&quot;headerlink&quot; title=&quot;1、作用&quot;&gt;&lt;/a&gt;1、作用&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;主要解决海量数据的存储和分析计算&lt;/p&gt;</summary>
    
    
    
    <category term="Hadoop" scheme="http://iscurry.com/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="http://iscurry.com/tags/Hadoop/"/>
    
    <category term="Detail" scheme="http://iscurry.com/tags/Detail/"/>
    
  </entry>
  
  <entry>
    <title>hexo博客搭建</title>
    <link href="http://iscurry.com/2019/05/29/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    <id>http://iscurry.com/2019/05/29/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/</id>
    <published>2019-05-29T07:03:48.000Z</published>
    <updated>2020-09-25T02:18:12.741Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、搭建步骤"><a href="#1、搭建步骤" class="headerlink" title="1、搭建步骤"></a>1、搭建步骤</h1><p>注：cmd为管理员模式 环境： win10+git+github+nodejs</p><ol><li><p>下载cmder完整版，配置cmder, 管理员bash，开机bash，中文字符集</p> <a id="more"></a></li><li><p>下载安装node.js</p></li><li><p>在nodejs下创建node_global、node_cache</p><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200910184220.png"></p></li><li><p>配置nodejs环境变量</p><ol><li>系统变量NODE_PATH——-D:\soft\nodejs\node_global\node_modules</li><li>系统变量PATH————D:\soft\nodejs\</li><li>用户Path—————将C:\Users\hua\AppData\Roaming\npm改为D:\soft\nodejs\node_global</li></ol></li><li><p>cmder查看node版本 node -v</p></li><li><p>cmder查看npm版本 npm -v</p></li><li><p>配置全局模块安装目录   npm config set prefix “D:\soft\nodejs\node_global”</p></li><li><p>配置缓存目录 npm config set cache “D:\soft\nodejs\node_cache”</p></li><li><p>验证配置成功 npm config list</p></li><li><p>由于npm镜像站点慢，安装淘宝站点 cnpm npm install -g cnpm –registry=<a href="https://registry.npm.taobao.org/">https://registry.npm.taobao.org</a></p></li><li><p>查看cnpm版本 cnpm -v</p></li><li><p>安装hexo客户端  cnpm install -g hexo-cli</p></li><li><p>查看hexo版本 hexo -v</p></li><li><p>切换淘宝源  <code>npm config set registry https://registry.npm.taobao.org info underscore</code></p><p>  <img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200411210650.png"></p><p> <img src="https://gitee.com/curryfor369/picgo/raw/master/img/image-20200411210851265.png" alt="image-20200411210851265"></p></li><li><p>进入一个目录创建一个新的文件夹 我的: /d/blog</p></li><li><p>cdmer切换到blog下，初始化hexo：  hexo init</p></li><li><p>我的报错install dependencies 找不到install类</p></li><li><p>进入blog里 cnpm install可以解决</p></li><li><p>启动hexo服务 hexo s，默认4000端口</p></li><li><p>ctrl+c停掉服务，进入blog目录</p></li><li><p>新建一篇文章  hexo n “我的第一篇文章”</p></li><li><p>进入 curryblog/source/_posts/  查看生成一个文件 “我的第一篇文章.md” vi进行编辑</p></li><li><p>退到blog/ 清理一下 hexo clean</p></li><li><p>在blog/重新构建一下 hexo g</p></li><li><p>启动服务进入localhost:4000,看到新增的一篇文章</p></li><li><p>新建一个github仓库 命名 githubUserName.github.io 我的是：gudeball.github.io</p></li><li><p>在blog目录下装一个git部署地插件 cnpm install –save hexo-deployer-git</p></li><li><p>进入blog/_config.yml 最下修改</p> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">type:</span> <span class="string">git</span></span><br><span class="line"><span class="attr">repo:</span> <span class="string">https://github.com/gudeball/gudeball.github.io.git</span> <span class="comment">#刚才创建的仓库地址</span></span><br><span class="line"><span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure></li><li><p>配置git全局信息</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git config --list <span class="comment"># 查看全部配置</span></span><br><span class="line">git config user.name<span class="comment"># 查看单个属性</span></span><br><span class="line">git config --global --edit <span class="comment"># 编辑配置</span></span><br><span class="line">git config --global user.name curryfor  <span class="comment"># 设置属性</span></span><br><span class="line">git config --global --<span class="built_in">unset</span> user.name   <span class="comment"># 删除属性</span></span><br><span class="line">git remote add origin https://github.com/curryfor369/curryfor369.github.io.git <span class="comment"># 推送地址</span></span><br></pre></td></tr></table></figure></li><li><p>部署到远端 hexo d</p></li><li><p>查看原来github的仓库多了一些东西</p></li><li><p>这时就能访问了，githubUsername.github.io进入博客主页 ，我的博客主页 gudeball.github.io</p></li><li><p>到此部署完成</p></li><li><p>hexo主题推荐： github.com/litten/hexo-theme-yilia</p></li><li><p>克隆一个主题 git clone <a href="https://github.com/litten/hexo-theme-yilia.git">https://github.com/litten/hexo-theme-yilia.git</a> themes/yilia</p></li><li><p>即进入blog/修改_config文件 找到 theme 默认是landscaps 修改为yilia</p></li><li><p>重新清理一下 hexo clean</p></li><li><p>重新生成一下 hexo g</p></li><li><p>本地查看 hexo s</p></li><li><p>推送 hexo d</p></li><li><p>查看博客 gudeball.github.io</p></li></ol><h1 id="2、简洁版步骤"><a href="#2、简洁版步骤" class="headerlink" title="2、简洁版步骤:"></a>2、简洁版步骤:</h1><ol><li>建一个空文件夹进入管理员cmd</li><li>初始化   hexo init</li><li>提示错误之后  cnpm install</li><li>安装git部署  cnpm install –save hexo-deployer-git</li><li>克隆主题  git clone <a href="https://github.com/litten/hexo-theme-yilia.git">https://github.com/litten/hexo-theme-yilia.git</a> themes/yilia</li><li>修改 _config.yml</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">title:</span> <span class="string">Curry&#x27;s</span> <span class="string">Blog</span></span><br><span class="line"><span class="attr">theme:</span> <span class="string">yilia</span></span><br><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">https://github.com/gudeball/gudeball.github.io.git</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br><span class="line"><span class="attr">jsonContent:</span></span><br><span class="line">  <span class="attr">meta:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">pages:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">posts:</span></span><br><span class="line">    <span class="attr">title:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">date:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">path:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">text:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">raw:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">content:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">slug:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">updated:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">comments:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">link:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">permalink:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">excerpt:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">categories:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">tags:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><ol start="7"><li>修改 yilia/_config.yml</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">root:</span> <span class="string">/</span></span><br><span class="line"></span><br><span class="line"><span class="attr">favicon:</span> <span class="string">https://gitee.com/curryfor369/picgo/raw/master/img/ball.ico</span> <span class="comment"># 图标</span></span><br><span class="line"></span><br><span class="line"><span class="attr">avatar:</span> <span class="string">https://gitee.com/curryfor369/picgo/raw/master/img/avatar.jpg</span> <span class="comment"># 头像</span></span><br><span class="line"></span><br><span class="line"><span class="attr">friends:</span> <span class="comment"># 友链</span></span><br><span class="line">  <span class="string">飞速直播:</span> <span class="string">https://www.fszb8.com/</span></span><br><span class="line">  <span class="string">微直播吧:</span> <span class="string">http://www.sjzfgw.cn/</span></span><br><span class="line">  <span class="string">山猫直播:</span> <span class="string">https://smzb.cn/room/19082</span></span><br><span class="line">  <span class="string">JRS直播:</span> <span class="string">https://www.dbwf88.com/</span></span><br><span class="line">  <span class="string">抓饭直播:</span> <span class="string">http://www.zhuafan.live/</span></span><br><span class="line">  <span class="string">黑土直播:</span> <span class="string">https://www.cnmysoft.com/</span></span><br><span class="line">  <span class="string">看球吧(录像):</span> <span class="string">http://www.kanqiuba.net/</span></span><br><span class="line">  <span class="string">直播吧(录像):</span> <span class="string">http://www.zhiboba.tv/</span></span><br><span class="line">  <span class="string">搜球吧(录像):</span> <span class="string">https://www.uusnba.com/</span></span><br><span class="line">  </span><br><span class="line"><span class="attr">aboutme:</span> <span class="string">email</span> <span class="string">nba@iscurry.com</span> <span class="comment"># 关于我</span></span><br></pre></td></tr></table></figure><ol start="8"><li><p>hexo clean</p></li><li><p>hexo g</p></li><li><p>hexo s</p></li><li><p>配置git全局属性</p></li><li><p>hexo d</p></li><li><p>模块丢失  在blog/ </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm init</span><br><span class="line">npm i hexo-generator-json-content --save </span><br></pre></td></tr></table></figure></li></ol><h1 id="3、实现文章目录"><a href="#3、实现文章目录" class="headerlink" title="3、实现文章目录"></a>3、实现文章目录</h1><h2 id="1、效果"><a href="#1、效果" class="headerlink" title="1、效果"></a>1、效果</h2><p><img src="d:\桌面\冉辰星总结\curryfor369Blog\source_posts\hexo博客搭建.assets\20200910184223-1600405507764.png"></p><h2 id="2、添加-CSS-样式"><a href="#2、添加-CSS-样式" class="headerlink" title="2、添加 CSS 样式"></a>2、添加 CSS 样式</h2><p>打开 <code>themes\yilia\source</code> 下的 <code>main.234bc0.css</code> 文件，直接在后面添加如下代码：  </p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 新添加的 */</span></span><br><span class="line"><span class="selector-id">#container</span> <span class="selector-class">.show-toc-btn</span>,<span class="selector-id">#container</span> <span class="selector-class">.toc-article</span>&#123;<span class="attribute">display</span>:block&#125;</span><br><span class="line"><span class="selector-class">.toc-article</span>&#123;<span class="attribute">z-index</span>:<span class="number">100</span>;<span class="attribute">background</span>:<span class="number">#fff</span>;<span class="attribute">border</span>:<span class="number">1px</span> solid <span class="number">#ccc</span>;<span class="attribute">max-width</span>:<span class="number">250px</span>;<span class="attribute">min-width</span>:<span class="number">150px</span>;<span class="attribute">max-height</span>:<span class="number">500px</span>;<span class="attribute">overflow-y</span>:auto;<span class="attribute">-webkit-box-shadow</span>:<span class="number">5px</span> <span class="number">5px</span> <span class="number">2px</span> <span class="number">#ccc</span>;<span class="attribute">box-shadow</span>:<span class="number">5px</span> <span class="number">5px</span> <span class="number">2px</span> <span class="number">#ccc</span>;<span class="attribute">font-size</span>:<span class="number">12px</span>;<span class="attribute">padding</span>:<span class="number">10px</span>;<span class="attribute">position</span>:fixed;<span class="attribute">right</span>:<span class="number">35px</span>;<span class="attribute">top</span>:<span class="number">129px</span>&#125;<span class="selector-class">.toc-article</span> <span class="selector-class">.toc-close</span>&#123;<span class="attribute">font-weight</span>:<span class="number">700</span>;<span class="attribute">font-size</span>:<span class="number">20px</span>;<span class="attribute">cursor</span>:pointer;<span class="attribute">float</span>:right;<span class="attribute">color</span>:<span class="number">#ccc</span>&#125;<span class="selector-class">.toc-article</span> <span class="selector-class">.toc-close</span><span class="selector-pseudo">:hover</span>&#123;<span class="attribute">color</span>:<span class="number">#000</span>&#125;<span class="selector-class">.toc-article</span> <span class="selector-class">.toc</span>&#123;<span class="attribute">font-size</span>:<span class="number">12px</span>;<span class="attribute">padding</span>:<span class="number">0</span>;<span class="attribute">line-height</span>:<span class="number">20px</span>&#125;<span class="selector-class">.toc-article</span> <span class="selector-class">.toc</span> <span class="selector-class">.toc-number</span>&#123;<span class="attribute">color</span>:<span class="number">#333</span>&#125;<span class="selector-class">.toc-article</span> <span class="selector-class">.toc</span> <span class="selector-class">.toc-text</span><span class="selector-pseudo">:hover</span>&#123;<span class="attribute">text-decoration</span>:underline;<span class="attribute">color</span>:<span class="number">#2a6496</span>&#125;<span class="selector-class">.toc-article</span> <span class="selector-tag">li</span>&#123;<span class="attribute">list-style-type</span>:none&#125;<span class="selector-class">.toc-article</span> <span class="selector-class">.toc-level-1</span>&#123;<span class="attribute">margin</span>:<span class="number">4px</span> <span class="number">0</span>&#125;<span class="selector-class">.toc-article</span> <span class="selector-class">.toc-child</span>&#123;&#125;<span class="keyword">@-moz-keyframes</span> cd-bounce-<span class="number">1</span>&#123;0%&#123;<span class="attribute">opacity</span>:<span class="number">0</span>;<span class="attribute">-o-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-webkit-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-moz-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-ms-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>)&#125;60%&#123;<span class="attribute">opacity</span>:<span class="number">1</span>;<span class="attribute">-o-transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>);<span class="attribute">-webkit-transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>);<span class="attribute">-moz-transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>);<span class="attribute">-ms-transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>);<span class="attribute">transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>)&#125;100%&#123;<span class="attribute">-o-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-webkit-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-moz-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-ms-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>)&#125;&#125;<span class="keyword">@-webkit-keyframes</span> cd-bounce-<span class="number">1</span>&#123;0%&#123;<span class="attribute">opacity</span>:<span class="number">0</span>;<span class="attribute">-o-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-webkit-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-moz-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-ms-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>)&#125;60%&#123;<span class="attribute">opacity</span>:<span class="number">1</span>;<span class="attribute">-o-transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>);<span class="attribute">-webkit-transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>);<span class="attribute">-moz-transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>);<span class="attribute">-ms-transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>);<span class="attribute">transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>)&#125;100%&#123;<span class="attribute">-o-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-webkit-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-moz-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-ms-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>)&#125;&#125;<span class="keyword">@-o-keyframes</span> cd-bounce-<span class="number">1</span>&#123;0%&#123;<span class="attribute">opacity</span>:<span class="number">0</span>;<span class="attribute">-o-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-webkit-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-moz-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-ms-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>)&#125;60%&#123;<span class="attribute">opacity</span>:<span class="number">1</span>;<span class="attribute">-o-transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>);<span class="attribute">-webkit-transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>);<span class="attribute">-moz-transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>);<span class="attribute">-ms-transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>);<span class="attribute">transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>)&#125;100%&#123;<span class="attribute">-o-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-webkit-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-moz-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-ms-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>)&#125;&#125;<span class="keyword">@keyframes</span> cd-bounce-<span class="number">1</span>&#123;0%&#123;<span class="attribute">opacity</span>:<span class="number">0</span>;<span class="attribute">-o-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-webkit-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-moz-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-ms-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>)&#125;60%&#123;<span class="attribute">opacity</span>:<span class="number">1</span>;<span class="attribute">-o-transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>);<span class="attribute">-webkit-transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>);<span class="attribute">-moz-transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>);<span class="attribute">-ms-transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>);<span class="attribute">transform</span>:<span class="built_in">scale</span>(<span class="number">1.01</span>)&#125;100%&#123;<span class="attribute">-o-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-webkit-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-moz-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">-ms-transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>);<span class="attribute">transform</span>:<span class="built_in">scale</span>(<span class="number">1</span>)&#125;&#125;<span class="selector-class">.show-toc-btn</span>&#123;<span class="attribute">display</span>:none;<span class="attribute">z-index</span>:<span class="number">10</span>;<span class="attribute">width</span>:<span class="number">30px</span>;<span class="attribute">min-height</span>:<span class="number">14px</span>;<span class="attribute">overflow</span>:hidden;<span class="attribute">padding</span>:<span class="number">4px</span> <span class="number">6px</span> <span class="number">8px</span> <span class="number">5px</span>;<span class="attribute">border</span>:<span class="number">1px</span> solid <span class="number">#ddd</span>;<span class="attribute">border-right</span>:none;<span class="attribute">position</span>:fixed;<span class="attribute">right</span>:<span class="number">40px</span>;<span class="attribute">text-align</span>:center;<span class="attribute">background-color</span>:<span class="number">#f9f9f9</span>&#125;<span class="selector-class">.show-toc-btn</span> <span class="selector-class">.btn-bg</span>&#123;<span class="attribute">margin-top</span>:<span class="number">2px</span>;<span class="attribute">display</span>:block;<span class="attribute">width</span>:<span class="number">16px</span>;<span class="attribute">height</span>:<span class="number">14px</span>;<span class="attribute">background</span>:<span class="built_in">url</span>(http://<span class="number">7</span>xtawy.com1.z0.glb.clouddn.com/show.png) no-repeat;<span class="attribute">-webkit-background-size</span>:<span class="number">100%</span>;<span class="attribute">-moz-background-size</span>:<span class="number">100%</span>;<span class="attribute">background-size</span>:<span class="number">100%</span>&#125;<span class="selector-class">.show-toc-btn</span> <span class="selector-class">.btn-text</span>&#123;<span class="attribute">color</span>:<span class="number">#999</span>;<span class="attribute">font-size</span>:<span class="number">12px</span>&#125;<span class="selector-class">.show-toc-btn</span><span class="selector-pseudo">:hover</span>&#123;<span class="attribute">cursor</span>:pointer&#125;<span class="selector-class">.show-toc-btn</span><span class="selector-pseudo">:hover</span> <span class="selector-class">.btn-bg</span>&#123;<span class="attribute">background-position</span>:<span class="number">0</span> -<span class="number">16px</span>&#125;<span class="selector-class">.show-toc-btn</span><span class="selector-pseudo">:hover</span> <span class="selector-class">.btn-text</span>&#123;<span class="attribute">font-size</span>:<span class="number">12px</span>;<span class="attribute">color</span>:<span class="number">#ea8010</span>&#125;</span><br><span class="line"><span class="selector-class">.toc-article</span> <span class="selector-tag">li</span> <span class="selector-tag">ol</span>, <span class="selector-class">.toc-article</span> <span class="selector-tag">li</span> <span class="selector-tag">ul</span> &#123;</span><br><span class="line">    <span class="attribute">margin-left</span>: <span class="number">30px</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.toc-article</span> <span class="selector-tag">ol</span>, <span class="selector-class">.toc-article</span> <span class="selector-tag">ul</span> &#123;</span><br><span class="line">    <span class="attribute">margin</span>: <span class="number">10px</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3、修改-article-ejs-文件"><a href="#3、修改-article-ejs-文件" class="headerlink" title="3、修改 article.ejs 文件"></a>3、修改 article.ejs 文件</h2><p>打开 <code>themes\yilia\layout\_partial</code> 文件夹下的 <code>article.ejs</code> 文件, 在 <code>&lt;/header&gt; &lt;% &#125; %&gt;</code> 下面加入如下内容（注意位置）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"> &lt;&#x2F;header&gt;</span><br><span class="line">    &lt;% &#125; %&gt;</span><br><span class="line">&lt;!-- 目录内容 --&gt;</span><br><span class="line">&lt;% if (!index &amp;&amp; post.toc)&#123; %&gt;</span><br><span class="line">    &lt;p class&#x3D;&quot;show-toc-btn&quot; id&#x3D;&quot;show-toc-btn&quot; onclick&#x3D;&quot;showToc();&quot; style&#x3D;&quot;display:none&quot;&gt;</span><br><span class="line">          &lt;span class&#x3D;&quot;btn-bg&quot;&gt;&lt;&#x2F;span&gt;</span><br><span class="line">          &lt;span class&#x3D;&quot;btn-text&quot;&gt;文章导航&lt;&#x2F;span&gt;</span><br><span class="line">          &lt;&#x2F;p&gt;</span><br><span class="line">    &lt;div id&#x3D;&quot;toc-article&quot; class&#x3D;&quot;toc-article&quot;&gt;</span><br><span class="line">        &lt;span id&#x3D;&quot;toc-close&quot; class&#x3D;&quot;toc-close&quot; title&#x3D;&quot;隐藏导航&quot; onclick&#x3D;&quot;showBtn();&quot;&gt;×&lt;&#x2F;span&gt;</span><br><span class="line">        &lt;strong class&#x3D;&quot;toc-title&quot;&gt;文章目录&lt;&#x2F;strong&gt;</span><br><span class="line">           &lt;%- toc(post.content) %&gt;</span><br><span class="line">         &lt;&#x2F;div&gt;</span><br><span class="line">   &lt;script type&#x3D;&quot;text&#x2F;javascript&quot;&gt;</span><br><span class="line">    function showToc()&#123;</span><br><span class="line">        var toc_article &#x3D; document.getElementById(&quot;toc-article&quot;);</span><br><span class="line">        var show_toc_btn &#x3D; document.getElementById(&quot;show-toc-btn&quot;);</span><br><span class="line">        toc_article.setAttribute(&quot;style&quot;,&quot;display:block&quot;);</span><br><span class="line">        show_toc_btn.setAttribute(&quot;style&quot;,&quot;display:none&quot;);</span><br><span class="line">        &#125;;</span><br><span class="line">    function showBtn()&#123;</span><br><span class="line">        var toc_article &#x3D; document.getElementById(&quot;toc-article&quot;);</span><br><span class="line">        var show_toc_btn &#x3D; document.getElementById(&quot;show-toc-btn&quot;);</span><br><span class="line">        toc_article.setAttribute(&quot;style&quot;,&quot;display:none&quot;);</span><br><span class="line">        show_toc_btn.setAttribute(&quot;style&quot;,&quot;display:block&quot;);</span><br><span class="line">        &#125;;</span><br><span class="line">   &lt;&#x2F;script&gt;</span><br><span class="line">      &lt;% &#125; %&gt;</span><br><span class="line">&lt;!-- 目录内容结束 --&gt;</span><br></pre></td></tr></table></figure><h2 id="4、在每篇文章开头加入：toc-true"><a href="#4、在每篇文章开头加入：toc-true" class="headerlink" title="4、在每篇文章开头加入：toc: true"></a>4、在每篇文章开头加入：<code>toc: true</code></h2><ul><li>在yilia\_config.yml 修改 toc: 1</li></ul><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200910184222.png" alt="image-20200904084332396"></p><h1 id="4、rss订阅"><a href="#4、rss订阅" class="headerlink" title="4、rss订阅"></a>4、rss订阅</h1><ol><li><p>安装插件 <code>npm install hexo-generator-feed</code></p></li><li><p>_config.yml </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Extensions</span></span><br><span class="line"><span class="attr">theme:</span> <span class="string">yilia</span></span><br><span class="line"><span class="attr">plugins:</span></span><br><span class="line">  <span class="string">hexo-generator-feed</span></span><br><span class="line"><span class="comment">#Feed Atom</span></span><br><span class="line"><span class="attr">feed:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">atom</span></span><br><span class="line">  <span class="attr">path:</span> <span class="string">atom.xml</span></span><br><span class="line">  <span class="attr">limit:</span> <span class="number">20</span> <span class="comment"># 最新的20篇文章</span></span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>yilia/_config.yml</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SubNav</span></span><br><span class="line"><span class="attr">subnav:</span></span><br><span class="line">  <span class="attr">rss:</span> <span class="string">&quot;atom.xml&quot;</span> </span><br></pre></td></tr></table></figure></li><li><p><code>hexo clean, hexo g</code>后会在<code>public</code>目录下生成<code>auto.xml</code>订阅文件，到浏览器进行访问，点击图标会跳转到<code>*/auto.xml</code>界面，显示最近的几篇文章的更新。</p></li></ol><h1 id="5、百度统计"><a href="#5、百度统计" class="headerlink" title="5、百度统计"></a>5、百度统计</h1><ol><li><p>百度统计进去注册，代码获取，新增网站，网站和首页都填curryfor369.github.io</p></li><li><p>复制代码到 /themes/yilia/layout/_partial/baidu-analytices.ejs</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span>&gt;</span></span><br><span class="line"><span class="javascript"><span class="keyword">var</span> _hmt = _hmt || [];</span></span><br><span class="line"><span class="javascript">(<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">  <span class="keyword">var</span> hm = <span class="built_in">document</span>.createElement(<span class="string">&quot;script&quot;</span>);</span></span><br><span class="line"><span class="javascript">  hm.src = <span class="string">&quot;https://hm.baidu.com/hm.js?这里是id&quot;</span>;</span></span><br><span class="line"><span class="javascript">  <span class="keyword">var</span> s = <span class="built_in">document</span>.getElementsByTagName(<span class="string">&quot;script&quot;</span>)[<span class="number">0</span>]; </span></span><br><span class="line">  s.parentNode.insertBefore(hm, s);</span><br><span class="line">&#125;)();</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>复制id到 /themes/yilia/_config.yml</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200910184221.png" alt="image-20200904084258994"></p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200910184224.png"></p></li></ol><h1 id="6、代码复制"><a href="#6、代码复制" class="headerlink" title="6、代码复制"></a>6、代码复制</h1><ol><li><p>创建yilia\source\jsclipboard_use.js，添加代码</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">$(<span class="string">&quot;.highlight&quot;</span>).wrap(<span class="string">&quot;&lt;div class=&#x27;code-wrapper&#x27; style=&#x27;position:relative&#x27;&gt;&lt;/div&gt;&quot;</span>);</span><br><span class="line"><span class="comment">/*页面载入完成后，创建复制按钮*/</span></span><br><span class="line">!<span class="function"><span class="keyword">function</span> (<span class="params">e, t, a</span>) </span>&#123;</span><br><span class="line">    <span class="comment">/* code */</span></span><br><span class="line">    <span class="keyword">var</span> initCopyCode = <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">        <span class="keyword">var</span> copyHtml = <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">        copyHtml += <span class="string">&#x27;&lt;button class=&quot;btn-copy&quot; data-clipboard-snippet=&quot;&quot;&gt;&#x27;</span>;</span><br><span class="line">        copyHtml += <span class="string">&#x27;  &lt;i class=&quot;fa fa-clipboard&quot;&gt;&lt;/i&gt;&lt;span&gt;复制&lt;/span&gt;&#x27;</span>;</span><br><span class="line">        copyHtml += <span class="string">&#x27;&lt;/button&gt;&#x27;</span>;</span><br><span class="line">        $(<span class="string">&quot;.highlight .code&quot;</span>).before(copyHtml);</span><br><span class="line">        <span class="keyword">var</span> clipboard = <span class="keyword">new</span> ClipboardJS(<span class="string">&#x27;.btn-copy&#x27;</span>, &#123;</span><br><span class="line">            target: <span class="function"><span class="keyword">function</span> (<span class="params">trigger</span>) </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> trigger.nextElementSibling;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        clipboard.on(<span class="string">&#x27;success&#x27;</span>, <span class="function"><span class="keyword">function</span> (<span class="params">e</span>) </span>&#123;</span><br><span class="line">            e.trigger.innerHTML = <span class="string">&quot;&lt;i class=&#x27;fa fa-clipboard&#x27;&gt;&lt;/i&gt;&lt;span&gt;复制成功&lt;/span&gt;&quot;</span></span><br><span class="line">            <span class="built_in">setTimeout</span>(<span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">                e.trigger.innerHTML = <span class="string">&quot;&lt;i class=&#x27;fa fa-clipboard&#x27;&gt;&lt;/i&gt;&lt;span&gt;复制&lt;/span&gt;&quot;</span></span><br><span class="line">            &#125;, <span class="number">1000</span>)</span><br><span class="line">           </span><br><span class="line">            e.clearSelection();</span><br><span class="line">        &#125;);</span><br><span class="line">        clipboard.on(<span class="string">&#x27;error&#x27;</span>, <span class="function"><span class="keyword">function</span> (<span class="params">e</span>) </span>&#123;</span><br><span class="line">            e.trigger.innerHTML = <span class="string">&quot;&lt;i class=&#x27;fa fa-clipboard&#x27;&gt;&lt;/i&gt;&lt;span&gt;复制失败&lt;/span&gt;&quot;</span></span><br><span class="line">            <span class="built_in">setTimeout</span>(<span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">                e.trigger.innerHTML = <span class="string">&quot;&lt;i class=&#x27;fa fa-clipboard&#x27;&gt;&lt;/i&gt;&lt;span&gt;复制&lt;/span&gt;&quot;</span></span><br><span class="line">            &#125;, <span class="number">1000</span>)</span><br><span class="line">            e.clearSelection();</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    initCopyCode();</span><br><span class="line">&#125;(<span class="built_in">window</span>, <span class="built_in">document</span>);</span><br></pre></td></tr></table></figure></li><li><p>在yilia\layout\layout.ejs 添加代码，&lt;body&gt;之前</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 代码块复制功能 --&gt;</span><br><span class="line">&lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;clipboard@2.0.4&#x2F;dist&#x2F;clipboard.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">&lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;https:&#x2F;&#x2F;apps.bdimg.com&#x2F;libs&#x2F;jquery&#x2F;2.1.4&#x2F;jquery.min.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">&lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;&#x2F;js&#x2F;clipboard_use.js&quot;&gt;&lt;&#x2F;script&gt;</span><br></pre></td></tr></table></figure></li><li><p>yilia\source\main.0cf68a.css 添加</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*代码复制按钮*/</span></span><br><span class="line"><span class="selector-class">.btn-copy</span> &#123;</span><br><span class="line">    <span class="attribute">display</span>: inline-block;</span><br><span class="line">    <span class="attribute">cursor</span>: pointer;</span><br><span class="line">    <span class="attribute">background-color</span>: <span class="number">#eee</span>;</span><br><span class="line">    <span class="attribute">background-image</span>: <span class="built_in">linear-gradient</span>(#fcfcfc, #eee);</span><br><span class="line">    <span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#d5d5d5</span>;</span><br><span class="line">    <span class="attribute">border-radius</span>: <span class="number">3px</span>;</span><br><span class="line">    <span class="attribute">-webkit-user-select</span>: none;</span><br><span class="line">    <span class="attribute">-moz-user-select</span>: none;</span><br><span class="line">    <span class="attribute">-ms-user-select</span>: none;</span><br><span class="line">    <span class="attribute">user-select</span>: none;</span><br><span class="line">    <span class="attribute">-webkit-appearance</span>: none;</span><br><span class="line">    <span class="attribute">font-size</span>: <span class="number">13px</span>;</span><br><span class="line">    <span class="attribute">font-weight</span>: <span class="number">700</span>;</span><br><span class="line">    <span class="attribute">line-height</span>: <span class="number">20px</span>;</span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#333</span>;</span><br><span class="line">    <span class="attribute">-webkit-transition</span>: opacity .<span class="number">3s</span> ease-in-out;</span><br><span class="line">    <span class="attribute">-o-transition</span>: opacity .<span class="number">3s</span> ease-in-out;</span><br><span class="line">    <span class="attribute">transition</span>: opacity .<span class="number">3s</span> ease-in-out;</span><br><span class="line">    <span class="attribute">padding</span>: <span class="number">2px</span> <span class="number">6px</span>;</span><br><span class="line">    <span class="attribute">position</span>: absolute;</span><br><span class="line">    <span class="attribute">right</span>: <span class="number">5px</span>;</span><br><span class="line">    <span class="attribute">top</span>: <span class="number">5px</span>;</span><br><span class="line">    <span class="attribute">opacity</span>: <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.btn-copy</span> <span class="selector-tag">span</span> &#123;</span><br><span class="line">    <span class="attribute">margin-left</span>: <span class="number">5px</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.code-wrapper</span><span class="selector-pseudo">:hover</span> <span class="selector-class">.btn-copy</span> &#123;</span><br><span class="line">    <span class="attribute">opacity</span>: <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h1 id="7、点击特效"><a href="#7、点击特效" class="headerlink" title="7、点击特效"></a>7、点击特效</h1><ol><li><p>yilia\source下新建love.js,内容</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!<span class="function"><span class="keyword">function</span>(<span class="params">e,t,a</span>)</span>&#123;<span class="function"><span class="keyword">function</span> <span class="title">n</span>(<span class="params"></span>)</span>&#123;c(<span class="string">&quot;.heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: &#x27;&#x27;;width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;&quot;</span>),o(),r()&#125;<span class="function"><span class="keyword">function</span> <span class="title">r</span>(<span class="params"></span>)</span>&#123;<span class="keyword">for</span>(<span class="keyword">var</span> e=<span class="number">0</span>;e&lt;d.length;e++)d[e].alpha&lt;=<span class="number">0</span>?(t.body.removeChild(d[e].el),d.splice(e,<span class="number">1</span>)):(d[e].y--,d[e].scale+=<span class="number">.004</span>,d[e].alpha-=<span class="number">.013</span>,d[e].el.style.cssText=<span class="string">&quot;left:&quot;</span>+d[e].x+<span class="string">&quot;px;top:&quot;</span>+d[e].y+<span class="string">&quot;px;opacity:&quot;</span>+d[e].alpha+<span class="string">&quot;;transform:scale(&quot;</span>+d[e].scale+<span class="string">&quot;,&quot;</span>+d[e].scale+<span class="string">&quot;) rotate(45deg);background:&quot;</span>+d[e].color+<span class="string">&quot;;z-index:99999&quot;</span>);requestAnimationFrame(r)&#125;<span class="function"><span class="keyword">function</span> <span class="title">o</span>(<span class="params"></span>)</span>&#123;<span class="keyword">var</span> t=<span class="string">&quot;function&quot;</span>==<span class="keyword">typeof</span> e.onclick&amp;&amp;e.onclick;e.onclick=<span class="function"><span class="keyword">function</span>(<span class="params">e</span>)</span>&#123;t&amp;&amp;t(),i(e)&#125;&#125;<span class="function"><span class="keyword">function</span> <span class="title">i</span>(<span class="params">e</span>)</span>&#123;<span class="keyword">var</span> a=t.createElement(<span class="string">&quot;div&quot;</span>);a.className=<span class="string">&quot;heart&quot;</span>,d.push(&#123;<span class="attr">el</span>:a,<span class="attr">x</span>:e.clientX<span class="number">-5</span>,<span class="attr">y</span>:e.clientY<span class="number">-5</span>,<span class="attr">scale</span>:<span class="number">1</span>,<span class="attr">alpha</span>:<span class="number">1</span>,<span class="attr">color</span>:s()&#125;),t.body.appendChild(a)&#125;<span class="function"><span class="keyword">function</span> <span class="title">c</span>(<span class="params">e</span>)</span>&#123;<span class="keyword">var</span> a=t.createElement(<span class="string">&quot;style&quot;</span>);a.type=<span class="string">&quot;text/css&quot;</span>;<span class="keyword">try</span>&#123;a.appendChild(t.createTextNode(e))&#125;<span class="keyword">catch</span>(t)&#123;a.styleSheet.cssText=e&#125;t.getElementsByTagName(<span class="string">&quot;head&quot;</span>)[<span class="number">0</span>].appendChild(a)&#125;<span class="function"><span class="keyword">function</span> <span class="title">s</span>(<span class="params"></span>)</span>&#123;<span class="keyword">return</span><span class="string">&quot;rgb(&quot;</span>+~~(<span class="number">255</span>*<span class="built_in">Math</span>.random())+<span class="string">&quot;,&quot;</span>+~~(<span class="number">255</span>*<span class="built_in">Math</span>.random())+<span class="string">&quot;,&quot;</span>+~~(<span class="number">255</span>*<span class="built_in">Math</span>.random())+<span class="string">&quot;)&quot;</span>&#125;<span class="keyword">var</span> d=[];e.requestAnimationFrame=<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;<span class="keyword">return</span> e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||<span class="function"><span class="keyword">function</span>(<span class="params">e</span>)</span>&#123;<span class="built_in">setTimeout</span>(e,<span class="number">1e3</span>/<span class="number">60</span>)&#125;&#125;(),n()&#125;(<span class="built_in">window</span>,<span class="built_in">document</span>);</span><br></pre></td></tr></table></figure><ol start="2"><li>引入js,yilia\layout\ _partial\footer.ejs中引入js</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 页面点击小红心 --&gt;</span><br><span class="line">&lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;&#x2F;love.js&quot;&gt;&lt;&#x2F;script&gt;</span><br></pre></td></tr></table></figure></li></ol><h1 id="8、标签，分类，归档，评论"><a href="#8、标签，分类，归档，评论" class="headerlink" title="8、标签，分类，归档，评论"></a>8、标签，分类，归档，评论</h1><h2 id="1-归档"><a href="#1-归档" class="headerlink" title="1. 归档"></a>1. 归档</h2><ol><li><p>修改\yilia\_config.yml</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200917082455.png"></p></li></ol><h2 id="2-分类"><a href="#2-分类" class="headerlink" title="2. 分类"></a>2. 分类</h2><ol><li><p>生成分类页面,生成到source/categories</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page <span class="string">&quot;categories&quot;</span></span><br></pre></td></tr></table></figure></li><li><p>修改分类页面配置, source/categories/index.md</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 文章分类</span><br><span class="line">date: 2019-10-30 16:36:49</span><br><span class="line">type: &quot;categories&quot;</span><br><span class="line">layout: &quot;categories&quot;</span><br><span class="line">comments: false #关闭评论</span><br><span class="line">---</span><br></pre></td></tr></table></figure></li><li><p>分类页面的css, yilia\source\main.000.css</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">category-all-page</span> &#123;</span><br><span class="line">    <span class="attribute">margin</span>: <span class="number">30px</span> <span class="number">40px</span> <span class="number">30px</span> <span class="number">40px</span>;</span><br><span class="line">    <span class="attribute">position</span>: relative;</span><br><span class="line">    <span class="attribute">min-height</span>: <span class="number">70vh</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="selector-class">.category-all-page</span> <span class="selector-tag">h2</span> &#123;</span><br><span class="line">    <span class="attribute">margin</span>: <span class="number">20px</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="selector-class">.category-all-page</span> <span class="selector-class">.category-all-title</span> &#123;</span><br><span class="line">    <span class="attribute">text-align</span>: center;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="selector-class">.category-all-page</span> <span class="selector-class">.category-all</span> &#123;</span><br><span class="line">    <span class="attribute">margin-top</span>: <span class="number">20px</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="selector-class">.category-all-page</span> <span class="selector-class">.category-list</span> &#123;</span><br><span class="line">    <span class="attribute">margin</span>: <span class="number">0</span>;</span><br><span class="line">    <span class="attribute">padding</span>: <span class="number">0</span>;</span><br><span class="line">    <span class="attribute">list-style</span>: none;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="selector-class">.category-all-page</span> <span class="selector-class">.category-list-item-list-item</span> &#123;</span><br><span class="line">    <span class="attribute">margin</span>: <span class="number">10px</span> <span class="number">15px</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="selector-class">.category-all-page</span> <span class="selector-class">.category-list-item-list-count</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: $grey;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="selector-class">.category-all-page</span> <span class="selector-class">.category-list-item-list-count</span><span class="selector-pseudo">:before</span> &#123;</span><br><span class="line">    <span class="attribute">display</span>: inline;</span><br><span class="line">    <span class="attribute">content</span>: <span class="string">&quot; (&quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="selector-class">.category-all-page</span> <span class="selector-class">.category-list-item-list-count</span><span class="selector-pseudo">:after</span> &#123;</span><br><span class="line">    <span class="attribute">display</span>: inline;</span><br><span class="line">    <span class="attribute">content</span>: <span class="string">&quot;) &quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="selector-class">.category-all-page</span> <span class="selector-class">.category-list-item</span> &#123;</span><br><span class="line">    <span class="attribute">margin</span>: <span class="number">10px</span> <span class="number">10px</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="selector-class">.category-all-page</span> <span class="selector-class">.category-list-count</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: $grey;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="selector-class">.category-all-page</span> <span class="selector-class">.category-list-count</span><span class="selector-pseudo">:before</span> &#123;</span><br><span class="line">    <span class="attribute">display</span>: inline;</span><br><span class="line">    <span class="attribute">content</span>: <span class="string">&quot; (&quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="selector-class">.category-all-page</span> <span class="selector-class">.category-list-count</span><span class="selector-pseudo">:after</span> &#123;</span><br><span class="line">    <span class="attribute">display</span>: inline;</span><br><span class="line">    <span class="attribute">content</span>: <span class="string">&quot;) &quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="selector-class">.category-all-page</span> <span class="selector-class">.category-list-child</span> &#123;</span><br><span class="line">    <span class="attribute">padding-left</span>: <span class="number">10px</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>修改yilia\layout\categories.ejs</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;article class&#x3D;&quot;article article-type-post show&quot;&gt;</span><br><span class="line">  &lt;header class&#x3D;&quot;article-header&quot; style&#x3D;&quot;border-bottom: 1px solid #ccc&quot;&gt;</span><br><span class="line">  &lt;h1 class&#x3D;&quot;article-title&quot; itemprop&#x3D;&quot;name&quot;&gt;</span><br><span class="line">    &lt;%&#x3D; page.title %&gt;</span><br><span class="line">  &lt;&#x2F;h1&gt;</span><br><span class="line">  &lt;&#x2F;header&gt;</span><br><span class="line"></span><br><span class="line">  &lt;% if (site.categories.length)&#123; %&gt;</span><br><span class="line">  &lt;div class&#x3D;&quot;category-all-page&quot;&gt;</span><br><span class="line">    &lt;h2&gt;共计&amp;nbsp;&lt;%&#x3D; site.categories.length %&gt;&amp;nbsp;个分类&lt;&#x2F;h2&gt;</span><br><span class="line">    &lt;%- list_categories(site.categories, &#123;</span><br><span class="line">      show_count: true,</span><br><span class="line">      class: &#39;category-list-item&#39;,</span><br><span class="line">      style: &#39;list&#39;,</span><br><span class="line">      depth:3,    #这里代表着几层分类，写入时把这个注释删掉</span><br><span class="line">      separator: &#39;&#39;</span><br><span class="line">    &#125;) %&gt;</span><br><span class="line">  &lt;&#x2F;div&gt;</span><br><span class="line">  &lt;% &#125; %&gt;</span><br><span class="line">&lt;&#x2F;article&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>配置yilia\_config.yml</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200917082456.png"></p></li><li><p>文章开头编写</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: hexo博客搭建</span><br><span class="line">date: 2018-03-29 15:03:48</span><br><span class="line">tags: blog</span><br><span class="line">toc: true</span><br><span class="line">categories: hexo</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200917082457.png"></p></li></ol><h2 id="3-标签"><a href="#3-标签" class="headerlink" title="3. 标签"></a>3. 标签</h2><ol><li><p>新建标签页面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page <span class="string">&quot;tags&quot;</span></span><br></pre></td></tr></table></figure></li><li><p>修改 生成的页面配置 source/tags/index.md</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: tags</span><br><span class="line">date: 2020-09-11 16:43:48</span><br><span class="line">type: tags</span><br><span class="line">layout: tags</span><br><span class="line">comments: false</span><br><span class="line">---</span><br></pre></td></tr></table></figure></li><li><p>修改yilia\layout\tags.ejs</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;article class&#x3D;&quot;article article-type-post show&quot;&gt;</span><br><span class="line">  &lt;header class&#x3D;&quot;article-header&quot; style&#x3D;&quot;border-bottom: 1px solid #ccc&quot;&gt;</span><br><span class="line">    &lt;h1 class&#x3D;&quot;article-title&quot; itemprop&#x3D;&quot;name&quot;&gt;标签&lt;&#x2F;h1&gt;</span><br><span class="line"></span><br><span class="line">  &lt;&#x2F;header&gt;</span><br><span class="line"></span><br><span class="line">  &lt;% if (site.tags.length) &#123; %&gt;</span><br><span class="line">    &lt;div class&#x3D;&quot;tag-cloud&quot;&gt;</span><br><span class="line">      &lt;div class&#x3D;&quot;tag-cloud-title&quot;&gt;</span><br><span class="line">        &lt;%- &quot;TOTAl : &quot; + site.tags.length %&gt;</span><br><span class="line">        &lt;br&gt;</span><br><span class="line">        &lt;br&gt;</span><br><span class="line">      &lt;&#x2F;div&gt;</span><br><span class="line"></span><br><span class="line">      &lt;div class&#x3D;&quot;tag-cloud-tags&quot;&gt; </span><br><span class="line">        &lt;%- tagcloud(&#123;</span><br><span class="line">          min_font: 12,</span><br><span class="line">          max_font: 30,</span><br><span class="line">          amount: 200,</span><br><span class="line">          color: true,</span><br><span class="line">          start_color: &#39;#555&#39;,</span><br><span class="line">          end_color: &#39;#111&#39;</span><br><span class="line">          &#125;) %&gt;</span><br><span class="line">      &lt;&#x2F;div&gt;</span><br><span class="line">    &lt;&#x2F;div&gt;</span><br><span class="line">  &lt;% &#125; %&gt;</span><br><span class="line">  </span><br></pre></td></tr></table></figure></li></ol><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200917082458.png"></p><h2 id="4-gitalk评论"><a href="#4-gitalk评论" class="headerlink" title="4. gitalk评论"></a>4. gitalk评论</h2><ol><li><p>在github–settings–devloper settings申请OAuth应用</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200917084644.png" alt="image-20200917083728484"></p></li></ol><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200917084645.png" alt="image-20200917084053381"></p><ol start="2"><li>yilia/_config.yml</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">gitalk:</span> </span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">clientID:</span> <span class="comment">#这里是申请的OAuth的Client ID</span></span><br><span class="line">  <span class="attr">clientSecret:</span> <span class="comment">#这里是申请的OAuth的秘钥</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">gitalk-comment</span> <span class="comment">#可以单独一个仓库保存评论，这里是仓库名</span></span><br><span class="line">  <span class="attr">owner:</span> <span class="string">curryfor369</span> <span class="comment">#github用户名</span></span><br><span class="line">  <span class="attr">admin:</span> <span class="string">curryfor369</span> <span class="comment">#github用户名</span></span><br><span class="line">  <span class="attr">distractionFreeMode:</span> <span class="literal">true</span> <span class="comment">#免打扰模式</span></span><br></pre></td></tr></table></figure><ol start="3"><li>yilia/layout/_partial/post/gitalk.ejs</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&lt;div id&#x3D;&quot;gitalk-container&quot; style&#x3D;&quot;padding: 0px 30px 0px 30px;&quot;&gt;&lt;&#x2F;div&gt; </span><br><span class="line"></span><br><span class="line">&lt;link rel&#x3D;&quot;stylesheet&quot; href&#x3D;&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;gitalk@1&#x2F;dist&#x2F;gitalk.css&quot;&gt;</span><br><span class="line">&lt;script src&#x3D;&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;gitalk@1&#x2F;dist&#x2F;gitalk.min.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">&lt;script type&#x3D;&quot;text&#x2F;javascript&quot;&gt;</span><br><span class="line"></span><br><span class="line">    if(&lt;%&#x3D;theme.gitalk.enable%&gt;)&#123;</span><br><span class="line">        var gitalk &#x3D; new Gitalk(&#123;</span><br><span class="line">        clientID: &#39;&lt;%&#x3D;theme.gitalk.clientID%&gt;&#39;,</span><br><span class="line">        clientSecret: &#39;&lt;%&#x3D;theme.gitalk.clientSecret%&gt;&#39;,</span><br><span class="line">        repo: &#39;&lt;%&#x3D;theme.gitalk.repo%&gt;&#39;,</span><br><span class="line">        owner: &#39;&lt;%&#x3D;theme.gitalk.owner%&gt;&#39;,</span><br><span class="line">        admin: [&#39;&lt;%&#x3D;theme.gitalk.admin%&gt;&#39;],</span><br><span class="line">        id: &#39;&lt;%&#x3D; page.date %&gt;&#39;,</span><br><span class="line">        distractionFreeMode: &#39;&lt;%&#x3D;theme.gitalk.distractionFreeMode%&gt;&#39;</span><br><span class="line">    &#125;)</span><br><span class="line">    gitalk.render(&#39;gitalk-container&#39;) </span><br><span class="line">    &#125;</span><br><span class="line">&lt;&#x2F;script&gt;</span><br></pre></td></tr></table></figure><ol start="4"><li>yilia/layout/_partial/article.ejs</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;% if (!index &amp;&amp; post.comments)&#123; %&gt;  在这个打标签里添加下方代码</span><br><span class="line"></span><br><span class="line">&lt;% if (theme.gitalk.enable &amp;&amp; theme.gitalk.distractionFreeMode)&#123; %&gt;</span><br><span class="line">        &lt;%- partial(&#39;post&#x2F;gitalk&#39;, &#123;</span><br><span class="line">            key: post.slug,</span><br><span class="line">            title: post.title,</span><br><span class="line">            url: config.url+url_for(post.path)</span><br><span class="line">         &#125;) %&gt;</span><br><span class="line">&lt;% &#125; %&gt;</span><br></pre></td></tr></table></figure><ol start="5"><li>yilia/source-src/comment.scss</li></ol><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#disqus_thread</span>, <span class="selector-class">.duoshuo</span>, <span class="selector-class">.cloud-tie-wrapper</span>, <span class="selector-id">#SOHUCS</span>, <span class="selector-id">#gitment-ctn</span>, <span class="selector-id">#gitalk-container</span>&#123;</span><br><span class="line"><span class="attribute">padding</span>: <span class="number">0</span> <span class="number">30px</span> <span class="meta">!important</span>;</span><br><span class="line"><span class="attribute">min-height</span>: <span class="number">20px</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-id">#SOHUCS</span> &#123;</span><br><span class="line">#SOHU_MAIN .module-cmt-list .block-cont-gw &#123;</span><br><span class="line"><span class="selector-tag">border-bottom</span>: 1<span class="selector-tag">px</span> <span class="selector-tag">dashed</span> <span class="selector-id">#c8c8c8</span> !<span class="selector-tag">important</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="9、侧边栏隐藏"><a href="#9、侧边栏隐藏" class="headerlink" title="9、侧边栏隐藏"></a>9、侧边栏隐藏</h1><ol><li><p>添加css,<code>themes\yilia\source\main.0cf68a.css</code></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*侧边栏隐藏*/</span></span><br><span class="line"><span class="selector-class">.mymenucontainer</span> &#123;</span><br><span class="line"><span class="attribute">display</span>:block;</span><br><span class="line"><span class="attribute">cursor</span>:pointer;</span><br><span class="line"><span class="attribute">left</span>:<span class="number">0</span>;</span><br><span class="line"><span class="attribute">top</span>:<span class="number">0</span>;</span><br><span class="line"><span class="attribute">width</span>:<span class="number">35px</span>;</span><br><span class="line"><span class="attribute">height</span>:<span class="number">35px</span>;</span><br><span class="line"><span class="attribute">z-index</span>:<span class="number">9999</span>;</span><br><span class="line"><span class="attribute">position</span>:fixed;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.bar1</span> &#123;</span><br><span class="line"><span class="attribute">width</span>:<span class="number">35px</span>;</span><br><span class="line"><span class="attribute">height</span>:<span class="number">3px</span>;</span><br><span class="line"><span class="attribute">background-color</span>:<span class="number">#333</span>;</span><br><span class="line"><span class="attribute">margin</span>:<span class="number">6px</span> <span class="number">0</span>;</span><br><span class="line"><span class="attribute">transition</span>:<span class="number">0.4s</span>;</span><br><span class="line"><span class="attribute">-webkit-transform</span>:<span class="built_in">rotate</span>(-<span class="number">45deg</span>) <span class="built_in">translate</span>(-<span class="number">8px</span>,<span class="number">8px</span>);</span><br><span class="line"><span class="attribute">transform</span>:<span class="built_in">rotate</span>(-<span class="number">45deg</span>) <span class="built_in">translate</span>(-<span class="number">8px</span>,<span class="number">8px</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.bar2</span> &#123;</span><br><span class="line"><span class="attribute">width</span>:<span class="number">35px</span>;</span><br><span class="line"><span class="attribute">height</span>:<span class="number">3px</span>;</span><br><span class="line"><span class="attribute">background-color</span>:<span class="number">#333</span>;</span><br><span class="line"><span class="attribute">margin</span>:<span class="number">6px</span> <span class="number">0</span>;</span><br><span class="line"><span class="attribute">transition</span>:<span class="number">0.4s</span>;</span><br><span class="line"><span class="attribute">opacity</span>:<span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.bar3</span> &#123;</span><br><span class="line"><span class="attribute">width</span>:<span class="number">35px</span>;</span><br><span class="line"><span class="attribute">height</span>:<span class="number">3px</span>;</span><br><span class="line"><span class="attribute">background-color</span>:<span class="number">#333</span>;</span><br><span class="line"><span class="attribute">margin</span>:<span class="number">6px</span> <span class="number">0</span>;</span><br><span class="line"><span class="attribute">transition</span>:<span class="number">0.4s</span>;</span><br><span class="line"><span class="attribute">-webkit-transform</span>:<span class="built_in">rotate</span>(<span class="number">45deg</span>) <span class="built_in">translate</span>(-<span class="number">4px</span>,-<span class="number">6px</span>);</span><br><span class="line"><span class="attribute">transform</span>:<span class="built_in">rotate</span>(<span class="number">45deg</span>) <span class="built_in">translate</span>(-<span class="number">4px</span>,-<span class="number">6px</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.change</span> <span class="selector-class">.bar1</span> &#123;</span><br><span class="line"><span class="attribute">-webkit-transform</span>:<span class="built_in">rotate</span>(<span class="number">0deg</span>) <span class="built_in">translate</span>(<span class="number">0px</span>,<span class="number">0px</span>);</span><br><span class="line"><span class="attribute">transform</span>:<span class="built_in">rotate</span>(<span class="number">0deg</span>) <span class="built_in">translate</span>(<span class="number">0px</span>,<span class="number">0px</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.change</span> <span class="selector-class">.bar2</span> &#123;</span><br><span class="line"><span class="attribute">opacity</span>:<span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.change</span> <span class="selector-class">.bar3</span> &#123;</span><br><span class="line"><span class="attribute">-webkit-transform</span>:<span class="built_in">rotate</span>(<span class="number">0deg</span>) <span class="built_in">translate</span>(<span class="number">0px</span>,<span class="number">0px</span>);</span><br><span class="line"><span class="attribute">transform</span>:<span class="built_in">rotate</span>(<span class="number">0deg</span>) <span class="built_in">translate</span>(<span class="number">0px</span>,<span class="number">0px</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*侧边栏隐藏结束*/</span></span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>添加按钮到相应的位置,``yilia\layout\layout.ejs<code>,在</code>&lt;div class=”left-col”`上方添加,在body后html前添加js</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;mymenucontainer&quot;</span> <span class="attr">onclick</span>=<span class="string">&quot;myFunction(this)&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;bar1&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;bar2&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;bar3&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="javascript">        <span class="keyword">var</span> hide = <span class="literal">false</span>;</span></span><br><span class="line"><span class="javascript">        <span class="function"><span class="keyword">function</span> <span class="title">myFunction</span>(<span class="params">x</span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">            x.classList.toggle(<span class="string">&quot;change&quot;</span>);</span></span><br><span class="line"><span class="javascript">            <span class="keyword">if</span>(hide == <span class="literal">false</span>)&#123;</span></span><br><span class="line"><span class="javascript">                $(<span class="string">&quot;.left-col&quot;</span>).css(<span class="string">&#x27;display&#x27;</span>, <span class="string">&#x27;none&#x27;</span>);</span></span><br><span class="line"><span class="javascript">                $(<span class="string">&quot;.mid-col&quot;</span>).css(<span class="string">&quot;left&quot;</span>, <span class="number">6</span>);</span></span><br><span class="line"><span class="javascript">                $(<span class="string">&quot;.tools-col&quot;</span>).css(<span class="string">&#x27;display&#x27;</span>, <span class="string">&#x27;none&#x27;</span>);</span></span><br><span class="line"><span class="javascript">                $(<span class="string">&quot;.tools-col.hide&quot;</span>).css(<span class="string">&#x27;display&#x27;</span>, <span class="string">&#x27;none&#x27;</span>);</span></span><br><span class="line"><span class="javascript">                hide = <span class="literal">true</span>;</span></span><br><span class="line"><span class="javascript">            &#125;<span class="keyword">else</span>&#123;</span></span><br><span class="line"><span class="javascript">                $(<span class="string">&quot;.left-col&quot;</span>).css(<span class="string">&#x27;display&#x27;</span>, <span class="string">&#x27;&#x27;</span>);</span></span><br><span class="line"><span class="javascript">                $(<span class="string">&quot;.mid-col&quot;</span>).css(<span class="string">&quot;left&quot;</span>, <span class="number">300</span>);</span></span><br><span class="line"><span class="javascript">                $(<span class="string">&quot;.tools-col&quot;</span>).css(<span class="string">&#x27;display&#x27;</span>, <span class="string">&#x27;&#x27;</span>);</span></span><br><span class="line"><span class="javascript">                $(<span class="string">&quot;.tools-col.hide&quot;</span>).css(<span class="string">&#x27;display&#x27;</span>, <span class="string">&#x27;&#x27;</span>);</span></span><br><span class="line"><span class="javascript">                hide = <span class="literal">false</span>;</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><h1 id="10、代码块长度"><a href="#10、代码块长度" class="headerlink" title="10、代码块长度"></a>10、代码块长度</h1><ol><li>修改yilia\source\main.0cd68a.css</li></ol><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.code-wrapper</span>&#123;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">80%</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="11、支持pdf浏览"><a href="#11、支持pdf浏览" class="headerlink" title="11、支持pdf浏览"></a>11、支持pdf浏览</h1><ol><li><p>安装插件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-pdf</span><br></pre></td></tr></table></figure></li><li><p>生成book页面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page book </span><br></pre></td></tr></table></figure></li><li><p>修改 source/book/index.md</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 外部链接</span></span><br><span class="line">&#123;% pdf http://7xov2f.com1.z0.glb.clouddn.com/bash_freshman.pdf %&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 本地连接(资源放在index.md同级目录下)</span></span><br><span class="line">&#123;% pdf ./pdf名字.pdf %&#125;</span><br></pre></td></tr></table></figure></li></ol><h1 id="x、遇到的问题"><a href="#x、遇到的问题" class="headerlink" title="x、遇到的问题"></a>x、遇到的问题</h1><ol><li><p>本地服务器访问时正常，部署到远端后，css无效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 修改 _config.yml 第17行的root选项 加上仓库名 后重新清理生成部署</span><br><span class="line">17 root: &#x2F;blog</span><br></pre></td></tr></table></figure></li><li><p>hexo d 部署失败</p><blockquote><p>FATAL {   err: Error: Spawn failed       at ChildProcess.<anonymous>……}</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看用户名，邮箱是否正确</span></span><br><span class="line">git config --global user.name <span class="string">&quot;gudeball&quot;</span></span><br><span class="line">git config --global user.email <span class="string">&quot;gudeball@163.com&quot;</span></span><br><span class="line"><span class="comment"># 配置后重新部署</span></span><br><span class="line"><span class="comment"># 若还是错误，删掉根目录下的.deploy_git，再重新部署</span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1、搭建步骤&quot;&gt;&lt;a href=&quot;#1、搭建步骤&quot; class=&quot;headerlink&quot; title=&quot;1、搭建步骤&quot;&gt;&lt;/a&gt;1、搭建步骤&lt;/h1&gt;&lt;p&gt;注：cmd为管理员模式 环境： win10+git+github+nodejs&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;下载cmder完整版，配置cmder, 管理员bash，开机bash，中文字符集&lt;/p&gt;</summary>
    
    
    
    <category term="hexo" scheme="http://iscurry.com/categories/hexo/"/>
    
    
    <category term="hexo" scheme="http://iscurry.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>Hbase(1)</title>
    <link href="http://iscurry.com/2019/02/22/Hbase(1)/"/>
    <id>http://iscurry.com/2019/02/22/Hbase(1)/</id>
    <published>2019-02-22T09:57:07.000Z</published>
    <updated>2020-09-25T02:18:35.544Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-hbase简介"><a href="#1-hbase简介" class="headerlink" title="1. hbase简介"></a>1. hbase简介</h1><ul><li><p>habse是面向列,可伸缩的非关系型开源数据库,其特点是==列式存储==</p></li><li><p>hbase中的所有数据==只有一种==数据类型 byte[]</p></li><li><p>hbase中不管显示的是几行数据==只要行键(rowkey)一样==,那==就是一行==数据。</p></li><li><p>==Hbase集群搭建设计思想== : HMaster因为是负责存储元数据的,所以重要,不要和其他节点放到</p><p>一起,因为不想一个死都死,HRegionServer尽量和DataNode放一起,因为要和HDFS交互数据,</p><p>所以要尽量减少网络间数据传输。Zk集群单独放。</p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922175517.png" alt="image-20200922175514505"></p></li></ul><a id="more"></a><h1 id="2-hbase安装"><a href="#2-hbase安装" class="headerlink" title="2. hbase安装"></a>2. hbase安装</h1><h3 id="1、上传，解压，改名，配置环境变量"><a href="#1、上传，解压，改名，配置环境变量" class="headerlink" title="1、上传，解压，改名，配置环境变量"></a>1、上传，解压，改名，配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf apache-hbase-1.1.2-bin.tar.gz  -C /opt/module/</span><br><span class="line">cd /opt/module/</span><br><span class="line">mv apache-hbase-1.1.2-bin hbase-1.1.2</span><br><span class="line"></span><br><span class="line">vim /etc/profile</span><br><span class="line">export HBASE_HOME=/opt/module/hbase-1.1.2</span><br><span class="line">export PATH=$PATH:$&#123;HBASE_HOME&#125;/bin</span><br></pre></td></tr></table></figure><h3 id="2、配置hbase-env-sh"><a href="#2、配置hbase-env-sh" class="headerlink" title="2、配置hbase-env.sh"></a>2、配置hbase-env.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/home/soft/jdk</span><br><span class="line">export HBASE_LOG_DIR=$&#123;HBASE_HOME&#125;/logs</span><br><span class="line">export HBASE_MANAGES_ZK=false #这个指定不用hbase自带的zk</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="3、配置hbase-site-xml"><a href="#3、配置hbase-site-xml" class="headerlink" title="3、配置hbase-site.xml"></a>3、配置hbase-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--告诉hbase启动的是集群模式--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>告诉hbase启动的是集群模式<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定共享的文件存储位置  hdfs--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://node1:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>-指定共享的文件存储位置  hdfs<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定本地文件存放目录--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/datas/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>指定本地文件存放目录<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--zookeeper的集群路径--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorom<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:2181,node2:2181,node3:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>zookeeper的集群路径<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="4、修改regionservers"><a href="#4、修改regionservers" class="headerlink" title="4、修改regionservers"></a>4、修改regionservers</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim regionservers</span><br><span class="line"></span><br><span class="line">node2</span><br><span class="line">node3</span><br></pre></td></tr></table></figure><h3 id="5、分发hbase文件和环境变量并刷新"><a href="#5、分发hbase文件和环境变量并刷新" class="headerlink" title="5、分发hbase文件和环境变量并刷新"></a>5、分发hbase文件和环境变量并刷新</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scp -r /opt/module/hbase-1.1.2/ node2:/opt/module/</span><br><span class="line">scp -r /opt/module/hbase-1.1.2/ node3:/opt/module/</span><br><span class="line"></span><br><span class="line">scp /etc/profile node2:/etc/</span><br><span class="line">scp /etc/profile node3:/etc/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ssh node1 </span><br><span class="line">source /etc/profile;</span><br><span class="line">ssh node2</span><br><span class="line">source /etc/profile;</span><br><span class="line">ssh node3 </span><br><span class="line">source /etc/profile;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="6、启动hdfs和zk"><a href="#6、启动hdfs和zk" class="headerlink" title="6、启动hdfs和zk"></a>6、启动hdfs和zk</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br><span class="line"></span><br><span class="line">for host in node1 node2 node3</span><br><span class="line">do</span><br><span class="line">ssh $&#123;host&#125; &quot;source /etc/profile; zkServer.sh stop; zkServer.sh start;&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h3 id="7、启动hbase"><a href="#7、启动hbase" class="headerlink" title="7、启动hbase"></a>7、启动hbase</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-hbase.sh </span><br></pre></td></tr></table></figure><h3 id="8、检测是否成功"><a href="#8、检测是否成功" class="headerlink" title="8、检测是否成功"></a>8、检测是否成功</h3><ul><li>jps检测<ul><li>HMaster</li><li>HRegionServer</li></ul></li><li>web检测 <ul><li>192.168.10.101:16010</li></ul></li></ul><h3 id="9、热备份"><a href="#9、热备份" class="headerlink" title="9、热备份"></a>9、热备份</h3><ul><li>用(守护)进程的命令方式 启动HMaster，多个HMaster之间可以实现热备份</li><li>现在node1是hmaster, 在node2上启动守护进程，可实现热备份</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase-daemon.sh start master</span><br></pre></td></tr></table></figure><h3 id="10、-高可用"><a href="#10、-高可用" class="headerlink" title="10、 高可用"></a>10、 高可用</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">touch conf/backup-masters</span><br><span class="line">echo &quot;node2&quot; &gt; backup-masters</span><br><span class="line">重启hbase</span><br></pre></td></tr></table></figure><h2 id="11、安装问题"><a href="#11、安装问题" class="headerlink" title="==11、安装问题=="></a>==11、安装问题==</h2><h3 id="1-安装好-能启动-不能创建"><a href="#1-安装好-能启动-不能创建" class="headerlink" title="1. 安装好,能启动,不能创建"></a>1. 安装好,能启动,不能创建</h3><ul><li><p>描述：可以hbase shell进入客户端，可以list,但是不能创建表，不能创建namespace</p></li><li><p>报错：org.apache.hadoop.hbase.PleaseHoldException: Master is initializing</p></li><li><p>解决：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">0. 删除zk里的hbase文件残留,重启hbase解决 !!! </span><br><span class="line">1. 进入zk客户端 zkCli.sh</span><br><span class="line">2. 查看列表 ls /  可以查看到[...,hbase]</span><br><span class="line">3. 删除hbase文件加  rmr /hbase</span><br><span class="line">4. 重启habse</span><br></pre></td></tr></table></figure></li></ul><h1 id="3-hbase-shell"><a href="#3-hbase-shell" class="headerlink" title="3. hbase shell"></a>3. hbase shell</h1><ol><li>进入shell   hbase shell</li><li>帮助 help</li><li>查看帮助事例 help ‘list_namespace_table’</li><li>可以先在Nodepad里编辑后贴入，防止出错</li></ol><p><img src="D:\user\Desktop\冉辰星总结\hbase\hbase.assets\image-20200530105908591.png"></p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922175444.png"></p><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922175438.png"></p><h3 id="2-要点"><a href="#2-要点" class="headerlink" title="2. ==要点=="></a>2. ==要点==</h3><ol><li>命名空间的操做</li><li>表的操作</li></ol><h1 id="4-hbase-api"><a href="#4-hbase-api" class="headerlink" title="4.hbase-api"></a>4.hbase-api</h1><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>commons-logging<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-logging<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- hbase mapreduce 的maven 依赖 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h1 id="5-HBase操作Mapreduce-hdfs-hbase"><a href="#5-HBase操作Mapreduce-hdfs-hbase" class="headerlink" title="5. HBase操作Mapreduce==[hdfs-hbase]=="></a>5. HBase操作Mapreduce==[hdfs-hbase]==</h1><ul><li><p>前提</p><ul><li><p>要用到yarn和hdsf,所以要==启动hadoop集群==</p></li><li><p>要设置HADOOP_CLASSPATH</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_HOME=/opt/module/hbase-1.1.2</span><br><span class="line">export HADOOP_HOME=/home/soft/hadoop</span><br><span class="line">export HADOOP_CLASSPATH=`$&#123;HBASE_HOME&#125;/bin/hbase mapredcp`</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="1-将-hdfs-上的-tsv-文件导入到-hbase-数据库中"><a href="#1-将-hdfs-上的-tsv-文件导入到-hbase-数据库中" class="headerlink" title="1. 将==hdfs==上的==tsv==文件导入到==hbase==数据库中"></a>1. 将==hdfs==上的==tsv==文件导入到==hbase==数据库中</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1. 创建表</span></span><br><span class="line">create &#x27;nba&#x27;,&#x27;info&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2. 制造数据</span></span><br><span class="line">vim nba.tsv</span><br><span class="line">2009currygs</span><br><span class="line">2011klgs</span><br><span class="line">2012greengs</span><br><span class="line">2013kawayisas</span><br><span class="line">2014miqiujazz</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3. 上传到hdfs</span></span><br><span class="line">hdfs dfs -mkdir /input_nba</span><br><span class="line">hdfs dfs -put nba.tsv /intput_nba</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4. 执行Mapreduce，将hdfs的.tsv文件导入hbase数据库</span></span><br><span class="line">yarn jar /opt/module/hbase-1.1.2/lib/hbase-server-1.1.2.jar \</span><br><span class="line">importtsv \</span><br><span class="line">-Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:teams nba \</span><br><span class="line">hdfs://node1:9000/input_nba</span><br></pre></td></tr></table></figure><h2 id="2-hbase-gt-hdfs"><a href="#2-hbase-gt-hdfs" class="headerlink" title="2. ==hbase== =&gt; ==hdfs=="></a>2. ==hbase== =&gt; ==hdfs==</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Driver export sqoop_hbase_stu /hiehie</span><br></pre></td></tr></table></figure><h2 id="3-通过Mapreduce将-hbase-另一个-hbase-库-【未完成】"><a href="#3-通过Mapreduce将-hbase-另一个-hbase-库-【未完成】" class="headerlink" title="3  通过Mapreduce将==hbase==另一个==hbase==库 ==【未完成】=="></a>3  通过Mapreduce将==hbase==另一个==hbase==库 ==【未完成】==</h2><h1 id="6-hbase–hive"><a href="#6-hbase–hive" class="headerlink" title="6. hbase–hive"></a>6. hbase–hive</h1><h2 id="1-hive关联habse已存在表"><a href="#1-hive关联habse已存在表" class="headerlink" title="1. hive关联habse已存在表"></a>1. hive关联habse已存在表</h2><ul><li>Hive==必须==创建==拓展表==</li></ul><ol><li><p>创建habse表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create &#x27;xing:hbase_hive&#x27;,&#x27;info&#x27; </span><br></pre></td></tr></table></figure></li><li><p>设置Hive属性</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set hbase.zookeeper.quorum=node1:2181,node2:2181,node3:2181;</span><br><span class="line">set zookeeper.znode.parent=/hbase;</span><br></pre></td></tr></table></figure></li><li><p>创建hive表，关联hbase</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE hbase_hive (</span><br><span class="line">rowkey string,</span><br><span class="line">info map&lt;STRING,STRING&gt;</span><br><span class="line">) STORED BY &#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;</span><br><span class="line">WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,info:&quot;)</span><br><span class="line">TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;xing:hbase_hive&quot;);</span><br></pre></td></tr></table></figure></li><li><p>向hbase插入数据，在hive中获取数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> put <span class="string">&#x27;xing:hbase_hive&#x27;</span>,<span class="string">&#x27;row1&#x27;</span>,<span class="string">&#x27;info:sex&#x27;</span>,<span class="string">&#x27;man&#x27;</span></span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> select * from hbase_hive;</span></span><br></pre></td></tr></table></figure></li></ol><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922175429.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><h2 id="2-hive–habse都是全新表"><a href="#2-hive–habse都是全新表" class="headerlink" title="2. hive–habse都是全新表"></a>2. hive–habse都是全新表</h2><ul><li>Hive==必须==创建==普通表==</li><li>如果hive创建报错，请重新编译 hive/lib/hive-hbase-handler-1.2.1.jar</li></ul><p><img src="https://gitee.com/curryfor369/picgo/raw/master/img/20200922175406.png"></p><ol><li><p>创建hive表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE hbase_hive2 (</span><br><span class="line">rowkey string,</span><br><span class="line">id String,</span><br><span class="line">name String,</span><br><span class="line">sex String</span><br><span class="line">) STORED BY &#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;</span><br><span class="line">WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,info:id,info:name,info:sex&quot;)</span><br><span class="line">TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;xing:hbase_hive2&quot;);</span><br></pre></td></tr></table></figure></li><li><p>设置Hive属性</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set hbase.zookeeper.quorum=node1:2181,node2:2181,node3:2181;</span><br><span class="line">set zookeeper.znode.parent=/hbase;</span><br></pre></td></tr></table></figure></li><li><p>创建hbase表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create &#x27;xing:hbase_hive2&#x27;,&#x27;info&#x27;;</span><br></pre></td></tr></table></figure></li><li><p>向hbase插入数据，在hive中获取数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> put <span class="string">&#x27;xing:hbase_hive2&#x27;</span>,<span class="string">&#x27;row1&#x27;</span>,<span class="string">&#x27;info:sex&#x27;</span>,<span class="string">&#x27;man&#x27;</span>,</span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> put <span class="string">&#x27;xing:hbase_hive2&#x27;</span>,<span class="string">&#x27;row1&#x27;</span>,<span class="string">&#x27;info:id&#x27;</span>,<span class="string">&#x27;10&#x27;</span></span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> <span class="built_in">set</span> hive.cli.print.header=<span class="literal">true</span>;  <span class="comment"># 查询显示字段名</span></span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> select * from hbase_hive2;</span></span><br></pre></td></tr></table></figure></li><li><p>查不到的用NULL填充6</p></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-hbase简介&quot;&gt;&lt;a href=&quot;#1-hbase简介&quot; class=&quot;headerlink&quot; title=&quot;1. hbase简介&quot;&gt;&lt;/a&gt;1. hbase简介&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;habse是面向列,可伸缩的非关系型开源数据库,其特点是==列式存储==&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;hbase中的所有数据==只有一种==数据类型 byte[]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;hbase中不管显示的是几行数据==只要行键(rowkey)一样==,那==就是一行==数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;==Hbase集群搭建设计思想== : HMaster因为是负责存储元数据的,所以重要,不要和其他节点放到&lt;/p&gt;
&lt;p&gt;一起,因为不想一个死都死,HRegionServer尽量和DataNode放一起,因为要和HDFS交互数据,&lt;/p&gt;
&lt;p&gt;所以要尽量减少网络间数据传输。Zk集群单独放。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://gitee.com/curryfor369/picgo/raw/master/img/20200922175517.png&quot; alt=&quot;image-20200922175514505&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Hbase" scheme="http://iscurry.com/categories/Hbase/"/>
    
    
    <category term="Hbase" scheme="http://iscurry.com/tags/Hbase/"/>
    
  </entry>
  
</feed>
